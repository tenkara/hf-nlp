{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/Raj/.cache/huggingface/datasets/rajknakka___parquet/rajknakka--github-issues-comments-d10cf254d383122f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
       "    num_rows: 4900\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load my recently created dataset from HuggingFace Datasets hub\n",
    "issues_dataset = load_dataset(\"rajknakka/github-issues-comments\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Raj\\.cache\\huggingface\\datasets\\rajknakka___parquet\\rajknakka--github-issues-comments-d10cf254d383122f\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-be07580daa56c5b4.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
       "    num_rows: 1598\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out pull requests as these tend to be rarely used for answering user queries and will introduce noise in our search engine\n",
    "issues_dataset = issues_dataset.filter(lambda row: row['is_pull_request'] == False and len(row[\"comments\"]) > 0)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 1598\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep the title, body, comments and html_url columns and remove the rest of the columns\n",
    "columns_to_keep = [\"title\", \"body\", \"comments\", \"html_url\"]\n",
    "columns = issues_dataset.column_names\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "prepared_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "prepared_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 1598\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create embeddings with the comments and their context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first conver the dataset to a pandas dataframe\n",
    "prepared_dataset.set_format(\"pandas\")\n",
    "issues_df = prepared_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi ! The audio data don\\'t always exist as files on disk - the blobs are often stored in the Arrow files. For now I\\'d suggest disabling decoding with `.cast_column(\"audio\", Audio(decode=False))` and apply your own decoding that handles corrupted files (maybe to filter them out ?)\\r\\n\\r\\ncc @sanchit-gandhi since it\\'s related to our discussion about allowing users to make decoding return `None` and show a warning when there are corrupted files',\n",
       " 'Thanks @lhoestq, I wasn\\'t aware of the decode flag. It makes more sense as you say to show a warning when there are corrupted files together with some metadata of the file that allows to filter them from the dataset.\\r\\n\\r\\nMy workaround was to catch the LibsndfileError and generate a dummy audio with an unsual sample rate to filter it later. However returning `None` seems better. \\r\\n\\r\\n`try:\\r\\n    array, sampling_rate = sf.read(file)\\r\\nexcept sf.LibsndfileError:\\r\\n    print(\"bad file\")\\r\\n    array = np.array([0.0])\\r\\n    sampling_rate = 99.000` \\r\\n\\r\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the dataframe to see the four columns\n",
    "issues_df[\"comments\"][1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Support for data with instance-wise dictionary...</td>\n",
       "      <td>Hi ! We use the Arrow columnar format under th...</td>\n",
       "      <td>### Feature request\\n\\nI notice that when load...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Return the audio filename when decoding fails ...</td>\n",
       "      <td>Hi ! The audio data don't always exist as file...</td>\n",
       "      <td>### Feature request\\r\\n\\r\\nReturn the audio fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Return the audio filename when decoding fails ...</td>\n",
       "      <td>Thanks @lhoestq, I wasn't aware of the decode ...</td>\n",
       "      <td>### Feature request\\r\\n\\r\\nReturn the audio fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>IndexError Not Solving -&gt; IndexError: Invalid ...</td>\n",
       "      <td>https://colab.research.google.com/#scrollTo=AQ...</td>\n",
       "      <td>### Describe the bug\\n\\nin &lt;cell line: 1&gt;:1   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Support for data with instance-wise dictionary...   \n",
       "1  Return the audio filename when decoding fails ...   \n",
       "2  Return the audio filename when decoding fails ...   \n",
       "3  IndexError Not Solving -> IndexError: Invalid ...   \n",
       "\n",
       "                                            comments  \\\n",
       "0  Hi ! We use the Arrow columnar format under th...   \n",
       "1  Hi ! The audio data don't always exist as file...   \n",
       "2  Thanks @lhoestq, I wasn't aware of the decode ...   \n",
       "3  https://colab.research.google.com/#scrollTo=AQ...   \n",
       "\n",
       "                                                body  \n",
       "0  ### Feature request\\n\\nI notice that when load...  \n",
       "1  ### Feature request\\r\\n\\r\\nReturn the audio fi...  \n",
       "2  ### Feature request\\r\\n\\r\\nReturn the audio fi...  \n",
       "3  ### Describe the bug\\n\\nin <cell line: 1>:1   ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explode the dataframe to have one row per comment\n",
    "comments_df = issues_df.explode(\"comments\", ignore_index=True)\n",
    "comments_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "html_url    0\n",
       "title       0\n",
       "comments    0\n",
       "body        6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for any null values in any of the columns of the dataframe\n",
    "comments_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Support for data with instance-wise dictionary...</td>\n",
       "      <td>Hi ! We use the Arrow columnar format under th...</td>\n",
       "      <td>### Feature request\\n\\nI notice that when load...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Return the audio filename when decoding fails ...</td>\n",
       "      <td>Hi ! The audio data don't always exist as file...</td>\n",
       "      <td>### Feature request\\r\\n\\r\\nReturn the audio fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Return the audio filename when decoding fails ...</td>\n",
       "      <td>Thanks @lhoestq, I wasn't aware of the decode ...</td>\n",
       "      <td>### Feature request\\r\\n\\r\\nReturn the audio fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>IndexError Not Solving -&gt; IndexError: Invalid ...</td>\n",
       "      <td>https://colab.research.google.com/#scrollTo=AQ...</td>\n",
       "      <td>### Describe the bug\\n\\nin &lt;cell line: 1&gt;:1   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>IndexError Not Solving -&gt; IndexError: Invalid ...</td>\n",
       "      <td>Looks related to https://discuss.huggingface.c...</td>\n",
       "      <td>### Describe the bug\\n\\nin &lt;cell line: 1&gt;:1   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5645</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>making sure datasets are not loaded in memory ...</td>\n",
       "      <td>Hi!  You can use the `assert not bool(dataset....</td>\n",
       "      <td>Hi\\r\\nI am dealing with large-scale datasets w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5646</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>sample multiple datasets</td>\n",
       "      <td>here I share my dataloader currently for multi...</td>\n",
       "      <td>Hi\\r\\nI am dealing with multiple datasets, I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5647</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>sample multiple datasets</td>\n",
       "      <td>Hi @rabeehkarimimahabadi any luck with updatin...</td>\n",
       "      <td>Hi\\r\\nI am dealing with multiple datasets, I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5648</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>sample multiple datasets</td>\n",
       "      <td>Hi @pushkalkatara yes I solved it back then, h...</td>\n",
       "      <td>Hi\\r\\nI am dealing with multiple datasets, I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5649</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>sample multiple datasets</td>\n",
       "      <td>Thanks @rabeehk for sharing. \\r\\n\\r\\nThe sampl...</td>\n",
       "      <td>Hi\\r\\nI am dealing with multiple datasets, I n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5644 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               html_url  \\\n",
       "0     https://github.com/huggingface/datasets/issues...   \n",
       "1     https://github.com/huggingface/datasets/issues...   \n",
       "2     https://github.com/huggingface/datasets/issues...   \n",
       "3     https://github.com/huggingface/datasets/issues...   \n",
       "4     https://github.com/huggingface/datasets/issues...   \n",
       "...                                                 ...   \n",
       "5645  https://github.com/huggingface/datasets/issues...   \n",
       "5646  https://github.com/huggingface/datasets/issues...   \n",
       "5647  https://github.com/huggingface/datasets/issues...   \n",
       "5648  https://github.com/huggingface/datasets/issues...   \n",
       "5649  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Support for data with instance-wise dictionary...   \n",
       "1     Return the audio filename when decoding fails ...   \n",
       "2     Return the audio filename when decoding fails ...   \n",
       "3     IndexError Not Solving -> IndexError: Invalid ...   \n",
       "4     IndexError Not Solving -> IndexError: Invalid ...   \n",
       "...                                                 ...   \n",
       "5645  making sure datasets are not loaded in memory ...   \n",
       "5646                          sample multiple datasets    \n",
       "5647                          sample multiple datasets    \n",
       "5648                          sample multiple datasets    \n",
       "5649                          sample multiple datasets    \n",
       "\n",
       "                                               comments  \\\n",
       "0     Hi ! We use the Arrow columnar format under th...   \n",
       "1     Hi ! The audio data don't always exist as file...   \n",
       "2     Thanks @lhoestq, I wasn't aware of the decode ...   \n",
       "3     https://colab.research.google.com/#scrollTo=AQ...   \n",
       "4     Looks related to https://discuss.huggingface.c...   \n",
       "...                                                 ...   \n",
       "5645  Hi!  You can use the `assert not bool(dataset....   \n",
       "5646  here I share my dataloader currently for multi...   \n",
       "5647  Hi @rabeehkarimimahabadi any luck with updatin...   \n",
       "5648  Hi @pushkalkatara yes I solved it back then, h...   \n",
       "5649  Thanks @rabeehk for sharing. \\r\\n\\r\\nThe sampl...   \n",
       "\n",
       "                                                   body  \n",
       "0     ### Feature request\\n\\nI notice that when load...  \n",
       "1     ### Feature request\\r\\n\\r\\nReturn the audio fi...  \n",
       "2     ### Feature request\\r\\n\\r\\nReturn the audio fi...  \n",
       "3     ### Describe the bug\\n\\nin <cell line: 1>:1   ...  \n",
       "4     ### Describe the bug\\n\\nin <cell line: 1>:1   ...  \n",
       "...                                                 ...  \n",
       "5645  Hi\\r\\nI am dealing with large-scale datasets w...  \n",
       "5646  Hi\\r\\nI am dealing with multiple datasets, I n...  \n",
       "5647  Hi\\r\\nI am dealing with multiple datasets, I n...  \n",
       "5648  Hi\\r\\nI am dealing with multiple datasets, I n...  \n",
       "5649  Hi\\r\\nI am dealing with multiple datasets, I n...  \n",
       "\n",
       "[5644 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove any rows with null values\n",
    "comments_df = comments_df.dropna()\n",
    "comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "html_url    0\n",
       "title       0\n",
       "comments    0\n",
       "body        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for any null values in any of the columns of the dataframe\n",
    "comments_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', '__index_level_0__'],\n",
       "    num_rows: 5644\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# done with pandas dataframe, convert back to HuggingFace dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aaf815e44c24c0da5df1d68ef602e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a new comments_length column to store the number of words per comment\n",
    "comments_dataset = comments_dataset.map(lambda row: {\"comments_length\": len(row[\"comments\"].split())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3addaba242e547b1a154e3f1dfb7bc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', '__index_level_0__', 'comments_length'],\n",
       "    num_rows: 4171\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the new column to filter out comments with less than 15 words\n",
    "comments_dataset = comments_dataset.filter(lambda row: row[\"comments_length\"] > 15)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to concatenate the title, body and comments columns into a single text column\n",
    "def concatenate_columns(row):\n",
    "    return {\"text\": row[\"title\"] + \" \\n \" + row[\"body\"] + \" \\n \" + row[\"comments\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97f889291e946399231f00354dd1a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4171 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', '__index_level_0__', 'comments_length', 'text'],\n",
       "    num_rows: 4171\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(concatenate_columns)\n",
    "comments_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the sentence-transformers library to encode our text into vectors\n",
    "# since we are using asymetric semantic search, short query whose answer we would like to find in a longer document\n",
    "# we will use the multi-qa-mpnet-base-dot-vi model which is trained on the Natural Questions dataset and has the best performance for semantic search\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# load the tokenizer with the same checkpoint as the model\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# speed up the embedding process by using the GPU\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a CLS pooling function on our model's output to get a single vector representation of the text\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to tokenize our text and convert it to tensors. Place tensors on the GPU, feed them to the model and pool the output\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(text_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    encoded_input = {key: value.to(device) for key, value in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function on a sample text from our corpus and inspect the output shape\n",
    "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6858a78d2f4d9ea43c214a50944818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4171 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use dataset.map() to apply the function to all the text in our dataset and put that in a new embeddings column\n",
    "# we will also convert the embeddings to numpy arrays because datasets can't store tensors and requires numpy arrays to index the embeddings with FAISS\n",
    "embeddings_dataset = comments_dataset.map(lambda row: {\"embeddings\": get_embeddings(row[\"text\"]).detach().cpu().numpy()[0]})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use FAISS for efficient similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\raj\\repos\\hf-nlp\\.venv\\lib\\site-packages (1.7.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135c0d1ef8454d45841527522cfe9fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', '__index_level_0__', 'comments_length', 'text', 'embeddings'],\n",
       "    num_rows: 4171\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a FAISS index and add the embeddings to it\n",
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform a semantic search on the index with a query\n",
    "query = \"How can I load a dataset offline?\"\n",
    "query_embedding = get_embeddings([query]).cpu().detach().numpy()\n",
    "query_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a similarity search on the index with the query embedding\n",
    "scores, retrieved_examples = embeddings_dataset.get_nearest_examples(\"embeddings\", query_embedding, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41.922493, 41.922493, 42.686325, 43.964485, 43.964485],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'html_url': ['https://github.com/huggingface/datasets/issues/2532',\n",
       "  'https://github.com/huggingface/datasets/issues/2532',\n",
       "  'https://github.com/huggingface/datasets/issues/1167',\n",
       "  'https://github.com/huggingface/datasets/issues/1830',\n",
       "  'https://github.com/huggingface/datasets/issues/1830'],\n",
       " 'title': [\"Tokenizer's normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task\",\n",
       "  \"Tokenizer's normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task\",\n",
       "  '❓ On-the-fly tokenization with datasets, tokenizers, and torch Datasets and Dataloaders',\n",
       "  'using map on loaded Tokenizer 10x - 100x slower than default Tokenizer?',\n",
       "  'using map on loaded Tokenizer 10x - 100x slower than default Tokenizer?'],\n",
       " 'comments': ['Hi @jerryIsHere, thanks for reporting the issue. But are you sure this is a bug in HuggingFace **Datasets**?',\n",
       "  '> Hi @jerryIsHere, thanks for reporting the issue. But are you sure this is a bug in HuggingFace **Datasets**?\\r\\n\\r\\nOh, I am sorry\\r\\nI would reopen the post on huggingface/transformers',\n",
       "  'We\\'re working on adding on-the-fly transforms in datasets.\\r\\nCurrently the only on-the-fly functions that can be applied are in `set_format` in which we transform the data in either numpy/torch/tf tensors or pandas.\\r\\nFor example\\r\\n```python\\r\\ndataset.set_format(\"torch\")\\r\\n```\\r\\napplies `torch.Tensor` to the dataset entries on-the-fly.\\r\\n\\r\\nWe plan to extend this to user-defined formatting transforms.\\r\\nFor example\\r\\n```python\\r\\ndataset.set_format(transform=tokenize)\\r\\n```\\r\\n\\r\\nWhat do you think ?',\n",
       "  \"Hi @wumpusman \\r\\n`datasets` has a caching mechanism that allows to cache the results of `.map` so that when you want to re-run it later it doesn't recompute it again.\\r\\nSo when you do `.map`, what actually happens is:\\r\\n1. compute the hash used to identify your `map` for the cache\\r\\n2. apply your function on every batch\\r\\n\\r\\nThis can explain the time difference between your different experiments.\\r\\n\\r\\nThe hash computation time depends of how complex your function is. For a tokenizer, the hash computation scans the lists of the words in the tokenizer to identify this tokenizer. Usually it takes 2-3 seconds.\\r\\n\\r\\nAlso note that you can disable caching though using\\r\\n```python\\r\\nimport datasets\\r\\n\\r\\ndatasets.set_caching_enabled(False)\\r\\n```\",\n",
       "  \"Hi @lhoestq ,\\r\\n\\r\\nThanks for the reply. It's entirely possible that is the issue. Since it's a side project I won't be looking at it till later this week, but, I'll verify it by disabling caching and hopefully I'll see the same runtime. \\r\\n\\r\\nAppreciate the reference,\\r\\n\\r\\nMichael\"],\n",
       " 'body': ['[This colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) implements a token classification input pipeline extending the logic from [this hugging example](https://huggingface.co/transformers/custom_datasets.html#tok-ner).\\r\\n\\r\\nThe pipeline works fine with most instance in different languages, but unfortunately, [the Japanese Kana ligature (a form of abbreviation? I don\\'t know Japanese well)](https://en.wikipedia.org/wiki/Kana_ligature) break the alignment of `return_offsets_mapping`:\\r\\n![image](https://user-images.githubusercontent.com/50871412/122904371-db192700-d382-11eb-8917-1775db76db69.png)\\r\\n\\r\\nWithout the try catch block, it riase `ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true`, example shown here [(another colab notebook)](https://colab.research.google.com/drive/1MmOqf3ppzzdKKyMWkn0bJy6DqzOO0SSm?usp=sharing)\\r\\n\\r\\nIt is clear that the normalizer is the process that break the alignment, as it is observed that `tokenizer._tokenizer.normalizer.normalize_str(\\'ヿ\\')` return \\'コト\\'.\\r\\n\\r\\nOne workaround is to include `tokenizer._tokenizer.normalizer.normalize_str` before the tokenizer preprocessing pipeline, which is also provided in the [first colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) with the name `udposTestDatasetWorkaround`.\\r\\n\\r\\nI guess similar logics should be included inside the tokenizer and the offsets_mapping generation process such that user don\\'t need to include them in their code. But I don\\'t understand the code of tokenizer well that I think I am not able to do this.\\r\\n\\r\\np.s.\\r\\n**I am using my own dataset building script in the provided example, but the script should be equivalent to the changes made by this [update](https://github.com/huggingface/datasets/pull/2466)**\\r\\n`get_dataset `is just a simple wrapping for `load_dataset`\\r\\nand the `tokenizer` is just `XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")`',\n",
       "  '[This colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) implements a token classification input pipeline extending the logic from [this hugging example](https://huggingface.co/transformers/custom_datasets.html#tok-ner).\\r\\n\\r\\nThe pipeline works fine with most instance in different languages, but unfortunately, [the Japanese Kana ligature (a form of abbreviation? I don\\'t know Japanese well)](https://en.wikipedia.org/wiki/Kana_ligature) break the alignment of `return_offsets_mapping`:\\r\\n![image](https://user-images.githubusercontent.com/50871412/122904371-db192700-d382-11eb-8917-1775db76db69.png)\\r\\n\\r\\nWithout the try catch block, it riase `ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true`, example shown here [(another colab notebook)](https://colab.research.google.com/drive/1MmOqf3ppzzdKKyMWkn0bJy6DqzOO0SSm?usp=sharing)\\r\\n\\r\\nIt is clear that the normalizer is the process that break the alignment, as it is observed that `tokenizer._tokenizer.normalizer.normalize_str(\\'ヿ\\')` return \\'コト\\'.\\r\\n\\r\\nOne workaround is to include `tokenizer._tokenizer.normalizer.normalize_str` before the tokenizer preprocessing pipeline, which is also provided in the [first colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) with the name `udposTestDatasetWorkaround`.\\r\\n\\r\\nI guess similar logics should be included inside the tokenizer and the offsets_mapping generation process such that user don\\'t need to include them in their code. But I don\\'t understand the code of tokenizer well that I think I am not able to do this.\\r\\n\\r\\np.s.\\r\\n**I am using my own dataset building script in the provided example, but the script should be equivalent to the changes made by this [update](https://github.com/huggingface/datasets/pull/2466)**\\r\\n`get_dataset `is just a simple wrapping for `load_dataset`\\r\\nand the `tokenizer` is just `XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")`',\n",
       "  'Hi there,\\r\\n\\r\\nI have a question regarding \"on-the-fly\" tokenization. This question was elicited by reading the \"How to train a new language model from scratch using Transformers and Tokenizers\" [here](https://huggingface.co/blog/how-to-train). Towards the end there is this sentence: \"If your dataset is very large, you can opt to load and tokenize examples on the fly, rather than as a preprocessing step\". I\\'ve tried coming up with a solution that would combine both `datasets` and `tokenizers`, but did not manage to find a good pattern.\\r\\n\\r\\nI guess the solution would entail wrapping a dataset into a Pytorch dataset.\\r\\n\\r\\nAs a concrete example from the [docs](https://huggingface.co/transformers/custom_datasets.html)\\r\\n\\r\\n```python\\r\\nimport torch\\r\\n\\r\\nclass SquadDataset(torch.utils.data.Dataset):\\r\\n    def __init__(self, encodings):\\r\\n        # instead of doing this beforehand, I\\'d like to do tokenization on the fly\\r\\n        self.encodings = encodings \\r\\n\\r\\n    def __getitem__(self, idx):\\r\\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.encodings.input_ids)\\r\\n\\r\\ntrain_dataset = SquadDataset(train_encodings)\\r\\n```\\r\\n\\r\\nHow would one implement this with \"on-the-fly\" tokenization exploiting the vectorized capabilities of tokenizers?\\r\\n\\r\\n\\r\\n----\\r\\n\\r\\nEdit: I have come up with this solution. It does what I want, but I feel it\\'s not very elegant\\r\\n\\r\\n```python\\r\\nclass CustomPytorchDataset(Dataset):\\r\\n    def __init__(self):\\r\\n        self.dataset = some_hf_dataset(...)\\r\\n        self.tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\\r\\n\\r\\n    def __getitem__(self, batch_idx):\\r\\n        instance = self.dataset[text_col][batch_idx]\\r\\n        tokenized_text = self.tokenizer(instance, truncation=True, padding=True)\\r\\n        return tokenized_text\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.dataset)\\r\\n\\r\\n    @staticmethod\\r\\n    def collate_fn(batch):\\r\\n        # batch is a list, however it will always contain 1 item because we should not use the\\r\\n        # batch_size argument as batch_size is controlled by the sampler\\r\\n        return {k: torch.tensor(v) for k, v in batch[0].items()}\\r\\n\\r\\ntorch_ds = CustomPytorchDataset()\\r\\n\\r\\n# NOTE: batch_sampler returns list of integers and since here we have SequentialSampler\\r\\n# it returns: [1, 2, 3], [4, 5, 6], etc. - check calling `list(batch_sampler)`\\r\\nbatch_sampler = BatchSampler(SequentialSampler(torch_ds), batch_size=3, drop_last=True)\\r\\n\\r\\n# NOTE: no `batch_size` as now the it is controlled by the sampler!\\r\\ndl = DataLoader(dataset=torch_ds, sampler=batch_sampler, collate_fn=torch_ds.collate_fn)\\r\\n```',\n",
       "  'This could total relate to me misunderstanding particular call functions, but I added words to a GPT2Tokenizer, and saved it to disk (note I\\'m only showing snippets but I can share more) and the map function ran much slower: \\r\\n\\r\\n````\\r\\ndef save_tokenizer(original_tokenizer,text,path=\"simpledata/tokenizer\"):\\r\\n    words_unique = set(text.split(\" \"))\\r\\n    for i in words_unique:\\r\\n        original_tokenizer.add_tokens(i)\\r\\n    original_tokenizer.save_pretrained(path)\\r\\n\\r\\ntokenizer2 = GPT2Tokenizer.from_pretrained(os.path.join(experiment_path,experiment_name,\"tokenizer_squad\"))\\r\\n\\r\\ntrain_set_baby=Dataset.from_dict({\"text\":[train_set[\"text\"][0][0:50]]})\\r\\n````\\r\\n\\r\\nI then applied the dataset map function on a fairly small set of text:\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\n\\r\\n\\r\\nThe run time for train_set_baby.map was 6 seconds, and the batch itself was 2.6 seconds\\r\\n\\r\\n**100% 1/1 [00:02<00:00, 2.60s/ba] CPU times: user 5.96 s, sys: 36 ms, total: 5.99 s Wall time: 5.99 s**\\r\\n\\r\\nIn comparison using (even after adding additional tokens): \\r\\n`\\r\\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")`\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\nThe time is \\r\\n**100% 1/1 [00:00<00:00, 34.09ba/s] CPU times: user 68.1 ms, sys: 16 µs, total: 68.1 ms Wall time: 62.9 ms**\\r\\n\\r\\nIt seems this might relate to the tokenizer save or load function, however, the issue appears to come up when I apply the loaded tokenizer  to the map function. \\r\\n\\r\\nI should also add that playing around with the amount of words I add to the tokenizer before I save it to disk and load it into memory  appears to impact the time it takes to run the map function. \\r\\n\\r\\n\\r\\n',\n",
       "  'This could total relate to me misunderstanding particular call functions, but I added words to a GPT2Tokenizer, and saved it to disk (note I\\'m only showing snippets but I can share more) and the map function ran much slower: \\r\\n\\r\\n````\\r\\ndef save_tokenizer(original_tokenizer,text,path=\"simpledata/tokenizer\"):\\r\\n    words_unique = set(text.split(\" \"))\\r\\n    for i in words_unique:\\r\\n        original_tokenizer.add_tokens(i)\\r\\n    original_tokenizer.save_pretrained(path)\\r\\n\\r\\ntokenizer2 = GPT2Tokenizer.from_pretrained(os.path.join(experiment_path,experiment_name,\"tokenizer_squad\"))\\r\\n\\r\\ntrain_set_baby=Dataset.from_dict({\"text\":[train_set[\"text\"][0][0:50]]})\\r\\n````\\r\\n\\r\\nI then applied the dataset map function on a fairly small set of text:\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\n\\r\\n\\r\\nThe run time for train_set_baby.map was 6 seconds, and the batch itself was 2.6 seconds\\r\\n\\r\\n**100% 1/1 [00:02<00:00, 2.60s/ba] CPU times: user 5.96 s, sys: 36 ms, total: 5.99 s Wall time: 5.99 s**\\r\\n\\r\\nIn comparison using (even after adding additional tokens): \\r\\n`\\r\\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")`\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\nThe time is \\r\\n**100% 1/1 [00:00<00:00, 34.09ba/s] CPU times: user 68.1 ms, sys: 16 µs, total: 68.1 ms Wall time: 62.9 ms**\\r\\n\\r\\nIt seems this might relate to the tokenizer save or load function, however, the issue appears to come up when I apply the loaded tokenizer  to the map function. \\r\\n\\r\\nI should also add that playing around with the amount of words I add to the tokenizer before I save it to disk and load it into memory  appears to impact the time it takes to run the map function. \\r\\n\\r\\n\\r\\n'],\n",
       " '__index_level_0__': [4335, 4336, 5619, 5249, 5250],\n",
       " 'comments_length': [18, 30, 63, 116, 47],\n",
       " 'text': ['Tokenizer\\'s normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task \\n [This colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) implements a token classification input pipeline extending the logic from [this hugging example](https://huggingface.co/transformers/custom_datasets.html#tok-ner).\\r\\n\\r\\nThe pipeline works fine with most instance in different languages, but unfortunately, [the Japanese Kana ligature (a form of abbreviation? I don\\'t know Japanese well)](https://en.wikipedia.org/wiki/Kana_ligature) break the alignment of `return_offsets_mapping`:\\r\\n![image](https://user-images.githubusercontent.com/50871412/122904371-db192700-d382-11eb-8917-1775db76db69.png)\\r\\n\\r\\nWithout the try catch block, it riase `ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true`, example shown here [(another colab notebook)](https://colab.research.google.com/drive/1MmOqf3ppzzdKKyMWkn0bJy6DqzOO0SSm?usp=sharing)\\r\\n\\r\\nIt is clear that the normalizer is the process that break the alignment, as it is observed that `tokenizer._tokenizer.normalizer.normalize_str(\\'ヿ\\')` return \\'コト\\'.\\r\\n\\r\\nOne workaround is to include `tokenizer._tokenizer.normalizer.normalize_str` before the tokenizer preprocessing pipeline, which is also provided in the [first colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) with the name `udposTestDatasetWorkaround`.\\r\\n\\r\\nI guess similar logics should be included inside the tokenizer and the offsets_mapping generation process such that user don\\'t need to include them in their code. But I don\\'t understand the code of tokenizer well that I think I am not able to do this.\\r\\n\\r\\np.s.\\r\\n**I am using my own dataset building script in the provided example, but the script should be equivalent to the changes made by this [update](https://github.com/huggingface/datasets/pull/2466)**\\r\\n`get_dataset `is just a simple wrapping for `load_dataset`\\r\\nand the `tokenizer` is just `XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")` \\n Hi @jerryIsHere, thanks for reporting the issue. But are you sure this is a bug in HuggingFace **Datasets**?',\n",
       "  'Tokenizer\\'s normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task \\n [This colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) implements a token classification input pipeline extending the logic from [this hugging example](https://huggingface.co/transformers/custom_datasets.html#tok-ner).\\r\\n\\r\\nThe pipeline works fine with most instance in different languages, but unfortunately, [the Japanese Kana ligature (a form of abbreviation? I don\\'t know Japanese well)](https://en.wikipedia.org/wiki/Kana_ligature) break the alignment of `return_offsets_mapping`:\\r\\n![image](https://user-images.githubusercontent.com/50871412/122904371-db192700-d382-11eb-8917-1775db76db69.png)\\r\\n\\r\\nWithout the try catch block, it riase `ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true`, example shown here [(another colab notebook)](https://colab.research.google.com/drive/1MmOqf3ppzzdKKyMWkn0bJy6DqzOO0SSm?usp=sharing)\\r\\n\\r\\nIt is clear that the normalizer is the process that break the alignment, as it is observed that `tokenizer._tokenizer.normalizer.normalize_str(\\'ヿ\\')` return \\'コト\\'.\\r\\n\\r\\nOne workaround is to include `tokenizer._tokenizer.normalizer.normalize_str` before the tokenizer preprocessing pipeline, which is also provided in the [first colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) with the name `udposTestDatasetWorkaround`.\\r\\n\\r\\nI guess similar logics should be included inside the tokenizer and the offsets_mapping generation process such that user don\\'t need to include them in their code. But I don\\'t understand the code of tokenizer well that I think I am not able to do this.\\r\\n\\r\\np.s.\\r\\n**I am using my own dataset building script in the provided example, but the script should be equivalent to the changes made by this [update](https://github.com/huggingface/datasets/pull/2466)**\\r\\n`get_dataset `is just a simple wrapping for `load_dataset`\\r\\nand the `tokenizer` is just `XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")` \\n > Hi @jerryIsHere, thanks for reporting the issue. But are you sure this is a bug in HuggingFace **Datasets**?\\r\\n\\r\\nOh, I am sorry\\r\\nI would reopen the post on huggingface/transformers',\n",
       "  '❓ On-the-fly tokenization with datasets, tokenizers, and torch Datasets and Dataloaders \\n Hi there,\\r\\n\\r\\nI have a question regarding \"on-the-fly\" tokenization. This question was elicited by reading the \"How to train a new language model from scratch using Transformers and Tokenizers\" [here](https://huggingface.co/blog/how-to-train). Towards the end there is this sentence: \"If your dataset is very large, you can opt to load and tokenize examples on the fly, rather than as a preprocessing step\". I\\'ve tried coming up with a solution that would combine both `datasets` and `tokenizers`, but did not manage to find a good pattern.\\r\\n\\r\\nI guess the solution would entail wrapping a dataset into a Pytorch dataset.\\r\\n\\r\\nAs a concrete example from the [docs](https://huggingface.co/transformers/custom_datasets.html)\\r\\n\\r\\n```python\\r\\nimport torch\\r\\n\\r\\nclass SquadDataset(torch.utils.data.Dataset):\\r\\n    def __init__(self, encodings):\\r\\n        # instead of doing this beforehand, I\\'d like to do tokenization on the fly\\r\\n        self.encodings = encodings \\r\\n\\r\\n    def __getitem__(self, idx):\\r\\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.encodings.input_ids)\\r\\n\\r\\ntrain_dataset = SquadDataset(train_encodings)\\r\\n```\\r\\n\\r\\nHow would one implement this with \"on-the-fly\" tokenization exploiting the vectorized capabilities of tokenizers?\\r\\n\\r\\n\\r\\n----\\r\\n\\r\\nEdit: I have come up with this solution. It does what I want, but I feel it\\'s not very elegant\\r\\n\\r\\n```python\\r\\nclass CustomPytorchDataset(Dataset):\\r\\n    def __init__(self):\\r\\n        self.dataset = some_hf_dataset(...)\\r\\n        self.tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\\r\\n\\r\\n    def __getitem__(self, batch_idx):\\r\\n        instance = self.dataset[text_col][batch_idx]\\r\\n        tokenized_text = self.tokenizer(instance, truncation=True, padding=True)\\r\\n        return tokenized_text\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.dataset)\\r\\n\\r\\n    @staticmethod\\r\\n    def collate_fn(batch):\\r\\n        # batch is a list, however it will always contain 1 item because we should not use the\\r\\n        # batch_size argument as batch_size is controlled by the sampler\\r\\n        return {k: torch.tensor(v) for k, v in batch[0].items()}\\r\\n\\r\\ntorch_ds = CustomPytorchDataset()\\r\\n\\r\\n# NOTE: batch_sampler returns list of integers and since here we have SequentialSampler\\r\\n# it returns: [1, 2, 3], [4, 5, 6], etc. - check calling `list(batch_sampler)`\\r\\nbatch_sampler = BatchSampler(SequentialSampler(torch_ds), batch_size=3, drop_last=True)\\r\\n\\r\\n# NOTE: no `batch_size` as now the it is controlled by the sampler!\\r\\ndl = DataLoader(dataset=torch_ds, sampler=batch_sampler, collate_fn=torch_ds.collate_fn)\\r\\n``` \\n We\\'re working on adding on-the-fly transforms in datasets.\\r\\nCurrently the only on-the-fly functions that can be applied are in `set_format` in which we transform the data in either numpy/torch/tf tensors or pandas.\\r\\nFor example\\r\\n```python\\r\\ndataset.set_format(\"torch\")\\r\\n```\\r\\napplies `torch.Tensor` to the dataset entries on-the-fly.\\r\\n\\r\\nWe plan to extend this to user-defined formatting transforms.\\r\\nFor example\\r\\n```python\\r\\ndataset.set_format(transform=tokenize)\\r\\n```\\r\\n\\r\\nWhat do you think ?',\n",
       "  'using map on loaded Tokenizer 10x - 100x slower than default Tokenizer? \\n This could total relate to me misunderstanding particular call functions, but I added words to a GPT2Tokenizer, and saved it to disk (note I\\'m only showing snippets but I can share more) and the map function ran much slower: \\r\\n\\r\\n````\\r\\ndef save_tokenizer(original_tokenizer,text,path=\"simpledata/tokenizer\"):\\r\\n    words_unique = set(text.split(\" \"))\\r\\n    for i in words_unique:\\r\\n        original_tokenizer.add_tokens(i)\\r\\n    original_tokenizer.save_pretrained(path)\\r\\n\\r\\ntokenizer2 = GPT2Tokenizer.from_pretrained(os.path.join(experiment_path,experiment_name,\"tokenizer_squad\"))\\r\\n\\r\\ntrain_set_baby=Dataset.from_dict({\"text\":[train_set[\"text\"][0][0:50]]})\\r\\n````\\r\\n\\r\\nI then applied the dataset map function on a fairly small set of text:\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\n\\r\\n\\r\\nThe run time for train_set_baby.map was 6 seconds, and the batch itself was 2.6 seconds\\r\\n\\r\\n**100% 1/1 [00:02<00:00, 2.60s/ba] CPU times: user 5.96 s, sys: 36 ms, total: 5.99 s Wall time: 5.99 s**\\r\\n\\r\\nIn comparison using (even after adding additional tokens): \\r\\n`\\r\\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")`\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\nThe time is \\r\\n**100% 1/1 [00:00<00:00, 34.09ba/s] CPU times: user 68.1 ms, sys: 16 µs, total: 68.1 ms Wall time: 62.9 ms**\\r\\n\\r\\nIt seems this might relate to the tokenizer save or load function, however, the issue appears to come up when I apply the loaded tokenizer  to the map function. \\r\\n\\r\\nI should also add that playing around with the amount of words I add to the tokenizer before I save it to disk and load it into memory  appears to impact the time it takes to run the map function. \\r\\n\\r\\n\\r\\n \\n Hi @wumpusman \\r\\n`datasets` has a caching mechanism that allows to cache the results of `.map` so that when you want to re-run it later it doesn\\'t recompute it again.\\r\\nSo when you do `.map`, what actually happens is:\\r\\n1. compute the hash used to identify your `map` for the cache\\r\\n2. apply your function on every batch\\r\\n\\r\\nThis can explain the time difference between your different experiments.\\r\\n\\r\\nThe hash computation time depends of how complex your function is. For a tokenizer, the hash computation scans the lists of the words in the tokenizer to identify this tokenizer. Usually it takes 2-3 seconds.\\r\\n\\r\\nAlso note that you can disable caching though using\\r\\n```python\\r\\nimport datasets\\r\\n\\r\\ndatasets.set_caching_enabled(False)\\r\\n```',\n",
       "  'using map on loaded Tokenizer 10x - 100x slower than default Tokenizer? \\n This could total relate to me misunderstanding particular call functions, but I added words to a GPT2Tokenizer, and saved it to disk (note I\\'m only showing snippets but I can share more) and the map function ran much slower: \\r\\n\\r\\n````\\r\\ndef save_tokenizer(original_tokenizer,text,path=\"simpledata/tokenizer\"):\\r\\n    words_unique = set(text.split(\" \"))\\r\\n    for i in words_unique:\\r\\n        original_tokenizer.add_tokens(i)\\r\\n    original_tokenizer.save_pretrained(path)\\r\\n\\r\\ntokenizer2 = GPT2Tokenizer.from_pretrained(os.path.join(experiment_path,experiment_name,\"tokenizer_squad\"))\\r\\n\\r\\ntrain_set_baby=Dataset.from_dict({\"text\":[train_set[\"text\"][0][0:50]]})\\r\\n````\\r\\n\\r\\nI then applied the dataset map function on a fairly small set of text:\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\n\\r\\n\\r\\nThe run time for train_set_baby.map was 6 seconds, and the batch itself was 2.6 seconds\\r\\n\\r\\n**100% 1/1 [00:02<00:00, 2.60s/ba] CPU times: user 5.96 s, sys: 36 ms, total: 5.99 s Wall time: 5.99 s**\\r\\n\\r\\nIn comparison using (even after adding additional tokens): \\r\\n`\\r\\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")`\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\nThe time is \\r\\n**100% 1/1 [00:00<00:00, 34.09ba/s] CPU times: user 68.1 ms, sys: 16 µs, total: 68.1 ms Wall time: 62.9 ms**\\r\\n\\r\\nIt seems this might relate to the tokenizer save or load function, however, the issue appears to come up when I apply the loaded tokenizer  to the map function. \\r\\n\\r\\nI should also add that playing around with the amount of words I add to the tokenizer before I save it to disk and load it into memory  appears to impact the time it takes to run the map function. \\r\\n\\r\\n\\r\\n \\n Hi @lhoestq ,\\r\\n\\r\\nThanks for the reply. It\\'s entirely possible that is the issue. Since it\\'s a side project I won\\'t be looking at it till later this week, but, I\\'ll verify it by disabling caching and hopefully I\\'ll see the same runtime. \\r\\n\\r\\nAppreciate the reference,\\r\\n\\r\\nMichael'],\n",
       " 'embeddings': [[-0.15369662642478943,\n",
       "   0.0790129229426384,\n",
       "   0.07378190755844116,\n",
       "   -0.03823181986808777,\n",
       "   0.09215767681598663,\n",
       "   -0.32468143105506897,\n",
       "   -0.08998150378465652,\n",
       "   0.10603199899196625,\n",
       "   -0.4617609679698944,\n",
       "   0.19867387413978577,\n",
       "   -0.20853732526302338,\n",
       "   0.30742037296295166,\n",
       "   0.27667680382728577,\n",
       "   0.030884919688105583,\n",
       "   -0.2191910743713379,\n",
       "   -0.16327746212482452,\n",
       "   0.1479930579662323,\n",
       "   0.219057098031044,\n",
       "   0.00562308682128787,\n",
       "   0.06112286448478699,\n",
       "   -0.1194193884730339,\n",
       "   0.45713841915130615,\n",
       "   -0.21233554184436798,\n",
       "   0.21436016261577606,\n",
       "   -0.28662627935409546,\n",
       "   -0.07063654065132141,\n",
       "   0.12557756900787354,\n",
       "   -0.31564846634864807,\n",
       "   -0.1683976948261261,\n",
       "   -0.3988146185874939,\n",
       "   0.2497144192457199,\n",
       "   0.11891039460897446,\n",
       "   -0.08367154747247696,\n",
       "   0.20431433618068695,\n",
       "   -0.00011269692913629115,\n",
       "   -0.05750177428126335,\n",
       "   -0.018035506829619408,\n",
       "   0.014288807287812233,\n",
       "   0.17867527902126312,\n",
       "   -0.19643253087997437,\n",
       "   0.12838613986968994,\n",
       "   -0.376918762922287,\n",
       "   -0.3356097638607025,\n",
       "   0.1073894202709198,\n",
       "   -0.06366998702287674,\n",
       "   0.039610855281353,\n",
       "   0.24081267416477203,\n",
       "   -0.05541888251900673,\n",
       "   0.6431405544281006,\n",
       "   0.41514983773231506,\n",
       "   0.14555539190769196,\n",
       "   -0.013489042408764362,\n",
       "   0.25451838970184326,\n",
       "   0.06076836958527565,\n",
       "   -0.31092360615730286,\n",
       "   0.2689674198627472,\n",
       "   -0.09506773203611374,\n",
       "   0.01900981180369854,\n",
       "   -0.15407243371009827,\n",
       "   0.031468506902456284,\n",
       "   -0.07465161383152008,\n",
       "   -0.009617595002055168,\n",
       "   -0.09227008372545242,\n",
       "   -0.4802752137184143,\n",
       "   -0.014885332435369492,\n",
       "   0.11489637941122055,\n",
       "   0.3214079439640045,\n",
       "   -0.4625910818576813,\n",
       "   0.18707258999347687,\n",
       "   0.01853308454155922,\n",
       "   -0.24988877773284912,\n",
       "   -0.0450734905898571,\n",
       "   -0.18307730555534363,\n",
       "   -0.3914948105812073,\n",
       "   -0.13682639598846436,\n",
       "   -0.26799407601356506,\n",
       "   0.20009689033031464,\n",
       "   -0.15426293015480042,\n",
       "   -0.1181238666176796,\n",
       "   0.11682857573032379,\n",
       "   0.01208376232534647,\n",
       "   0.13784609735012054,\n",
       "   0.029742859303951263,\n",
       "   -0.21203945577144623,\n",
       "   -0.2942862808704376,\n",
       "   0.5647349953651428,\n",
       "   -0.06541191041469574,\n",
       "   0.04264478012919426,\n",
       "   -0.20475590229034424,\n",
       "   -0.010176555253565311,\n",
       "   0.3918432295322418,\n",
       "   -0.04282088577747345,\n",
       "   0.005983348935842514,\n",
       "   0.437328964471817,\n",
       "   -0.2852553129196167,\n",
       "   -0.05852540582418442,\n",
       "   0.19706958532333374,\n",
       "   -0.4548269510269165,\n",
       "   -0.007932859472930431,\n",
       "   -0.035667333751916885,\n",
       "   0.12427128851413727,\n",
       "   0.2060960829257965,\n",
       "   0.07195869833230972,\n",
       "   0.060917071998119354,\n",
       "   -0.1630159616470337,\n",
       "   0.464278906583786,\n",
       "   0.2114512026309967,\n",
       "   0.27663567662239075,\n",
       "   0.19684246182441711,\n",
       "   0.11708666384220123,\n",
       "   -0.0398002564907074,\n",
       "   -0.04044095799326897,\n",
       "   0.06278236955404282,\n",
       "   -0.12514930963516235,\n",
       "   -0.04987660422921181,\n",
       "   0.2508595883846283,\n",
       "   -0.37355348467826843,\n",
       "   0.10397126525640488,\n",
       "   -0.23151284456253052,\n",
       "   0.4121430814266205,\n",
       "   -0.07186693698167801,\n",
       "   0.3000815808773041,\n",
       "   0.03386527672410011,\n",
       "   -0.09935843199491501,\n",
       "   0.03932022303342819,\n",
       "   0.28934115171432495,\n",
       "   -0.34031638503074646,\n",
       "   -0.10191559046506882,\n",
       "   -0.0826217532157898,\n",
       "   0.10865523666143417,\n",
       "   -0.19528396427631378,\n",
       "   0.01898312196135521,\n",
       "   -0.2947452664375305,\n",
       "   0.12470927834510803,\n",
       "   -0.19662198424339294,\n",
       "   0.108177550137043,\n",
       "   -0.02755059115588665,\n",
       "   0.12801189720630646,\n",
       "   0.3569863736629486,\n",
       "   -0.03269282355904579,\n",
       "   -0.08280514180660248,\n",
       "   -0.09475267678499222,\n",
       "   0.536851167678833,\n",
       "   0.27820247411727905,\n",
       "   0.026152536273002625,\n",
       "   0.1261873096227646,\n",
       "   0.11548148840665817,\n",
       "   -0.359840452671051,\n",
       "   -0.3748219311237335,\n",
       "   -0.043390970677137375,\n",
       "   -0.21579334139823914,\n",
       "   0.08999038487672806,\n",
       "   0.14790287613868713,\n",
       "   0.20128220319747925,\n",
       "   -0.10737680643796921,\n",
       "   0.134821355342865,\n",
       "   -0.3785093426704407,\n",
       "   0.28587764501571655,\n",
       "   0.33080118894577026,\n",
       "   -0.24652434885501862,\n",
       "   0.002981877885758877,\n",
       "   -0.36790862679481506,\n",
       "   -0.41353675723075867,\n",
       "   -0.1898319572210312,\n",
       "   0.3192931115627289,\n",
       "   -0.14425157010555267,\n",
       "   -0.0017331935232505202,\n",
       "   -0.009171119891107082,\n",
       "   0.11065865308046341,\n",
       "   0.18913918733596802,\n",
       "   0.484011173248291,\n",
       "   0.13396534323692322,\n",
       "   0.31385910511016846,\n",
       "   0.36314108967781067,\n",
       "   -0.4917096197605133,\n",
       "   0.48556509613990784,\n",
       "   0.0987912192940712,\n",
       "   -0.13364584743976593,\n",
       "   -0.08816645294427872,\n",
       "   -0.1546434760093689,\n",
       "   -0.3981320858001709,\n",
       "   0.2574174702167511,\n",
       "   -0.1778469979763031,\n",
       "   -0.28533002734184265,\n",
       "   0.14593149721622467,\n",
       "   0.1648341566324234,\n",
       "   0.16425158083438873,\n",
       "   0.46753373742103577,\n",
       "   -0.13454070687294006,\n",
       "   0.09679176658391953,\n",
       "   -0.08240294456481934,\n",
       "   0.3093528747558594,\n",
       "   -0.05164700001478195,\n",
       "   -0.26832032203674316,\n",
       "   0.01252705231308937,\n",
       "   -0.03489473834633827,\n",
       "   -0.230736643075943,\n",
       "   -0.23822236061096191,\n",
       "   -0.15970268845558167,\n",
       "   -0.08852948248386383,\n",
       "   0.10251940786838531,\n",
       "   -0.08720844984054565,\n",
       "   0.04389937222003937,\n",
       "   -0.12961752712726593,\n",
       "   0.023830508813261986,\n",
       "   -0.152864471077919,\n",
       "   0.04286302998661995,\n",
       "   -0.01160067692399025,\n",
       "   -0.36197882890701294,\n",
       "   0.2263038605451584,\n",
       "   0.006499935407191515,\n",
       "   -0.16657176613807678,\n",
       "   -0.24409866333007812,\n",
       "   0.17804349958896637,\n",
       "   -0.1699587106704712,\n",
       "   -0.13742013275623322,\n",
       "   0.22984519600868225,\n",
       "   0.06616337597370148,\n",
       "   0.1778305023908615,\n",
       "   0.33349093794822693,\n",
       "   0.055316127836704254,\n",
       "   0.299461305141449,\n",
       "   0.18764953315258026,\n",
       "   0.259883314371109,\n",
       "   -0.23794032633304596,\n",
       "   0.16552138328552246,\n",
       "   -0.15393280982971191,\n",
       "   0.03288349509239197,\n",
       "   -0.1076623871922493,\n",
       "   0.44957235455513,\n",
       "   0.057686492800712585,\n",
       "   -0.11599187552928925,\n",
       "   -0.12629830837249756,\n",
       "   0.22838307917118073,\n",
       "   0.2532852590084076,\n",
       "   -0.04426857829093933,\n",
       "   -0.28391075134277344,\n",
       "   0.08557260781526566,\n",
       "   0.14712095260620117,\n",
       "   0.1632165163755417,\n",
       "   -0.37553104758262634,\n",
       "   0.284096360206604,\n",
       "   -0.2077518254518509,\n",
       "   -0.12244737148284912,\n",
       "   0.5265040397644043,\n",
       "   0.8932770490646362,\n",
       "   -0.03842442110180855,\n",
       "   0.12675237655639648,\n",
       "   -0.14578308165073395,\n",
       "   -0.02217038907110691,\n",
       "   0.44586607813835144,\n",
       "   0.11814215779304504,\n",
       "   -0.15466345846652985,\n",
       "   -0.2694556713104248,\n",
       "   0.03720812499523163,\n",
       "   0.2975228726863861,\n",
       "   -0.10649247467517853,\n",
       "   0.025268638506531715,\n",
       "   -0.09403879195451736,\n",
       "   0.1421896517276764,\n",
       "   0.21936139464378357,\n",
       "   0.06661650538444519,\n",
       "   -0.22007596492767334,\n",
       "   0.1170540302991867,\n",
       "   0.004420798737555742,\n",
       "   -0.2848398983478546,\n",
       "   0.1736074984073639,\n",
       "   0.0940917357802391,\n",
       "   0.230821892619133,\n",
       "   0.04038390889763832,\n",
       "   0.15981221199035645,\n",
       "   0.27577751874923706,\n",
       "   -0.23048537969589233,\n",
       "   -0.16235174238681793,\n",
       "   0.1398642510175705,\n",
       "   0.015991086140275,\n",
       "   0.11915136873722076,\n",
       "   0.5081241726875305,\n",
       "   0.11125757545232773,\n",
       "   0.4917434751987457,\n",
       "   -0.2569989860057831,\n",
       "   -0.25069427490234375,\n",
       "   -0.3280733525753021,\n",
       "   0.046981632709503174,\n",
       "   -0.1642935574054718,\n",
       "   0.23284126818180084,\n",
       "   -0.20010194182395935,\n",
       "   -0.33812475204467773,\n",
       "   -0.41888493299484253,\n",
       "   -0.14505556225776672,\n",
       "   -0.11501073837280273,\n",
       "   -0.5577638745307922,\n",
       "   0.13810966908931732,\n",
       "   0.04132494330406189,\n",
       "   -0.27524101734161377,\n",
       "   0.4234623312950134,\n",
       "   0.1134362518787384,\n",
       "   0.25387027859687805,\n",
       "   -0.11761431396007538,\n",
       "   0.056621648371219635,\n",
       "   -0.22700800001621246,\n",
       "   0.04969858378171921,\n",
       "   -0.4241374135017395,\n",
       "   -0.0032033738680183887,\n",
       "   0.1883004754781723,\n",
       "   -0.22715216875076294,\n",
       "   0.18677809834480286,\n",
       "   -0.3008561432361603,\n",
       "   -0.05237271636724472,\n",
       "   0.1612110733985901,\n",
       "   -0.7381555438041687,\n",
       "   0.15768727660179138,\n",
       "   0.05999556928873062,\n",
       "   0.03008008748292923,\n",
       "   0.2673285901546478,\n",
       "   -0.2804104685783386,\n",
       "   -0.40846776962280273,\n",
       "   -0.5457122921943665,\n",
       "   0.11075834184885025,\n",
       "   0.027297455817461014,\n",
       "   -0.3636150062084198,\n",
       "   0.123990997672081,\n",
       "   -0.08176761120557785,\n",
       "   -0.1428299844264984,\n",
       "   -0.16446270048618317,\n",
       "   -0.023630576208233833,\n",
       "   0.06920341402292252,\n",
       "   -0.20587818324565887,\n",
       "   -0.0429568737745285,\n",
       "   -0.4525321424007416,\n",
       "   0.20525097846984863,\n",
       "   0.23798345029354095,\n",
       "   -0.4658522605895996,\n",
       "   -0.09506651759147644,\n",
       "   -0.4946015179157257,\n",
       "   0.033081021159887314,\n",
       "   -0.15787646174430847,\n",
       "   -0.08936261385679245,\n",
       "   0.20966856181621552,\n",
       "   -0.20818322896957397,\n",
       "   -0.06073040887713432,\n",
       "   -0.23062755167484283,\n",
       "   -0.30899056792259216,\n",
       "   0.4237656891345978,\n",
       "   0.037743356078863144,\n",
       "   -0.03285640850663185,\n",
       "   -0.021978747099637985,\n",
       "   0.023334551602602005,\n",
       "   0.06988731771707535,\n",
       "   -0.11609327793121338,\n",
       "   -0.0024436607491225004,\n",
       "   0.06314858049154282,\n",
       "   0.13902555406093597,\n",
       "   -0.03184518590569496,\n",
       "   0.0003429394564591348,\n",
       "   0.09777169674634933,\n",
       "   0.07185140997171402,\n",
       "   -0.24975049495697021,\n",
       "   0.1135583221912384,\n",
       "   -0.21417386829853058,\n",
       "   0.022147919982671738,\n",
       "   0.2703063189983368,\n",
       "   0.2845877408981323,\n",
       "   0.23099061846733093,\n",
       "   -0.21089822053909302,\n",
       "   0.04621359705924988,\n",
       "   0.04788656160235405,\n",
       "   0.14703409373760223,\n",
       "   -0.24811892211437225,\n",
       "   -0.18722982704639435,\n",
       "   0.13886113464832306,\n",
       "   0.1633288860321045,\n",
       "   -0.0804206132888794,\n",
       "   0.17763835191726685,\n",
       "   0.0043391454964876175,\n",
       "   0.012496109120547771,\n",
       "   0.07874155789613724,\n",
       "   0.06714481860399246,\n",
       "   0.12392530590295792,\n",
       "   -0.5669612288475037,\n",
       "   0.17406636476516724,\n",
       "   0.006335017737001181,\n",
       "   0.30788594484329224,\n",
       "   -0.022439854219555855,\n",
       "   0.008805678226053715,\n",
       "   -0.15569645166397095,\n",
       "   -0.1675529032945633,\n",
       "   -0.023014768958091736,\n",
       "   0.09127756208181381,\n",
       "   -0.06852403283119202,\n",
       "   0.08332468569278717,\n",
       "   -0.6093360185623169,\n",
       "   -0.4087057113647461,\n",
       "   -0.579093873500824,\n",
       "   0.17966988682746887,\n",
       "   0.37692520022392273,\n",
       "   0.515849232673645,\n",
       "   0.06319715827703476,\n",
       "   0.002239369321614504,\n",
       "   -0.060926616191864014,\n",
       "   0.190102681517601,\n",
       "   0.43978190422058105,\n",
       "   -0.00251980172470212,\n",
       "   -0.3540674149990082,\n",
       "   0.15873591601848602,\n",
       "   -0.2482479363679886,\n",
       "   0.044808514416217804,\n",
       "   -0.0599946528673172,\n",
       "   0.11924609541893005,\n",
       "   0.4059155583381653,\n",
       "   0.46247854828834534,\n",
       "   0.6241881847381592,\n",
       "   -0.10790733247995377,\n",
       "   -0.04445132985711098,\n",
       "   -0.28336101770401,\n",
       "   0.0380275584757328,\n",
       "   -0.04374247044324875,\n",
       "   -0.027917567640542984,\n",
       "   -0.23200881481170654,\n",
       "   -0.12563617527484894,\n",
       "   0.021412033587694168,\n",
       "   0.2826705574989319,\n",
       "   0.3797212839126587,\n",
       "   0.4987151324748993,\n",
       "   0.1252160668373108,\n",
       "   -0.32483309507369995,\n",
       "   -0.10135369002819061,\n",
       "   -0.0680505782365799,\n",
       "   -0.037591997534036636,\n",
       "   0.2665782570838928,\n",
       "   -0.010677196085453033,\n",
       "   0.04789252206683159,\n",
       "   0.05936800315976143,\n",
       "   0.017635155469179153,\n",
       "   0.1956768035888672,\n",
       "   0.43816667795181274,\n",
       "   -0.2038092464208603,\n",
       "   0.22620700299739838,\n",
       "   -0.5027624368667603,\n",
       "   -0.21395233273506165,\n",
       "   0.06295879930257797,\n",
       "   0.17093297839164734,\n",
       "   0.21437546610832214,\n",
       "   0.1790311634540558,\n",
       "   0.12522545456886292,\n",
       "   -0.26667046546936035,\n",
       "   0.019562991335988045,\n",
       "   -0.02694743499159813,\n",
       "   0.5451053977012634,\n",
       "   -0.11037778109312057,\n",
       "   0.2474050521850586,\n",
       "   -0.21154019236564636,\n",
       "   0.19465620815753937,\n",
       "   -0.11201859265565872,\n",
       "   0.13194939494132996,\n",
       "   -0.1759558767080307,\n",
       "   0.19865207374095917,\n",
       "   0.009430152364075184,\n",
       "   -0.48689043521881104,\n",
       "   0.296367883682251,\n",
       "   0.39506205916404724,\n",
       "   0.953365683555603,\n",
       "   0.4690327048301697,\n",
       "   0.23238296806812286,\n",
       "   0.3443707823753357,\n",
       "   -0.32530924677848816,\n",
       "   0.557752251625061,\n",
       "   0.012184720486402512,\n",
       "   0.2279827892780304,\n",
       "   -0.4324994385242462,\n",
       "   -0.13418342173099518,\n",
       "   -0.04566381499171257,\n",
       "   -0.006129713263362646,\n",
       "   -0.10332527756690979,\n",
       "   -0.1976793110370636,\n",
       "   -0.03306591883301735,\n",
       "   0.17414328455924988,\n",
       "   0.2883642315864563,\n",
       "   0.32332462072372437,\n",
       "   0.027767598628997803,\n",
       "   -0.09828920662403107,\n",
       "   0.0814574807882309,\n",
       "   -0.3617430031299591,\n",
       "   -0.0013470695121213794,\n",
       "   0.02340947464108467,\n",
       "   -0.06497212499380112,\n",
       "   0.0034665209241211414,\n",
       "   -0.16220100224018097,\n",
       "   0.11704571545124054,\n",
       "   -0.14431649446487427,\n",
       "   0.16573494672775269,\n",
       "   -0.26354482769966125,\n",
       "   -0.07000738382339478,\n",
       "   -0.2716578245162964,\n",
       "   0.09895646572113037,\n",
       "   0.32647469639778137,\n",
       "   -0.28814497590065,\n",
       "   0.0499260388314724,\n",
       "   0.03686298057436943,\n",
       "   0.619189977645874,\n",
       "   0.3315456509590149,\n",
       "   0.02258175052702427,\n",
       "   -0.03396696224808693,\n",
       "   0.1469225287437439,\n",
       "   0.24282389879226685,\n",
       "   0.04453616589307785,\n",
       "   -0.45319730043411255,\n",
       "   0.3092099726200104,\n",
       "   0.06240389868617058,\n",
       "   0.2238800972700119,\n",
       "   -0.4029237627983093,\n",
       "   0.09802257269620895,\n",
       "   0.21580921113491058,\n",
       "   -0.2508915364742279,\n",
       "   -0.0615018792450428,\n",
       "   0.038258738815784454,\n",
       "   0.003156213089823723,\n",
       "   0.22505708038806915,\n",
       "   -0.11112836003303528,\n",
       "   0.2028561681509018,\n",
       "   -0.22562788426876068,\n",
       "   0.13363876938819885,\n",
       "   -0.03378220647573471,\n",
       "   -0.2314988523721695,\n",
       "   -0.008900954388082027,\n",
       "   -0.03696194291114807,\n",
       "   -0.31491824984550476,\n",
       "   0.04173863306641579,\n",
       "   0.20637695491313934,\n",
       "   -0.039449140429496765,\n",
       "   -0.5680711269378662,\n",
       "   0.2585473358631134,\n",
       "   -0.38277044892311096,\n",
       "   -0.25139185786247253,\n",
       "   -0.00745072728022933,\n",
       "   0.2281174659729004,\n",
       "   0.1966436803340912,\n",
       "   -0.06704260408878326,\n",
       "   0.13692480325698853,\n",
       "   -0.09811098873615265,\n",
       "   -0.2944403290748596,\n",
       "   -0.1854884922504425,\n",
       "   0.21928206086158752,\n",
       "   0.32198190689086914,\n",
       "   -0.14073726534843445,\n",
       "   0.045698702335357666,\n",
       "   -0.35104355216026306,\n",
       "   -0.15889854729175568,\n",
       "   0.0010028320830315351,\n",
       "   -0.17866310477256775,\n",
       "   0.21997712552547455,\n",
       "   0.32523733377456665,\n",
       "   0.5128350257873535,\n",
       "   -0.38019323348999023,\n",
       "   0.13780711591243744,\n",
       "   -0.28977635502815247,\n",
       "   -0.2866572439670563,\n",
       "   -0.18543271720409393,\n",
       "   -0.011455366387963295,\n",
       "   -0.4478391110897064,\n",
       "   -0.12181394547224045,\n",
       "   -0.46384549140930176,\n",
       "   -0.06610330939292908,\n",
       "   0.039645932614803314,\n",
       "   0.4677853584289551,\n",
       "   -0.09399233013391495,\n",
       "   -0.15835075080394745,\n",
       "   -0.1563607156276703,\n",
       "   0.09351412951946259,\n",
       "   0.31579917669296265,\n",
       "   -0.15541194379329681,\n",
       "   0.1816108673810959,\n",
       "   0.4209153950214386,\n",
       "   -0.10544483363628387,\n",
       "   -0.27601030468940735,\n",
       "   -0.012592889368534088,\n",
       "   -0.006285151932388544,\n",
       "   -0.034377019852399826,\n",
       "   0.21518538892269135,\n",
       "   0.21609291434288025,\n",
       "   0.4562656581401825,\n",
       "   0.10220461338758469,\n",
       "   0.021515870466828346,\n",
       "   0.1370662897825241,\n",
       "   0.04691414162516594,\n",
       "   0.1742347925901413,\n",
       "   0.4892757833003998,\n",
       "   -0.306526243686676,\n",
       "   -0.060269538313150406,\n",
       "   0.06501121819019318,\n",
       "   0.16091647744178772,\n",
       "   0.2588517367839813,\n",
       "   -0.10352449119091034,\n",
       "   0.21405132114887238,\n",
       "   0.1330474317073822,\n",
       "   -0.39813461899757385,\n",
       "   0.21249234676361084,\n",
       "   0.01665422134101391,\n",
       "   0.24890410900115967,\n",
       "   -0.056915707886219025,\n",
       "   -0.4193786084651947,\n",
       "   0.48726049065589905,\n",
       "   0.15493600070476532,\n",
       "   -0.07179554551839828,\n",
       "   0.05234542861580849,\n",
       "   0.39590156078338623,\n",
       "   -0.11986337602138519,\n",
       "   -0.058713000267744064,\n",
       "   0.053430426865816116,\n",
       "   0.29619428515434265,\n",
       "   -0.017406895756721497,\n",
       "   0.28207290172576904,\n",
       "   0.09700989723205566,\n",
       "   0.2905801832675934,\n",
       "   -0.03407804295420647,\n",
       "   0.1603579968214035,\n",
       "   0.029296476393938065,\n",
       "   0.017020221799612045,\n",
       "   0.4946972727775574,\n",
       "   0.44040077924728394,\n",
       "   -0.19571861624717712,\n",
       "   0.15615253150463104,\n",
       "   0.2504730522632599,\n",
       "   0.043895937502384186,\n",
       "   0.3230760097503662,\n",
       "   0.03081596828997135,\n",
       "   0.02197292633354664,\n",
       "   -0.0927131175994873,\n",
       "   -0.18686744570732117,\n",
       "   -0.2750904858112335,\n",
       "   0.1224018856883049,\n",
       "   -0.08907732367515564,\n",
       "   0.21071434020996094,\n",
       "   0.3414238691329956,\n",
       "   0.04269546642899513,\n",
       "   -0.06346478313207626,\n",
       "   -0.062165431678295135,\n",
       "   -0.009410033002495766,\n",
       "   0.17256198823451996,\n",
       "   -0.6092424392700195,\n",
       "   -0.10437506437301636,\n",
       "   -0.39376524090766907,\n",
       "   -0.06387688219547272,\n",
       "   -0.31301403045654297,\n",
       "   -0.08785939961671829,\n",
       "   0.1186138167977333,\n",
       "   0.16066332161426544,\n",
       "   0.013257626444101334,\n",
       "   -0.20745663344860077,\n",
       "   -0.32737478613853455,\n",
       "   -0.01577477902173996,\n",
       "   0.21656467020511627,\n",
       "   0.008498117327690125,\n",
       "   0.2358187735080719,\n",
       "   -0.08231794089078903,\n",
       "   0.2951459586620331,\n",
       "   0.010712101124227047,\n",
       "   0.09431701898574829,\n",
       "   -0.22710292041301727,\n",
       "   0.11802985519170761,\n",
       "   0.5410673022270203,\n",
       "   0.19452491402626038,\n",
       "   -0.3048996925354004,\n",
       "   -0.18987484276294708,\n",
       "   -0.14286130666732788,\n",
       "   0.015929803252220154,\n",
       "   -0.22178509831428528,\n",
       "   0.37238162755966187,\n",
       "   -0.04617297276854515,\n",
       "   -0.11681733280420303,\n",
       "   0.18493109941482544,\n",
       "   0.04564589634537697,\n",
       "   -0.17474904656410217,\n",
       "   -0.049691926687955856,\n",
       "   0.2591226100921631,\n",
       "   0.09560989588499069,\n",
       "   0.2560717463493347,\n",
       "   0.3022283613681793,\n",
       "   0.02700878493487835,\n",
       "   0.04608719423413277,\n",
       "   0.1359395682811737,\n",
       "   -0.016635127365589142,\n",
       "   0.10464511066675186,\n",
       "   0.018618179485201836,\n",
       "   -0.19132107496261597,\n",
       "   -0.02246939390897751,\n",
       "   0.0496055893599987,\n",
       "   -0.13723240792751312,\n",
       "   0.08899516612291336,\n",
       "   0.2402992993593216,\n",
       "   0.35740625858306885,\n",
       "   0.5412004590034485,\n",
       "   -0.092180535197258,\n",
       "   -0.20618975162506104,\n",
       "   -0.41495999693870544,\n",
       "   -0.386312872171402,\n",
       "   0.5211243629455566,\n",
       "   0.47850504517555237,\n",
       "   -0.01684792898595333,\n",
       "   -0.06296141445636749,\n",
       "   -0.19649207592010498,\n",
       "   -0.10751695930957794,\n",
       "   0.09571262449026108,\n",
       "   -0.41491007804870605,\n",
       "   0.1647544503211975,\n",
       "   0.08979453891515732,\n",
       "   0.16718745231628418,\n",
       "   -0.15547358989715576,\n",
       "   0.2812332510948181,\n",
       "   -0.0012405310990288854,\n",
       "   -0.4019346237182617,\n",
       "   -0.10898085683584213,\n",
       "   -0.07242894172668457,\n",
       "   0.29898542165756226,\n",
       "   0.02877512201666832,\n",
       "   0.060196150094270706,\n",
       "   0.20420923829078674,\n",
       "   -0.013470284640789032,\n",
       "   -0.1505277007818222,\n",
       "   0.3201372027397156,\n",
       "   -0.052113741636276245,\n",
       "   0.6280629634857178,\n",
       "   0.13540640473365784,\n",
       "   -0.1455906629562378,\n",
       "   0.44118598103523254,\n",
       "   -0.017650779336690903,\n",
       "   0.02867714874446392,\n",
       "   -0.5843862891197205,\n",
       "   0.3166666328907013,\n",
       "   -0.23926115036010742,\n",
       "   0.31472042202949524,\n",
       "   0.03731575980782509,\n",
       "   -0.47179460525512695,\n",
       "   -0.0502217672765255,\n",
       "   0.025254080072045326,\n",
       "   0.3487577736377716,\n",
       "   -0.1720809042453766,\n",
       "   -0.3881038427352905,\n",
       "   0.30295976996421814,\n",
       "   -0.21387402713298798,\n",
       "   -0.03577963635325432,\n",
       "   0.05062970891594887,\n",
       "   0.3882572650909424,\n",
       "   0.26503050327301025,\n",
       "   0.44946613907814026,\n",
       "   -0.2877140939235687,\n",
       "   -0.6340733170509338,\n",
       "   -0.033007677644491196,\n",
       "   -0.21014854311943054,\n",
       "   -0.2857018709182739,\n",
       "   -0.19992060959339142,\n",
       "   -0.11783657222986221,\n",
       "   0.29622119665145874,\n",
       "   -0.1532808393239975,\n",
       "   -0.13815076649188995,\n",
       "   -0.2883766293525696,\n",
       "   0.15956315398216248,\n",
       "   -0.23512032628059387,\n",
       "   -0.29982009530067444,\n",
       "   0.24229291081428528,\n",
       "   -0.173307865858078,\n",
       "   -0.03217180445790291,\n",
       "   -0.19824360311031342,\n",
       "   0.3979836106300354,\n",
       "   0.174154594540596,\n",
       "   -0.25920817255973816,\n",
       "   0.10108791291713715,\n",
       "   -0.14171193540096283],\n",
       "  [-0.15369662642478943,\n",
       "   0.0790129229426384,\n",
       "   0.07378190755844116,\n",
       "   -0.03823181986808777,\n",
       "   0.09215767681598663,\n",
       "   -0.32468143105506897,\n",
       "   -0.08998150378465652,\n",
       "   0.10603199899196625,\n",
       "   -0.4617609679698944,\n",
       "   0.19867387413978577,\n",
       "   -0.20853732526302338,\n",
       "   0.30742037296295166,\n",
       "   0.27667680382728577,\n",
       "   0.030884919688105583,\n",
       "   -0.2191910743713379,\n",
       "   -0.16327746212482452,\n",
       "   0.1479930579662323,\n",
       "   0.219057098031044,\n",
       "   0.00562308682128787,\n",
       "   0.06112286448478699,\n",
       "   -0.1194193884730339,\n",
       "   0.45713841915130615,\n",
       "   -0.21233554184436798,\n",
       "   0.21436016261577606,\n",
       "   -0.28662627935409546,\n",
       "   -0.07063654065132141,\n",
       "   0.12557756900787354,\n",
       "   -0.31564846634864807,\n",
       "   -0.1683976948261261,\n",
       "   -0.3988146185874939,\n",
       "   0.2497144192457199,\n",
       "   0.11891039460897446,\n",
       "   -0.08367154747247696,\n",
       "   0.20431433618068695,\n",
       "   -0.00011269692913629115,\n",
       "   -0.05750177428126335,\n",
       "   -0.018035506829619408,\n",
       "   0.014288807287812233,\n",
       "   0.17867527902126312,\n",
       "   -0.19643253087997437,\n",
       "   0.12838613986968994,\n",
       "   -0.376918762922287,\n",
       "   -0.3356097638607025,\n",
       "   0.1073894202709198,\n",
       "   -0.06366998702287674,\n",
       "   0.039610855281353,\n",
       "   0.24081267416477203,\n",
       "   -0.05541888251900673,\n",
       "   0.6431405544281006,\n",
       "   0.41514983773231506,\n",
       "   0.14555539190769196,\n",
       "   -0.013489042408764362,\n",
       "   0.25451838970184326,\n",
       "   0.06076836958527565,\n",
       "   -0.31092360615730286,\n",
       "   0.2689674198627472,\n",
       "   -0.09506773203611374,\n",
       "   0.01900981180369854,\n",
       "   -0.15407243371009827,\n",
       "   0.031468506902456284,\n",
       "   -0.07465161383152008,\n",
       "   -0.009617595002055168,\n",
       "   -0.09227008372545242,\n",
       "   -0.4802752137184143,\n",
       "   -0.014885332435369492,\n",
       "   0.11489637941122055,\n",
       "   0.3214079439640045,\n",
       "   -0.4625910818576813,\n",
       "   0.18707258999347687,\n",
       "   0.01853308454155922,\n",
       "   -0.24988877773284912,\n",
       "   -0.0450734905898571,\n",
       "   -0.18307730555534363,\n",
       "   -0.3914948105812073,\n",
       "   -0.13682639598846436,\n",
       "   -0.26799407601356506,\n",
       "   0.20009689033031464,\n",
       "   -0.15426293015480042,\n",
       "   -0.1181238666176796,\n",
       "   0.11682857573032379,\n",
       "   0.01208376232534647,\n",
       "   0.13784609735012054,\n",
       "   0.029742859303951263,\n",
       "   -0.21203945577144623,\n",
       "   -0.2942862808704376,\n",
       "   0.5647349953651428,\n",
       "   -0.06541191041469574,\n",
       "   0.04264478012919426,\n",
       "   -0.20475590229034424,\n",
       "   -0.010176555253565311,\n",
       "   0.3918432295322418,\n",
       "   -0.04282088577747345,\n",
       "   0.005983348935842514,\n",
       "   0.437328964471817,\n",
       "   -0.2852553129196167,\n",
       "   -0.05852540582418442,\n",
       "   0.19706958532333374,\n",
       "   -0.4548269510269165,\n",
       "   -0.007932859472930431,\n",
       "   -0.035667333751916885,\n",
       "   0.12427128851413727,\n",
       "   0.2060960829257965,\n",
       "   0.07195869833230972,\n",
       "   0.060917071998119354,\n",
       "   -0.1630159616470337,\n",
       "   0.464278906583786,\n",
       "   0.2114512026309967,\n",
       "   0.27663567662239075,\n",
       "   0.19684246182441711,\n",
       "   0.11708666384220123,\n",
       "   -0.0398002564907074,\n",
       "   -0.04044095799326897,\n",
       "   0.06278236955404282,\n",
       "   -0.12514930963516235,\n",
       "   -0.04987660422921181,\n",
       "   0.2508595883846283,\n",
       "   -0.37355348467826843,\n",
       "   0.10397126525640488,\n",
       "   -0.23151284456253052,\n",
       "   0.4121430814266205,\n",
       "   -0.07186693698167801,\n",
       "   0.3000815808773041,\n",
       "   0.03386527672410011,\n",
       "   -0.09935843199491501,\n",
       "   0.03932022303342819,\n",
       "   0.28934115171432495,\n",
       "   -0.34031638503074646,\n",
       "   -0.10191559046506882,\n",
       "   -0.0826217532157898,\n",
       "   0.10865523666143417,\n",
       "   -0.19528396427631378,\n",
       "   0.01898312196135521,\n",
       "   -0.2947452664375305,\n",
       "   0.12470927834510803,\n",
       "   -0.19662198424339294,\n",
       "   0.108177550137043,\n",
       "   -0.02755059115588665,\n",
       "   0.12801189720630646,\n",
       "   0.3569863736629486,\n",
       "   -0.03269282355904579,\n",
       "   -0.08280514180660248,\n",
       "   -0.09475267678499222,\n",
       "   0.536851167678833,\n",
       "   0.27820247411727905,\n",
       "   0.026152536273002625,\n",
       "   0.1261873096227646,\n",
       "   0.11548148840665817,\n",
       "   -0.359840452671051,\n",
       "   -0.3748219311237335,\n",
       "   -0.043390970677137375,\n",
       "   -0.21579334139823914,\n",
       "   0.08999038487672806,\n",
       "   0.14790287613868713,\n",
       "   0.20128220319747925,\n",
       "   -0.10737680643796921,\n",
       "   0.134821355342865,\n",
       "   -0.3785093426704407,\n",
       "   0.28587764501571655,\n",
       "   0.33080118894577026,\n",
       "   -0.24652434885501862,\n",
       "   0.002981877885758877,\n",
       "   -0.36790862679481506,\n",
       "   -0.41353675723075867,\n",
       "   -0.1898319572210312,\n",
       "   0.3192931115627289,\n",
       "   -0.14425157010555267,\n",
       "   -0.0017331935232505202,\n",
       "   -0.009171119891107082,\n",
       "   0.11065865308046341,\n",
       "   0.18913918733596802,\n",
       "   0.484011173248291,\n",
       "   0.13396534323692322,\n",
       "   0.31385910511016846,\n",
       "   0.36314108967781067,\n",
       "   -0.4917096197605133,\n",
       "   0.48556509613990784,\n",
       "   0.0987912192940712,\n",
       "   -0.13364584743976593,\n",
       "   -0.08816645294427872,\n",
       "   -0.1546434760093689,\n",
       "   -0.3981320858001709,\n",
       "   0.2574174702167511,\n",
       "   -0.1778469979763031,\n",
       "   -0.28533002734184265,\n",
       "   0.14593149721622467,\n",
       "   0.1648341566324234,\n",
       "   0.16425158083438873,\n",
       "   0.46753373742103577,\n",
       "   -0.13454070687294006,\n",
       "   0.09679176658391953,\n",
       "   -0.08240294456481934,\n",
       "   0.3093528747558594,\n",
       "   -0.05164700001478195,\n",
       "   -0.26832032203674316,\n",
       "   0.01252705231308937,\n",
       "   -0.03489473834633827,\n",
       "   -0.230736643075943,\n",
       "   -0.23822236061096191,\n",
       "   -0.15970268845558167,\n",
       "   -0.08852948248386383,\n",
       "   0.10251940786838531,\n",
       "   -0.08720844984054565,\n",
       "   0.04389937222003937,\n",
       "   -0.12961752712726593,\n",
       "   0.023830508813261986,\n",
       "   -0.152864471077919,\n",
       "   0.04286302998661995,\n",
       "   -0.01160067692399025,\n",
       "   -0.36197882890701294,\n",
       "   0.2263038605451584,\n",
       "   0.006499935407191515,\n",
       "   -0.16657176613807678,\n",
       "   -0.24409866333007812,\n",
       "   0.17804349958896637,\n",
       "   -0.1699587106704712,\n",
       "   -0.13742013275623322,\n",
       "   0.22984519600868225,\n",
       "   0.06616337597370148,\n",
       "   0.1778305023908615,\n",
       "   0.33349093794822693,\n",
       "   0.055316127836704254,\n",
       "   0.299461305141449,\n",
       "   0.18764953315258026,\n",
       "   0.259883314371109,\n",
       "   -0.23794032633304596,\n",
       "   0.16552138328552246,\n",
       "   -0.15393280982971191,\n",
       "   0.03288349509239197,\n",
       "   -0.1076623871922493,\n",
       "   0.44957235455513,\n",
       "   0.057686492800712585,\n",
       "   -0.11599187552928925,\n",
       "   -0.12629830837249756,\n",
       "   0.22838307917118073,\n",
       "   0.2532852590084076,\n",
       "   -0.04426857829093933,\n",
       "   -0.28391075134277344,\n",
       "   0.08557260781526566,\n",
       "   0.14712095260620117,\n",
       "   0.1632165163755417,\n",
       "   -0.37553104758262634,\n",
       "   0.284096360206604,\n",
       "   -0.2077518254518509,\n",
       "   -0.12244737148284912,\n",
       "   0.5265040397644043,\n",
       "   0.8932770490646362,\n",
       "   -0.03842442110180855,\n",
       "   0.12675237655639648,\n",
       "   -0.14578308165073395,\n",
       "   -0.02217038907110691,\n",
       "   0.44586607813835144,\n",
       "   0.11814215779304504,\n",
       "   -0.15466345846652985,\n",
       "   -0.2694556713104248,\n",
       "   0.03720812499523163,\n",
       "   0.2975228726863861,\n",
       "   -0.10649247467517853,\n",
       "   0.025268638506531715,\n",
       "   -0.09403879195451736,\n",
       "   0.1421896517276764,\n",
       "   0.21936139464378357,\n",
       "   0.06661650538444519,\n",
       "   -0.22007596492767334,\n",
       "   0.1170540302991867,\n",
       "   0.004420798737555742,\n",
       "   -0.2848398983478546,\n",
       "   0.1736074984073639,\n",
       "   0.0940917357802391,\n",
       "   0.230821892619133,\n",
       "   0.04038390889763832,\n",
       "   0.15981221199035645,\n",
       "   0.27577751874923706,\n",
       "   -0.23048537969589233,\n",
       "   -0.16235174238681793,\n",
       "   0.1398642510175705,\n",
       "   0.015991086140275,\n",
       "   0.11915136873722076,\n",
       "   0.5081241726875305,\n",
       "   0.11125757545232773,\n",
       "   0.4917434751987457,\n",
       "   -0.2569989860057831,\n",
       "   -0.25069427490234375,\n",
       "   -0.3280733525753021,\n",
       "   0.046981632709503174,\n",
       "   -0.1642935574054718,\n",
       "   0.23284126818180084,\n",
       "   -0.20010194182395935,\n",
       "   -0.33812475204467773,\n",
       "   -0.41888493299484253,\n",
       "   -0.14505556225776672,\n",
       "   -0.11501073837280273,\n",
       "   -0.5577638745307922,\n",
       "   0.13810966908931732,\n",
       "   0.04132494330406189,\n",
       "   -0.27524101734161377,\n",
       "   0.4234623312950134,\n",
       "   0.1134362518787384,\n",
       "   0.25387027859687805,\n",
       "   -0.11761431396007538,\n",
       "   0.056621648371219635,\n",
       "   -0.22700800001621246,\n",
       "   0.04969858378171921,\n",
       "   -0.4241374135017395,\n",
       "   -0.0032033738680183887,\n",
       "   0.1883004754781723,\n",
       "   -0.22715216875076294,\n",
       "   0.18677809834480286,\n",
       "   -0.3008561432361603,\n",
       "   -0.05237271636724472,\n",
       "   0.1612110733985901,\n",
       "   -0.7381555438041687,\n",
       "   0.15768727660179138,\n",
       "   0.05999556928873062,\n",
       "   0.03008008748292923,\n",
       "   0.2673285901546478,\n",
       "   -0.2804104685783386,\n",
       "   -0.40846776962280273,\n",
       "   -0.5457122921943665,\n",
       "   0.11075834184885025,\n",
       "   0.027297455817461014,\n",
       "   -0.3636150062084198,\n",
       "   0.123990997672081,\n",
       "   -0.08176761120557785,\n",
       "   -0.1428299844264984,\n",
       "   -0.16446270048618317,\n",
       "   -0.023630576208233833,\n",
       "   0.06920341402292252,\n",
       "   -0.20587818324565887,\n",
       "   -0.0429568737745285,\n",
       "   -0.4525321424007416,\n",
       "   0.20525097846984863,\n",
       "   0.23798345029354095,\n",
       "   -0.4658522605895996,\n",
       "   -0.09506651759147644,\n",
       "   -0.4946015179157257,\n",
       "   0.033081021159887314,\n",
       "   -0.15787646174430847,\n",
       "   -0.08936261385679245,\n",
       "   0.20966856181621552,\n",
       "   -0.20818322896957397,\n",
       "   -0.06073040887713432,\n",
       "   -0.23062755167484283,\n",
       "   -0.30899056792259216,\n",
       "   0.4237656891345978,\n",
       "   0.037743356078863144,\n",
       "   -0.03285640850663185,\n",
       "   -0.021978747099637985,\n",
       "   0.023334551602602005,\n",
       "   0.06988731771707535,\n",
       "   -0.11609327793121338,\n",
       "   -0.0024436607491225004,\n",
       "   0.06314858049154282,\n",
       "   0.13902555406093597,\n",
       "   -0.03184518590569496,\n",
       "   0.0003429394564591348,\n",
       "   0.09777169674634933,\n",
       "   0.07185140997171402,\n",
       "   -0.24975049495697021,\n",
       "   0.1135583221912384,\n",
       "   -0.21417386829853058,\n",
       "   0.022147919982671738,\n",
       "   0.2703063189983368,\n",
       "   0.2845877408981323,\n",
       "   0.23099061846733093,\n",
       "   -0.21089822053909302,\n",
       "   0.04621359705924988,\n",
       "   0.04788656160235405,\n",
       "   0.14703409373760223,\n",
       "   -0.24811892211437225,\n",
       "   -0.18722982704639435,\n",
       "   0.13886113464832306,\n",
       "   0.1633288860321045,\n",
       "   -0.0804206132888794,\n",
       "   0.17763835191726685,\n",
       "   0.0043391454964876175,\n",
       "   0.012496109120547771,\n",
       "   0.07874155789613724,\n",
       "   0.06714481860399246,\n",
       "   0.12392530590295792,\n",
       "   -0.5669612288475037,\n",
       "   0.17406636476516724,\n",
       "   0.006335017737001181,\n",
       "   0.30788594484329224,\n",
       "   -0.022439854219555855,\n",
       "   0.008805678226053715,\n",
       "   -0.15569645166397095,\n",
       "   -0.1675529032945633,\n",
       "   -0.023014768958091736,\n",
       "   0.09127756208181381,\n",
       "   -0.06852403283119202,\n",
       "   0.08332468569278717,\n",
       "   -0.6093360185623169,\n",
       "   -0.4087057113647461,\n",
       "   -0.579093873500824,\n",
       "   0.17966988682746887,\n",
       "   0.37692520022392273,\n",
       "   0.515849232673645,\n",
       "   0.06319715827703476,\n",
       "   0.002239369321614504,\n",
       "   -0.060926616191864014,\n",
       "   0.190102681517601,\n",
       "   0.43978190422058105,\n",
       "   -0.00251980172470212,\n",
       "   -0.3540674149990082,\n",
       "   0.15873591601848602,\n",
       "   -0.2482479363679886,\n",
       "   0.044808514416217804,\n",
       "   -0.0599946528673172,\n",
       "   0.11924609541893005,\n",
       "   0.4059155583381653,\n",
       "   0.46247854828834534,\n",
       "   0.6241881847381592,\n",
       "   -0.10790733247995377,\n",
       "   -0.04445132985711098,\n",
       "   -0.28336101770401,\n",
       "   0.0380275584757328,\n",
       "   -0.04374247044324875,\n",
       "   -0.027917567640542984,\n",
       "   -0.23200881481170654,\n",
       "   -0.12563617527484894,\n",
       "   0.021412033587694168,\n",
       "   0.2826705574989319,\n",
       "   0.3797212839126587,\n",
       "   0.4987151324748993,\n",
       "   0.1252160668373108,\n",
       "   -0.32483309507369995,\n",
       "   -0.10135369002819061,\n",
       "   -0.0680505782365799,\n",
       "   -0.037591997534036636,\n",
       "   0.2665782570838928,\n",
       "   -0.010677196085453033,\n",
       "   0.04789252206683159,\n",
       "   0.05936800315976143,\n",
       "   0.017635155469179153,\n",
       "   0.1956768035888672,\n",
       "   0.43816667795181274,\n",
       "   -0.2038092464208603,\n",
       "   0.22620700299739838,\n",
       "   -0.5027624368667603,\n",
       "   -0.21395233273506165,\n",
       "   0.06295879930257797,\n",
       "   0.17093297839164734,\n",
       "   0.21437546610832214,\n",
       "   0.1790311634540558,\n",
       "   0.12522545456886292,\n",
       "   -0.26667046546936035,\n",
       "   0.019562991335988045,\n",
       "   -0.02694743499159813,\n",
       "   0.5451053977012634,\n",
       "   -0.11037778109312057,\n",
       "   0.2474050521850586,\n",
       "   -0.21154019236564636,\n",
       "   0.19465620815753937,\n",
       "   -0.11201859265565872,\n",
       "   0.13194939494132996,\n",
       "   -0.1759558767080307,\n",
       "   0.19865207374095917,\n",
       "   0.009430152364075184,\n",
       "   -0.48689043521881104,\n",
       "   0.296367883682251,\n",
       "   0.39506205916404724,\n",
       "   0.953365683555603,\n",
       "   0.4690327048301697,\n",
       "   0.23238296806812286,\n",
       "   0.3443707823753357,\n",
       "   -0.32530924677848816,\n",
       "   0.557752251625061,\n",
       "   0.012184720486402512,\n",
       "   0.2279827892780304,\n",
       "   -0.4324994385242462,\n",
       "   -0.13418342173099518,\n",
       "   -0.04566381499171257,\n",
       "   -0.006129713263362646,\n",
       "   -0.10332527756690979,\n",
       "   -0.1976793110370636,\n",
       "   -0.03306591883301735,\n",
       "   0.17414328455924988,\n",
       "   0.2883642315864563,\n",
       "   0.32332462072372437,\n",
       "   0.027767598628997803,\n",
       "   -0.09828920662403107,\n",
       "   0.0814574807882309,\n",
       "   -0.3617430031299591,\n",
       "   -0.0013470695121213794,\n",
       "   0.02340947464108467,\n",
       "   -0.06497212499380112,\n",
       "   0.0034665209241211414,\n",
       "   -0.16220100224018097,\n",
       "   0.11704571545124054,\n",
       "   -0.14431649446487427,\n",
       "   0.16573494672775269,\n",
       "   -0.26354482769966125,\n",
       "   -0.07000738382339478,\n",
       "   -0.2716578245162964,\n",
       "   0.09895646572113037,\n",
       "   0.32647469639778137,\n",
       "   -0.28814497590065,\n",
       "   0.0499260388314724,\n",
       "   0.03686298057436943,\n",
       "   0.619189977645874,\n",
       "   0.3315456509590149,\n",
       "   0.02258175052702427,\n",
       "   -0.03396696224808693,\n",
       "   0.1469225287437439,\n",
       "   0.24282389879226685,\n",
       "   0.04453616589307785,\n",
       "   -0.45319730043411255,\n",
       "   0.3092099726200104,\n",
       "   0.06240389868617058,\n",
       "   0.2238800972700119,\n",
       "   -0.4029237627983093,\n",
       "   0.09802257269620895,\n",
       "   0.21580921113491058,\n",
       "   -0.2508915364742279,\n",
       "   -0.0615018792450428,\n",
       "   0.038258738815784454,\n",
       "   0.003156213089823723,\n",
       "   0.22505708038806915,\n",
       "   -0.11112836003303528,\n",
       "   0.2028561681509018,\n",
       "   -0.22562788426876068,\n",
       "   0.13363876938819885,\n",
       "   -0.03378220647573471,\n",
       "   -0.2314988523721695,\n",
       "   -0.008900954388082027,\n",
       "   -0.03696194291114807,\n",
       "   -0.31491824984550476,\n",
       "   0.04173863306641579,\n",
       "   0.20637695491313934,\n",
       "   -0.039449140429496765,\n",
       "   -0.5680711269378662,\n",
       "   0.2585473358631134,\n",
       "   -0.38277044892311096,\n",
       "   -0.25139185786247253,\n",
       "   -0.00745072728022933,\n",
       "   0.2281174659729004,\n",
       "   0.1966436803340912,\n",
       "   -0.06704260408878326,\n",
       "   0.13692480325698853,\n",
       "   -0.09811098873615265,\n",
       "   -0.2944403290748596,\n",
       "   -0.1854884922504425,\n",
       "   0.21928206086158752,\n",
       "   0.32198190689086914,\n",
       "   -0.14073726534843445,\n",
       "   0.045698702335357666,\n",
       "   -0.35104355216026306,\n",
       "   -0.15889854729175568,\n",
       "   0.0010028320830315351,\n",
       "   -0.17866310477256775,\n",
       "   0.21997712552547455,\n",
       "   0.32523733377456665,\n",
       "   0.5128350257873535,\n",
       "   -0.38019323348999023,\n",
       "   0.13780711591243744,\n",
       "   -0.28977635502815247,\n",
       "   -0.2866572439670563,\n",
       "   -0.18543271720409393,\n",
       "   -0.011455366387963295,\n",
       "   -0.4478391110897064,\n",
       "   -0.12181394547224045,\n",
       "   -0.46384549140930176,\n",
       "   -0.06610330939292908,\n",
       "   0.039645932614803314,\n",
       "   0.4677853584289551,\n",
       "   -0.09399233013391495,\n",
       "   -0.15835075080394745,\n",
       "   -0.1563607156276703,\n",
       "   0.09351412951946259,\n",
       "   0.31579917669296265,\n",
       "   -0.15541194379329681,\n",
       "   0.1816108673810959,\n",
       "   0.4209153950214386,\n",
       "   -0.10544483363628387,\n",
       "   -0.27601030468940735,\n",
       "   -0.012592889368534088,\n",
       "   -0.006285151932388544,\n",
       "   -0.034377019852399826,\n",
       "   0.21518538892269135,\n",
       "   0.21609291434288025,\n",
       "   0.4562656581401825,\n",
       "   0.10220461338758469,\n",
       "   0.021515870466828346,\n",
       "   0.1370662897825241,\n",
       "   0.04691414162516594,\n",
       "   0.1742347925901413,\n",
       "   0.4892757833003998,\n",
       "   -0.306526243686676,\n",
       "   -0.060269538313150406,\n",
       "   0.06501121819019318,\n",
       "   0.16091647744178772,\n",
       "   0.2588517367839813,\n",
       "   -0.10352449119091034,\n",
       "   0.21405132114887238,\n",
       "   0.1330474317073822,\n",
       "   -0.39813461899757385,\n",
       "   0.21249234676361084,\n",
       "   0.01665422134101391,\n",
       "   0.24890410900115967,\n",
       "   -0.056915707886219025,\n",
       "   -0.4193786084651947,\n",
       "   0.48726049065589905,\n",
       "   0.15493600070476532,\n",
       "   -0.07179554551839828,\n",
       "   0.05234542861580849,\n",
       "   0.39590156078338623,\n",
       "   -0.11986337602138519,\n",
       "   -0.058713000267744064,\n",
       "   0.053430426865816116,\n",
       "   0.29619428515434265,\n",
       "   -0.017406895756721497,\n",
       "   0.28207290172576904,\n",
       "   0.09700989723205566,\n",
       "   0.2905801832675934,\n",
       "   -0.03407804295420647,\n",
       "   0.1603579968214035,\n",
       "   0.029296476393938065,\n",
       "   0.017020221799612045,\n",
       "   0.4946972727775574,\n",
       "   0.44040077924728394,\n",
       "   -0.19571861624717712,\n",
       "   0.15615253150463104,\n",
       "   0.2504730522632599,\n",
       "   0.043895937502384186,\n",
       "   0.3230760097503662,\n",
       "   0.03081596828997135,\n",
       "   0.02197292633354664,\n",
       "   -0.0927131175994873,\n",
       "   -0.18686744570732117,\n",
       "   -0.2750904858112335,\n",
       "   0.1224018856883049,\n",
       "   -0.08907732367515564,\n",
       "   0.21071434020996094,\n",
       "   0.3414238691329956,\n",
       "   0.04269546642899513,\n",
       "   -0.06346478313207626,\n",
       "   -0.062165431678295135,\n",
       "   -0.009410033002495766,\n",
       "   0.17256198823451996,\n",
       "   -0.6092424392700195,\n",
       "   -0.10437506437301636,\n",
       "   -0.39376524090766907,\n",
       "   -0.06387688219547272,\n",
       "   -0.31301403045654297,\n",
       "   -0.08785939961671829,\n",
       "   0.1186138167977333,\n",
       "   0.16066332161426544,\n",
       "   0.013257626444101334,\n",
       "   -0.20745663344860077,\n",
       "   -0.32737478613853455,\n",
       "   -0.01577477902173996,\n",
       "   0.21656467020511627,\n",
       "   0.008498117327690125,\n",
       "   0.2358187735080719,\n",
       "   -0.08231794089078903,\n",
       "   0.2951459586620331,\n",
       "   0.010712101124227047,\n",
       "   0.09431701898574829,\n",
       "   -0.22710292041301727,\n",
       "   0.11802985519170761,\n",
       "   0.5410673022270203,\n",
       "   0.19452491402626038,\n",
       "   -0.3048996925354004,\n",
       "   -0.18987484276294708,\n",
       "   -0.14286130666732788,\n",
       "   0.015929803252220154,\n",
       "   -0.22178509831428528,\n",
       "   0.37238162755966187,\n",
       "   -0.04617297276854515,\n",
       "   -0.11681733280420303,\n",
       "   0.18493109941482544,\n",
       "   0.04564589634537697,\n",
       "   -0.17474904656410217,\n",
       "   -0.049691926687955856,\n",
       "   0.2591226100921631,\n",
       "   0.09560989588499069,\n",
       "   0.2560717463493347,\n",
       "   0.3022283613681793,\n",
       "   0.02700878493487835,\n",
       "   0.04608719423413277,\n",
       "   0.1359395682811737,\n",
       "   -0.016635127365589142,\n",
       "   0.10464511066675186,\n",
       "   0.018618179485201836,\n",
       "   -0.19132107496261597,\n",
       "   -0.02246939390897751,\n",
       "   0.0496055893599987,\n",
       "   -0.13723240792751312,\n",
       "   0.08899516612291336,\n",
       "   0.2402992993593216,\n",
       "   0.35740625858306885,\n",
       "   0.5412004590034485,\n",
       "   -0.092180535197258,\n",
       "   -0.20618975162506104,\n",
       "   -0.41495999693870544,\n",
       "   -0.386312872171402,\n",
       "   0.5211243629455566,\n",
       "   0.47850504517555237,\n",
       "   -0.01684792898595333,\n",
       "   -0.06296141445636749,\n",
       "   -0.19649207592010498,\n",
       "   -0.10751695930957794,\n",
       "   0.09571262449026108,\n",
       "   -0.41491007804870605,\n",
       "   0.1647544503211975,\n",
       "   0.08979453891515732,\n",
       "   0.16718745231628418,\n",
       "   -0.15547358989715576,\n",
       "   0.2812332510948181,\n",
       "   -0.0012405310990288854,\n",
       "   -0.4019346237182617,\n",
       "   -0.10898085683584213,\n",
       "   -0.07242894172668457,\n",
       "   0.29898542165756226,\n",
       "   0.02877512201666832,\n",
       "   0.060196150094270706,\n",
       "   0.20420923829078674,\n",
       "   -0.013470284640789032,\n",
       "   -0.1505277007818222,\n",
       "   0.3201372027397156,\n",
       "   -0.052113741636276245,\n",
       "   0.6280629634857178,\n",
       "   0.13540640473365784,\n",
       "   -0.1455906629562378,\n",
       "   0.44118598103523254,\n",
       "   -0.017650779336690903,\n",
       "   0.02867714874446392,\n",
       "   -0.5843862891197205,\n",
       "   0.3166666328907013,\n",
       "   -0.23926115036010742,\n",
       "   0.31472042202949524,\n",
       "   0.03731575980782509,\n",
       "   -0.47179460525512695,\n",
       "   -0.0502217672765255,\n",
       "   0.025254080072045326,\n",
       "   0.3487577736377716,\n",
       "   -0.1720809042453766,\n",
       "   -0.3881038427352905,\n",
       "   0.30295976996421814,\n",
       "   -0.21387402713298798,\n",
       "   -0.03577963635325432,\n",
       "   0.05062970891594887,\n",
       "   0.3882572650909424,\n",
       "   0.26503050327301025,\n",
       "   0.44946613907814026,\n",
       "   -0.2877140939235687,\n",
       "   -0.6340733170509338,\n",
       "   -0.033007677644491196,\n",
       "   -0.21014854311943054,\n",
       "   -0.2857018709182739,\n",
       "   -0.19992060959339142,\n",
       "   -0.11783657222986221,\n",
       "   0.29622119665145874,\n",
       "   -0.1532808393239975,\n",
       "   -0.13815076649188995,\n",
       "   -0.2883766293525696,\n",
       "   0.15956315398216248,\n",
       "   -0.23512032628059387,\n",
       "   -0.29982009530067444,\n",
       "   0.24229291081428528,\n",
       "   -0.173307865858078,\n",
       "   -0.03217180445790291,\n",
       "   -0.19824360311031342,\n",
       "   0.3979836106300354,\n",
       "   0.174154594540596,\n",
       "   -0.25920817255973816,\n",
       "   0.10108791291713715,\n",
       "   -0.14171193540096283],\n",
       "  [-0.05715339630842209,\n",
       "   -0.037861987948417664,\n",
       "   0.0511646531522274,\n",
       "   -0.0909028947353363,\n",
       "   0.17802214622497559,\n",
       "   -0.022166887298226357,\n",
       "   0.5987794995307922,\n",
       "   0.08288966119289398,\n",
       "   -0.3151780664920807,\n",
       "   -0.0732150599360466,\n",
       "   0.10414636880159378,\n",
       "   0.26249727606773376,\n",
       "   -0.28402188420295715,\n",
       "   0.0880035012960434,\n",
       "   0.1834016740322113,\n",
       "   -0.08207002282142639,\n",
       "   0.2904807925224304,\n",
       "   0.029753077775239944,\n",
       "   0.1173345148563385,\n",
       "   0.17717719078063965,\n",
       "   0.16712749004364014,\n",
       "   -0.3500303030014038,\n",
       "   -0.21165437996387482,\n",
       "   0.07447997480630875,\n",
       "   -0.4641675055027008,\n",
       "   -0.31109490990638733,\n",
       "   -0.2085750699043274,\n",
       "   -0.35937681794166565,\n",
       "   -0.1359572410583496,\n",
       "   -0.4139527976512909,\n",
       "   0.11055445671081543,\n",
       "   0.32109424471855164,\n",
       "   0.5333424210548401,\n",
       "   0.5197013020515442,\n",
       "   -0.00012201326171634719,\n",
       "   0.006074527278542519,\n",
       "   -0.002198513364419341,\n",
       "   -0.003848564811050892,\n",
       "   -0.225519061088562,\n",
       "   -0.08147905766963959,\n",
       "   0.595574140548706,\n",
       "   -0.2086481750011444,\n",
       "   0.04167422652244568,\n",
       "   -0.1967037469148636,\n",
       "   0.12415026128292084,\n",
       "   -0.3029721677303314,\n",
       "   -0.04991457611322403,\n",
       "   -0.3180961608886719,\n",
       "   0.4491652846336365,\n",
       "   -0.13739384710788727,\n",
       "   0.05022527277469635,\n",
       "   0.38426101207733154,\n",
       "   -0.140932098031044,\n",
       "   0.19931739568710327,\n",
       "   -0.0296622347086668,\n",
       "   0.23494714498519897,\n",
       "   -0.08495688438415527,\n",
       "   -0.2966647446155548,\n",
       "   0.31958720088005066,\n",
       "   -0.18337267637252808,\n",
       "   -0.23528225719928741,\n",
       "   0.06106877326965332,\n",
       "   -0.11439529061317444,\n",
       "   -0.3018169701099396,\n",
       "   0.3582299053668976,\n",
       "   -0.08900466561317444,\n",
       "   -0.4720636010169983,\n",
       "   -0.2796252965927124,\n",
       "   -0.1780114769935608,\n",
       "   0.08476091921329498,\n",
       "   0.13839992880821228,\n",
       "   -0.4279208779335022,\n",
       "   -0.5727866291999817,\n",
       "   -0.4943040609359741,\n",
       "   -0.04712022840976715,\n",
       "   0.03200612962245941,\n",
       "   -0.32908135652542114,\n",
       "   0.06106193736195564,\n",
       "   -0.04651736468076706,\n",
       "   -0.0024525420740246773,\n",
       "   -0.4008658528327942,\n",
       "   -0.1909390687942505,\n",
       "   -0.03806787729263306,\n",
       "   0.18028101325035095,\n",
       "   -0.1636812835931778,\n",
       "   0.3774634897708893,\n",
       "   -0.029722800478339195,\n",
       "   0.2500405013561249,\n",
       "   -0.1865842640399933,\n",
       "   -0.24113106727600098,\n",
       "   0.3104633688926697,\n",
       "   -0.32868289947509766,\n",
       "   0.17688196897506714,\n",
       "   0.4461233913898468,\n",
       "   -0.2475443184375763,\n",
       "   -0.17675459384918213,\n",
       "   -0.056702032685279846,\n",
       "   -0.3885207772254944,\n",
       "   0.22580337524414062,\n",
       "   -0.06040925160050392,\n",
       "   -0.11102228611707687,\n",
       "   0.11925766617059708,\n",
       "   -0.1659700721502304,\n",
       "   -0.01583314687013626,\n",
       "   0.07526952028274536,\n",
       "   0.26925310492515564,\n",
       "   0.17796187102794647,\n",
       "   0.21520251035690308,\n",
       "   0.11015590280294418,\n",
       "   -0.41738906502723694,\n",
       "   0.008766356855630875,\n",
       "   0.045220062136650085,\n",
       "   -0.3886714279651642,\n",
       "   0.2294883131980896,\n",
       "   0.0866202861070633,\n",
       "   0.03661713749170303,\n",
       "   -0.3295235335826874,\n",
       "   0.12103348225355148,\n",
       "   0.19422955811023712,\n",
       "   -0.03273119032382965,\n",
       "   -0.3979845345020294,\n",
       "   0.0642561987042427,\n",
       "   0.09895709902048111,\n",
       "   -0.007773875258862972,\n",
       "   -0.11880028247833252,\n",
       "   0.01481539011001587,\n",
       "   -0.00215281848795712,\n",
       "   0.18084067106246948,\n",
       "   0.053675852715969086,\n",
       "   -0.0783504918217659,\n",
       "   0.049911536276340485,\n",
       "   0.004377687815576792,\n",
       "   -0.1358768790960312,\n",
       "   0.08135977387428284,\n",
       "   -0.01528873573988676,\n",
       "   0.10481224954128265,\n",
       "   -0.03664098680019379,\n",
       "   0.16550420224666595,\n",
       "   0.1909094899892807,\n",
       "   0.009988979436457157,\n",
       "   0.21362720429897308,\n",
       "   -0.05657275393605232,\n",
       "   0.11374958604574203,\n",
       "   -0.10609255731105804,\n",
       "   -0.04529683291912079,\n",
       "   0.11644716560840607,\n",
       "   0.15705810487270355,\n",
       "   0.09142156690359116,\n",
       "   -0.29163745045661926,\n",
       "   0.2853477895259857,\n",
       "   -0.0932709351181984,\n",
       "   -0.26450812816619873,\n",
       "   0.2515757083892822,\n",
       "   0.03320438042283058,\n",
       "   -0.39459916949272156,\n",
       "   -0.23967701196670532,\n",
       "   -0.32170623540878296,\n",
       "   0.3632383346557617,\n",
       "   0.19680561125278473,\n",
       "   0.048605527728796005,\n",
       "   0.09220842272043228,\n",
       "   -0.06120068207383156,\n",
       "   0.16841590404510498,\n",
       "   0.017315197736024857,\n",
       "   0.31846338510513306,\n",
       "   0.21234270930290222,\n",
       "   -0.23126320540905,\n",
       "   -0.36788180470466614,\n",
       "   0.6276065707206726,\n",
       "   0.15109051764011383,\n",
       "   0.33598020672798157,\n",
       "   0.025994766503572464,\n",
       "   -0.4712957441806793,\n",
       "   0.48072054982185364,\n",
       "   0.009487710893154144,\n",
       "   0.2042943686246872,\n",
       "   0.317051500082016,\n",
       "   -0.3109451234340668,\n",
       "   -0.0012996576260775328,\n",
       "   0.23922905325889587,\n",
       "   -0.18398872017860413,\n",
       "   -0.02504666894674301,\n",
       "   -0.042363423854112625,\n",
       "   0.021418536081910133,\n",
       "   0.45408326387405396,\n",
       "   0.09615036100149155,\n",
       "   0.0890527069568634,\n",
       "   0.5082173943519592,\n",
       "   -0.055825382471084595,\n",
       "   0.07459042966365814,\n",
       "   -0.0885537788271904,\n",
       "   -0.27102386951446533,\n",
       "   0.3134453594684601,\n",
       "   0.08993515372276306,\n",
       "   0.15905943512916565,\n",
       "   0.010973961092531681,\n",
       "   -0.08714559674263,\n",
       "   0.21939226984977722,\n",
       "   -0.07129738479852676,\n",
       "   -0.18727366626262665,\n",
       "   0.17857246100902557,\n",
       "   -0.14517159759998322,\n",
       "   -0.12547747790813446,\n",
       "   0.13831843435764313,\n",
       "   0.019939469173550606,\n",
       "   -0.06205848604440689,\n",
       "   -0.184197798371315,\n",
       "   0.20158584415912628,\n",
       "   -0.08980457484722137,\n",
       "   0.19728024303913116,\n",
       "   -0.1764647364616394,\n",
       "   -0.2715863287448883,\n",
       "   -0.0019048720132559538,\n",
       "   0.1634162962436676,\n",
       "   -0.017741601914167404,\n",
       "   -0.0604434572160244,\n",
       "   -0.034226756542921066,\n",
       "   -0.21719644963741302,\n",
       "   -0.06141040101647377,\n",
       "   0.06881114840507507,\n",
       "   -0.41985929012298584,\n",
       "   -0.04463883116841316,\n",
       "   -0.35176631808280945,\n",
       "   0.1824169009923935,\n",
       "   -0.07266104966402054,\n",
       "   0.18205580115318298,\n",
       "   0.17862510681152344,\n",
       "   -0.024658355861902237,\n",
       "   -0.23891571164131165,\n",
       "   0.1298803836107254,\n",
       "   0.036573298275470734,\n",
       "   -0.2153061181306839,\n",
       "   0.07378661632537842,\n",
       "   0.2548118531703949,\n",
       "   0.025701288133859634,\n",
       "   0.053078342229127884,\n",
       "   -0.15080495178699493,\n",
       "   0.40751180052757263,\n",
       "   0.20541366934776306,\n",
       "   0.21796815097332,\n",
       "   0.06485170125961304,\n",
       "   0.014456403441727161,\n",
       "   0.02280537411570549,\n",
       "   -0.01116274856030941,\n",
       "   -0.05006345361471176,\n",
       "   0.6243646740913391,\n",
       "   0.17381443083286285,\n",
       "   0.41883423924446106,\n",
       "   0.058721285313367844,\n",
       "   -0.0847589448094368,\n",
       "   0.13612647354602814,\n",
       "   0.14789728820323944,\n",
       "   -0.21766652166843414,\n",
       "   0.1060156375169754,\n",
       "   -0.19079157710075378,\n",
       "   0.14046885073184967,\n",
       "   0.6474383473396301,\n",
       "   -0.09467322379350662,\n",
       "   -0.021215278655290604,\n",
       "   -0.04184922203421593,\n",
       "   -0.11643175780773163,\n",
       "   -0.19203124940395355,\n",
       "   0.032626256346702576,\n",
       "   0.08576169610023499,\n",
       "   -0.44825029373168945,\n",
       "   -0.1540813446044922,\n",
       "   0.012411403469741344,\n",
       "   -0.2460068166255951,\n",
       "   -0.03968732804059982,\n",
       "   0.03824397549033165,\n",
       "   0.1952545940876007,\n",
       "   -0.03841441497206688,\n",
       "   0.03110532835125923,\n",
       "   0.023777293041348457,\n",
       "   0.11964035034179688,\n",
       "   0.29074299335479736,\n",
       "   -0.09527832269668579,\n",
       "   -0.026454437524080276,\n",
       "   0.09454167634248734,\n",
       "   0.39827823638916016,\n",
       "   -0.1978989988565445,\n",
       "   -0.2764560580253601,\n",
       "   0.15611055493354797,\n",
       "   0.08138782531023026,\n",
       "   -0.08810638636350632,\n",
       "   0.26452431082725525,\n",
       "   -0.146212637424469,\n",
       "   0.031248565763235092,\n",
       "   -0.09867112338542938,\n",
       "   -0.5169736742973328,\n",
       "   -0.45925265550613403,\n",
       "   -0.03765309974551201,\n",
       "   0.06119660288095474,\n",
       "   -0.1395256370306015,\n",
       "   0.13321737945079803,\n",
       "   0.33745741844177246,\n",
       "   -0.11673196405172348,\n",
       "   0.6314312219619751,\n",
       "   0.0603884682059288,\n",
       "   -0.24201011657714844,\n",
       "   0.21377167105674744,\n",
       "   -0.23430918157100677,\n",
       "   -0.09640617668628693,\n",
       "   -0.08414366096258163,\n",
       "   0.2045980989933014,\n",
       "   -0.33387482166290283,\n",
       "   0.28495490550994873,\n",
       "   0.2845367193222046,\n",
       "   0.1369120329618454,\n",
       "   -0.18333658576011658,\n",
       "   -0.40721702575683594,\n",
       "   0.30907467007637024,\n",
       "   0.07214383035898209,\n",
       "   0.18432773649692535,\n",
       "   -0.04691252484917641,\n",
       "   -0.34854134917259216,\n",
       "   -0.08489014208316803,\n",
       "   -0.2920384109020233,\n",
       "   -0.05872920900583267,\n",
       "   -0.20096634328365326,\n",
       "   -0.09474778920412064,\n",
       "   -0.02533281221985817,\n",
       "   -0.07703903317451477,\n",
       "   0.05653328821063042,\n",
       "   0.02953539788722992,\n",
       "   0.0890287533402443,\n",
       "   -0.32333600521087646,\n",
       "   0.018335040658712387,\n",
       "   0.08090779930353165,\n",
       "   -0.028400050476193428,\n",
       "   0.07599983364343643,\n",
       "   -0.02289961278438568,\n",
       "   -0.2220894694328308,\n",
       "   0.1977064609527588,\n",
       "   -0.22012342512607574,\n",
       "   -0.061892978847026825,\n",
       "   0.05600334703922272,\n",
       "   -0.42517969012260437,\n",
       "   0.32803818583488464,\n",
       "   0.032330479472875595,\n",
       "   -0.1505909562110901,\n",
       "   -0.26555150747299194,\n",
       "   -0.27502647042274475,\n",
       "   0.14640860259532928,\n",
       "   0.4522668421268463,\n",
       "   -0.2196105420589447,\n",
       "   -0.04946426674723625,\n",
       "   -0.13885830342769623,\n",
       "   0.11966525763273239,\n",
       "   -0.2595694363117218,\n",
       "   0.11293143033981323,\n",
       "   0.3263930082321167,\n",
       "   0.05253230780363083,\n",
       "   0.1158333569765091,\n",
       "   0.005820828955620527,\n",
       "   0.24950727820396423,\n",
       "   -0.01381588727235794,\n",
       "   -0.25525379180908203,\n",
       "   0.052987318485975266,\n",
       "   0.21937714517116547,\n",
       "   0.32254043221473694,\n",
       "   0.28269487619400024,\n",
       "   1.0907588005065918,\n",
       "   -0.2653214931488037,\n",
       "   -0.44171610474586487,\n",
       "   0.1172584518790245,\n",
       "   -0.0038028510753065348,\n",
       "   0.08310651779174805,\n",
       "   -0.03587116301059723,\n",
       "   -0.22381894290447235,\n",
       "   -0.111977219581604,\n",
       "   -0.38279014825820923,\n",
       "   -0.042990654706954956,\n",
       "   0.20872999727725983,\n",
       "   0.10941559076309204,\n",
       "   -0.08014620095491409,\n",
       "   -0.002027827315032482,\n",
       "   -0.0019529613200575113,\n",
       "   -0.0970238447189331,\n",
       "   -0.2663554847240448,\n",
       "   0.22930462658405304,\n",
       "   -0.4422942101955414,\n",
       "   0.4132721722126007,\n",
       "   -0.09419417381286621,\n",
       "   0.25496357679367065,\n",
       "   -0.4347328543663025,\n",
       "   -0.02474633790552616,\n",
       "   0.08484935760498047,\n",
       "   0.20529863238334656,\n",
       "   0.4099353849887848,\n",
       "   0.09387565404176712,\n",
       "   -0.566199779510498,\n",
       "   0.23538435995578766,\n",
       "   -0.23324322700500488,\n",
       "   0.12012115865945816,\n",
       "   0.020623743534088135,\n",
       "   0.7507364153862,\n",
       "   0.33639535307884216,\n",
       "   -0.25196972489356995,\n",
       "   0.12900704145431519,\n",
       "   -0.1288745254278183,\n",
       "   0.30853691697120667,\n",
       "   0.34160077571868896,\n",
       "   -0.16348791122436523,\n",
       "   0.11568388342857361,\n",
       "   -0.2812088131904602,\n",
       "   -0.23127557337284088,\n",
       "   -0.01797068864107132,\n",
       "   -0.1686774492263794,\n",
       "   0.24810631573200226,\n",
       "   0.14643944799900055,\n",
       "   0.45534026622772217,\n",
       "   -0.22536931931972504,\n",
       "   -0.24254028499126434,\n",
       "   -0.35243165493011475,\n",
       "   0.24376296997070312,\n",
       "   0.0014684908092021942,\n",
       "   -0.2186923325061798,\n",
       "   0.05712272971868515,\n",
       "   0.22554059326648712,\n",
       "   -0.5106266140937805,\n",
       "   0.30686989426612854,\n",
       "   0.21263977885246277,\n",
       "   0.09549983590841293,\n",
       "   -0.06199461966753006,\n",
       "   0.0917520821094513,\n",
       "   -0.4535752832889557,\n",
       "   -0.08502031117677689,\n",
       "   0.07702832669019699,\n",
       "   0.07257583737373352,\n",
       "   -0.15610384941101074,\n",
       "   0.06797180324792862,\n",
       "   0.2716444730758667,\n",
       "   0.09605152159929276,\n",
       "   -0.12592089176177979,\n",
       "   -0.01330939307808876,\n",
       "   -0.06310998648405075,\n",
       "   0.1258792132139206,\n",
       "   -0.6170742511749268,\n",
       "   0.0007479725172743201,\n",
       "   0.2541421949863434,\n",
       "   0.4118504822254181,\n",
       "   0.18196074664592743,\n",
       "   0.024055423215031624,\n",
       "   0.48595771193504333,\n",
       "   -0.3031938076019287,\n",
       "   -0.009407044388353825,\n",
       "   -0.3962804973125458,\n",
       "   0.5464118123054504,\n",
       "   0.02422306500375271,\n",
       "   -0.025299090892076492,\n",
       "   -0.5309440493583679,\n",
       "   -0.40346571803092957,\n",
       "   0.3645172715187073,\n",
       "   0.17735730111598969,\n",
       "   -0.18968196213245392,\n",
       "   0.3228042721748352,\n",
       "   0.0861283466219902,\n",
       "   -0.3093383014202118,\n",
       "   0.4607326090335846,\n",
       "   0.35497555136680603,\n",
       "   0.8964493870735168,\n",
       "   -0.023643579334020615,\n",
       "   0.39791741967201233,\n",
       "   -0.22235989570617676,\n",
       "   0.24558988213539124,\n",
       "   0.39347484707832336,\n",
       "   -0.47779616713523865,\n",
       "   0.030030855908989906,\n",
       "   -0.36762720346450806,\n",
       "   0.10461726039648056,\n",
       "   -0.08317579329013824,\n",
       "   -0.07684146612882614,\n",
       "   0.013157869689166546,\n",
       "   0.3261485695838928,\n",
       "   0.054482024163007736,\n",
       "   0.0050422800704836845,\n",
       "   -0.0008295138250105083,\n",
       "   0.40589410066604614,\n",
       "   0.06350888311862946,\n",
       "   0.13679786026477814,\n",
       "   -0.03984883055090904,\n",
       "   -0.49663934111595154,\n",
       "   0.037490393966436386,\n",
       "   0.03344159200787544,\n",
       "   -0.06474875658750534,\n",
       "   0.28630852699279785,\n",
       "   -0.14284229278564453,\n",
       "   0.28489410877227783,\n",
       "   0.13188424706459045,\n",
       "   -0.13049937784671783,\n",
       "   -0.0563553161919117,\n",
       "   -0.11789434403181076,\n",
       "   -0.26383933424949646,\n",
       "   0.17712311446666718,\n",
       "   -0.06357906013727188,\n",
       "   -0.3627169728279114,\n",
       "   0.22569262981414795,\n",
       "   0.10144682228565216,\n",
       "   0.35916608572006226,\n",
       "   0.3173240125179291,\n",
       "   0.17447814345359802,\n",
       "   0.04095346853137016,\n",
       "   -0.17618802189826965,\n",
       "   0.004344359505921602,\n",
       "   -0.14035968482494354,\n",
       "   -0.22089652717113495,\n",
       "   0.35511288046836853,\n",
       "   0.17501693964004517,\n",
       "   0.17215701937675476,\n",
       "   0.020168185234069824,\n",
       "   -0.07307253032922745,\n",
       "   -0.09314516186714172,\n",
       "   -0.35977983474731445,\n",
       "   0.08956165611743927,\n",
       "   0.7337258458137512,\n",
       "   -0.2959563136100769,\n",
       "   0.17703822255134583,\n",
       "   0.1553662121295929,\n",
       "   0.29931414127349854,\n",
       "   -0.03369800001382828,\n",
       "   -0.008200293406844139,\n",
       "   0.13813477754592896,\n",
       "   -0.21390628814697266,\n",
       "   0.10799501091241837,\n",
       "   -0.09512346982955933,\n",
       "   -0.06390257924795151,\n",
       "   0.026754293590784073,\n",
       "   0.2608214318752289,\n",
       "   -0.15438733994960785,\n",
       "   -0.4901171028614044,\n",
       "   0.2969227135181427,\n",
       "   0.013302325271070004,\n",
       "   -0.17297542095184326,\n",
       "   0.056741759181022644,\n",
       "   0.5062624216079712,\n",
       "   0.4875364303588867,\n",
       "   -0.21126697957515717,\n",
       "   -0.14710505306720734,\n",
       "   -0.04535345733165741,\n",
       "   0.07659200578927994,\n",
       "   -0.06972917914390564,\n",
       "   0.3390699625015259,\n",
       "   0.23827826976776123,\n",
       "   -0.2658875286579132,\n",
       "   -0.10528169572353363,\n",
       "   -0.1975068598985672,\n",
       "   -0.5801361799240112,\n",
       "   0.011816324666142464,\n",
       "   0.12684175372123718,\n",
       "   0.23586393892765045,\n",
       "   0.3718905448913574,\n",
       "   0.32271766662597656,\n",
       "   -0.21534711122512817,\n",
       "   0.2921597361564636,\n",
       "   -0.16736963391304016,\n",
       "   0.09815343469381332,\n",
       "   0.14677029848098755,\n",
       "   0.30982503294944763,\n",
       "   -0.0961347222328186,\n",
       "   -0.19941075146198273,\n",
       "   -0.2798696756362915,\n",
       "   0.00725592439994216,\n",
       "   -0.07616165280342102,\n",
       "   0.1189475953578949,\n",
       "   0.1027219220995903,\n",
       "   -0.1377081722021103,\n",
       "   -0.36785805225372314,\n",
       "   0.22484546899795532,\n",
       "   0.4112251400947571,\n",
       "   -0.16529452800750732,\n",
       "   0.029341448098421097,\n",
       "   0.10790460556745529,\n",
       "   -0.08382316678762436,\n",
       "   -0.31250330805778503,\n",
       "   0.26723700761795044,\n",
       "   0.2581845223903656,\n",
       "   -0.05534966662526131,\n",
       "   0.0948166623711586,\n",
       "   0.4142948389053345,\n",
       "   0.4423994719982147,\n",
       "   -0.40792933106422424,\n",
       "   -0.10194231569766998,\n",
       "   0.29154181480407715,\n",
       "   0.24712064862251282,\n",
       "   0.34785687923431396,\n",
       "   0.13275931775569916,\n",
       "   0.12895305454730988,\n",
       "   0.009222466498613358,\n",
       "   -0.062277115881443024,\n",
       "   0.31196844577789307,\n",
       "   0.652568519115448,\n",
       "   0.14792035520076752,\n",
       "   0.07868858426809311,\n",
       "   -0.04821998253464699,\n",
       "   0.30420824885368347,\n",
       "   -0.09312968701124191,\n",
       "   0.07287274301052094,\n",
       "   0.367718905210495,\n",
       "   -0.09398839622735977,\n",
       "   -0.20068369805812836,\n",
       "   0.8060770034790039,\n",
       "   0.08405852317810059,\n",
       "   -0.0009938501752912998,\n",
       "   -0.2954791188240051,\n",
       "   0.0068539949133992195,\n",
       "   -0.34027254581451416,\n",
       "   -0.017259934917092323,\n",
       "   0.2794938385486603,\n",
       "   0.23546521365642548,\n",
       "   0.44676798582077026,\n",
       "   -0.23620618879795074,\n",
       "   -0.13100138306617737,\n",
       "   0.5702826976776123,\n",
       "   -0.07040809094905853,\n",
       "   0.10652947425842285,\n",
       "   0.26914873719215393,\n",
       "   -0.13520827889442444,\n",
       "   0.04439690336585045,\n",
       "   0.22588272392749786,\n",
       "   0.2633190453052521,\n",
       "   0.18286769092082977,\n",
       "   -0.0025700454134494066,\n",
       "   -0.10958817601203918,\n",
       "   0.7173179984092712,\n",
       "   0.15545766055583954,\n",
       "   -0.008736170828342438,\n",
       "   0.1039079874753952,\n",
       "   -0.3417072296142578,\n",
       "   -0.19074536859989166,\n",
       "   -0.19127444922924042,\n",
       "   0.487099826335907,\n",
       "   0.25073370337486267,\n",
       "   -0.2644146680831909,\n",
       "   0.4150305986404419,\n",
       "   -0.021516703069210052,\n",
       "   -0.05671367421746254,\n",
       "   0.16003364324569702,\n",
       "   0.3881200850009918,\n",
       "   -0.0895276665687561,\n",
       "   -0.1384505182504654,\n",
       "   -0.11399991065263748,\n",
       "   -0.08308194577693939,\n",
       "   -0.21524780988693237,\n",
       "   -0.07317209243774414,\n",
       "   -0.05669666826725006,\n",
       "   -0.3669997453689575,\n",
       "   -0.3092123568058014,\n",
       "   -0.01782919093966484,\n",
       "   0.06735646724700928,\n",
       "   -0.164295494556427,\n",
       "   0.07824534177780151,\n",
       "   0.24783603847026825,\n",
       "   0.22228461503982544,\n",
       "   -0.24090635776519775,\n",
       "   0.024760542437434196,\n",
       "   -0.0885448083281517,\n",
       "   0.24685311317443848,\n",
       "   -0.10932281613349915,\n",
       "   -0.06423216313123703,\n",
       "   0.07476507127285004,\n",
       "   0.18187536299228668,\n",
       "   -0.32444000244140625,\n",
       "   0.0828917846083641,\n",
       "   -0.36068904399871826,\n",
       "   0.274442583322525,\n",
       "   0.08586643636226654,\n",
       "   0.16584032773971558,\n",
       "   -0.2622157633304596,\n",
       "   -0.15433859825134277,\n",
       "   0.13127319514751434,\n",
       "   -0.0612487830221653,\n",
       "   -0.09221889078617096,\n",
       "   -0.18388532102108002,\n",
       "   0.09224677085876465,\n",
       "   -0.3349148631095886,\n",
       "   -0.15847374498844147,\n",
       "   0.5190868973731995,\n",
       "   0.10129626095294952,\n",
       "   -0.0951884463429451,\n",
       "   -0.02818481996655464,\n",
       "   0.15167297422885895,\n",
       "   -0.20144227147102356,\n",
       "   0.11955239623785019,\n",
       "   0.39718207716941833,\n",
       "   -0.10259880125522614,\n",
       "   -0.016633035615086555,\n",
       "   -0.05521589145064354,\n",
       "   0.015392579138278961,\n",
       "   0.018222751095891,\n",
       "   0.33470913767814636,\n",
       "   0.2925769090652466,\n",
       "   0.08678612858057022,\n",
       "   -0.19913817942142487,\n",
       "   0.026330996304750443,\n",
       "   -0.3334692120552063,\n",
       "   0.0017620387952774763,\n",
       "   0.24790994822978973,\n",
       "   -0.09281692653894424,\n",
       "   -0.2671651244163513,\n",
       "   -0.14378559589385986,\n",
       "   0.1598135381937027,\n",
       "   0.21963489055633545,\n",
       "   -0.39987045526504517,\n",
       "   0.01793515682220459,\n",
       "   -0.2632226347923279,\n",
       "   0.14057223498821259,\n",
       "   -0.1145290657877922,\n",
       "   0.19853675365447998,\n",
       "   -0.28040197491645813,\n",
       "   0.2988179624080658,\n",
       "   0.03786475956439972,\n",
       "   -0.03219632804393768,\n",
       "   -0.10268381237983704,\n",
       "   0.2618415653705597,\n",
       "   -0.14943131804466248,\n",
       "   0.12696288526058197,\n",
       "   -0.2775554358959198,\n",
       "   0.06727716326713562,\n",
       "   -0.4449775218963623,\n",
       "   0.46234920620918274,\n",
       "   0.0617387518286705,\n",
       "   0.5032163262367249,\n",
       "   0.06401586532592773,\n",
       "   0.17546716332435608,\n",
       "   -0.02487410604953766,\n",
       "   -0.003424870315939188,\n",
       "   -0.3323584496974945,\n",
       "   0.18697263300418854,\n",
       "   -0.20274581015110016,\n",
       "   0.03247237205505371,\n",
       "   -0.18313735723495483,\n",
       "   -0.41767945885658264,\n",
       "   -0.20729762315750122,\n",
       "   -0.3010694682598114,\n",
       "   0.08933591842651367,\n",
       "   -0.4719409942626953,\n",
       "   -0.42946282029151917,\n",
       "   0.1154167577624321,\n",
       "   -0.3035818934440613,\n",
       "   0.021023161709308624,\n",
       "   -0.034585949033498764,\n",
       "   0.5569847822189331,\n",
       "   0.018521910533308983,\n",
       "   0.31111153960227966,\n",
       "   -0.26158151030540466,\n",
       "   -0.15165001153945923,\n",
       "   0.34476545453071594,\n",
       "   -0.4638010859489441,\n",
       "   -0.20192976295948029,\n",
       "   -0.29537221789360046,\n",
       "   -0.25605425238609314,\n",
       "   0.3270573616027832,\n",
       "   0.11506644636392593,\n",
       "   -0.4861242473125458,\n",
       "   -0.029865819960832596,\n",
       "   0.41793763637542725,\n",
       "   -0.17446748912334442,\n",
       "   0.19824226200580597,\n",
       "   0.06356322020292282,\n",
       "   0.10410860925912857,\n",
       "   -0.23499079048633575,\n",
       "   -0.3590209186077118,\n",
       "   0.5853842496871948,\n",
       "   -0.15043964982032776,\n",
       "   0.12994293868541718,\n",
       "   -0.026125330477952957,\n",
       "   -0.3002535402774811],\n",
       "  [-0.4040469229221344,\n",
       "   -0.013007267378270626,\n",
       "   -0.10247044265270233,\n",
       "   0.10342935472726822,\n",
       "   0.1453051120042801,\n",
       "   -0.12255392223596573,\n",
       "   0.25505441427230835,\n",
       "   0.18908894062042236,\n",
       "   0.13067765533924103,\n",
       "   -0.00650223670527339,\n",
       "   -0.05422317981719971,\n",
       "   0.4988860785961151,\n",
       "   -0.17248110473155975,\n",
       "   -0.27275946736335754,\n",
       "   -0.1320430487394333,\n",
       "   0.06504266709089279,\n",
       "   0.219759002327919,\n",
       "   0.02241688407957554,\n",
       "   0.2416452020406723,\n",
       "   -0.03428642824292183,\n",
       "   -0.12187288701534271,\n",
       "   0.10230212658643723,\n",
       "   -0.17672573029994965,\n",
       "   0.24139969050884247,\n",
       "   -0.3152627646923065,\n",
       "   -0.07799308747053146,\n",
       "   0.0812312588095665,\n",
       "   -0.20963358879089355,\n",
       "   -0.0027562291361391544,\n",
       "   -0.568961501121521,\n",
       "   -0.2084297239780426,\n",
       "   0.15812459588050842,\n",
       "   0.020943094044923782,\n",
       "   0.5331247448921204,\n",
       "   -0.0001148736773757264,\n",
       "   0.04034265875816345,\n",
       "   -0.029163340106606483,\n",
       "   0.27314773201942444,\n",
       "   0.13471728563308716,\n",
       "   0.04436144605278969,\n",
       "   0.35295453667640686,\n",
       "   -0.19428537786006927,\n",
       "   -0.16858315467834473,\n",
       "   -0.22576645016670227,\n",
       "   -0.05417079105973244,\n",
       "   -0.2808632552623749,\n",
       "   -0.022122494876384735,\n",
       "   -0.22417743504047394,\n",
       "   0.06830833852291107,\n",
       "   0.030041778460144997,\n",
       "   0.1608586460351944,\n",
       "   0.11519497632980347,\n",
       "   -0.39643150568008423,\n",
       "   -0.059515759348869324,\n",
       "   -0.0024587970692664385,\n",
       "   -0.10422490537166595,\n",
       "   -0.09915456920862198,\n",
       "   0.014026980847120285,\n",
       "   0.07132960855960846,\n",
       "   -0.25518715381622314,\n",
       "   -0.21165646612644196,\n",
       "   0.19490495324134827,\n",
       "   -0.2130429893732071,\n",
       "   -0.14781391620635986,\n",
       "   0.2687234878540039,\n",
       "   0.2163694202899933,\n",
       "   0.03461211174726486,\n",
       "   -0.2425246387720108,\n",
       "   0.16808678209781647,\n",
       "   0.23163501918315887,\n",
       "   0.06701083481311798,\n",
       "   -0.02721273899078369,\n",
       "   -0.01712493970990181,\n",
       "   -0.3350793123245239,\n",
       "   -0.3706912398338318,\n",
       "   -0.028482357040047646,\n",
       "   0.1461227536201477,\n",
       "   -0.4334975779056549,\n",
       "   -0.12065141648054123,\n",
       "   -0.08960272371768951,\n",
       "   -0.4140651524066925,\n",
       "   0.20377342402935028,\n",
       "   0.26424267888069153,\n",
       "   -0.12239381670951843,\n",
       "   -0.2039308398962021,\n",
       "   0.43524429202079773,\n",
       "   0.09542801976203918,\n",
       "   0.14973458647727966,\n",
       "   0.1041521355509758,\n",
       "   -0.09975191205739975,\n",
       "   -0.28079167008399963,\n",
       "   -0.26437777280807495,\n",
       "   -0.15957693755626678,\n",
       "   0.37330421805381775,\n",
       "   -0.3415870666503906,\n",
       "   -0.17395517230033875,\n",
       "   0.269295334815979,\n",
       "   -0.3091347813606262,\n",
       "   -0.14078836143016815,\n",
       "   0.11428023129701614,\n",
       "   -0.14243130385875702,\n",
       "   0.19466497004032135,\n",
       "   0.20834127068519592,\n",
       "   -0.04711996391415596,\n",
       "   0.06630698591470718,\n",
       "   0.24600408971309662,\n",
       "   0.038447603583335876,\n",
       "   0.17384621500968933,\n",
       "   0.029690774157643318,\n",
       "   -0.48029252886772156,\n",
       "   -0.31605061888694763,\n",
       "   -0.09036099165678024,\n",
       "   -0.18887561559677124,\n",
       "   0.020214010030031204,\n",
       "   0.34587207436561584,\n",
       "   -0.24245859682559967,\n",
       "   -0.23708045482635498,\n",
       "   -0.0725833922624588,\n",
       "   -0.020596446469426155,\n",
       "   -0.1713072508573532,\n",
       "   -0.3799442946910858,\n",
       "   -0.09833759069442749,\n",
       "   0.06586087495088577,\n",
       "   0.1854548454284668,\n",
       "   0.058747224509716034,\n",
       "   0.061038099229335785,\n",
       "   -0.36479803919792175,\n",
       "   -0.2021666169166565,\n",
       "   -0.09283147007226944,\n",
       "   -0.076726034283638,\n",
       "   -0.12862756848335266,\n",
       "   -0.17200061678886414,\n",
       "   -0.1044916957616806,\n",
       "   0.25425776839256287,\n",
       "   -0.18827340006828308,\n",
       "   0.09130566567182541,\n",
       "   0.2601751685142517,\n",
       "   0.07899510860443115,\n",
       "   -0.11537614464759827,\n",
       "   0.21030858159065247,\n",
       "   0.21960484981536865,\n",
       "   -0.36432310938835144,\n",
       "   0.10472845286130905,\n",
       "   -0.11305434256792068,\n",
       "   0.0748009979724884,\n",
       "   0.43602681159973145,\n",
       "   -0.0055078305304050446,\n",
       "   0.0017840805230662227,\n",
       "   -0.2595288157463074,\n",
       "   0.2082756757736206,\n",
       "   -0.2649088203907013,\n",
       "   -0.22648879885673523,\n",
       "   0.2413855344057083,\n",
       "   0.1191600114107132,\n",
       "   -0.08314219862222672,\n",
       "   -0.12216080725193024,\n",
       "   -0.49422940611839294,\n",
       "   0.4385454058647156,\n",
       "   0.25392258167266846,\n",
       "   -0.11635340005159378,\n",
       "   -0.07491347938776016,\n",
       "   -0.18091346323490143,\n",
       "   -0.5298194289207458,\n",
       "   -0.21674206852912903,\n",
       "   0.26792478561401367,\n",
       "   -0.01877214014530182,\n",
       "   -0.07221256196498871,\n",
       "   -0.1937801092863083,\n",
       "   0.3418041467666626,\n",
       "   0.1921718418598175,\n",
       "   0.3103512227535248,\n",
       "   0.64305579662323,\n",
       "   -0.3864825367927551,\n",
       "   0.7953374981880188,\n",
       "   -0.12505000829696655,\n",
       "   0.7483616471290588,\n",
       "   0.25799399614334106,\n",
       "   -0.24770566821098328,\n",
       "   -0.37781792879104614,\n",
       "   0.11663863062858582,\n",
       "   -0.17846864461898804,\n",
       "   0.27222153544425964,\n",
       "   -0.28185170888900757,\n",
       "   -0.12006309628486633,\n",
       "   0.7026370167732239,\n",
       "   -0.2252788096666336,\n",
       "   0.34198451042175293,\n",
       "   0.10614461451768875,\n",
       "   0.24699543416500092,\n",
       "   0.3040957748889923,\n",
       "   -0.24837352335453033,\n",
       "   -0.04918995499610901,\n",
       "   0.550458550453186,\n",
       "   -0.08022262901067734,\n",
       "   -0.2242528200149536,\n",
       "   -0.1509813666343689,\n",
       "   -0.15340656042099,\n",
       "   0.11113347858190536,\n",
       "   -0.031554292887449265,\n",
       "   -0.12037328630685806,\n",
       "   -0.15540924668312073,\n",
       "   0.06173209100961685,\n",
       "   0.015111537650227547,\n",
       "   0.027055859565734863,\n",
       "   -0.06373577564954758,\n",
       "   0.21922646462917328,\n",
       "   0.14711281657218933,\n",
       "   0.21719422936439514,\n",
       "   0.07411728799343109,\n",
       "   0.06839253008365631,\n",
       "   0.4290308356285095,\n",
       "   -0.006116968579590321,\n",
       "   0.1055164784193039,\n",
       "   -0.06455086171627045,\n",
       "   -0.15043260157108307,\n",
       "   -0.14192801713943481,\n",
       "   0.1726527363061905,\n",
       "   0.1282206028699875,\n",
       "   0.06789840757846832,\n",
       "   0.08919261395931244,\n",
       "   0.23117290437221527,\n",
       "   -0.10697982460260391,\n",
       "   0.015444044955074787,\n",
       "   0.12702296674251556,\n",
       "   0.4437766373157501,\n",
       "   -0.30414214730262756,\n",
       "   0.14068694412708282,\n",
       "   -0.16028903424739838,\n",
       "   -0.10612241923809052,\n",
       "   -0.1936035454273224,\n",
       "   0.14087413251399994,\n",
       "   0.10873258113861084,\n",
       "   -0.18346592783927917,\n",
       "   0.1624327152967453,\n",
       "   0.1247907429933548,\n",
       "   0.309466689825058,\n",
       "   -0.173070028424263,\n",
       "   0.20741264522075653,\n",
       "   0.11455957591533661,\n",
       "   0.2654833495616913,\n",
       "   0.04486135393381119,\n",
       "   0.11170506477355957,\n",
       "   0.016418036073446274,\n",
       "   -0.1618008315563202,\n",
       "   -0.10114365816116333,\n",
       "   0.2487524300813675,\n",
       "   0.7665553092956543,\n",
       "   0.2934233546257019,\n",
       "   -0.0981309786438942,\n",
       "   -0.19185040891170502,\n",
       "   -0.00744950445368886,\n",
       "   0.20522291958332062,\n",
       "   0.1254594922065735,\n",
       "   0.09028635919094086,\n",
       "   -0.01721389777958393,\n",
       "   0.3012901544570923,\n",
       "   0.4247046113014221,\n",
       "   0.1712668240070343,\n",
       "   0.3508589267730713,\n",
       "   0.474333792924881,\n",
       "   0.5267605781555176,\n",
       "   -0.12619903683662415,\n",
       "   -0.21092325448989868,\n",
       "   0.11692432314157486,\n",
       "   -0.34138455986976624,\n",
       "   -0.4390028715133667,\n",
       "   -0.03245577588677406,\n",
       "   -0.4132477045059204,\n",
       "   0.16794642806053162,\n",
       "   0.10433095693588257,\n",
       "   0.22383278608322144,\n",
       "   -0.168983593583107,\n",
       "   -0.013874867931008339,\n",
       "   0.0025530215352773666,\n",
       "   0.27276208996772766,\n",
       "   0.23192831873893738,\n",
       "   -0.006252425257116556,\n",
       "   -0.03470802679657936,\n",
       "   0.16773183643817902,\n",
       "   0.235477477312088,\n",
       "   -0.3278358280658722,\n",
       "   -0.16834309697151184,\n",
       "   -0.017744487151503563,\n",
       "   0.24668414890766144,\n",
       "   -0.02020339109003544,\n",
       "   -0.08809307962656021,\n",
       "   0.06602860242128372,\n",
       "   0.1790386587381363,\n",
       "   0.4836558401584625,\n",
       "   -0.07303351163864136,\n",
       "   0.043879907578229904,\n",
       "   -0.23364877700805664,\n",
       "   -0.08682698756456375,\n",
       "   0.011286372318863869,\n",
       "   0.07300631701946259,\n",
       "   0.1429343819618225,\n",
       "   0.20160536468029022,\n",
       "   0.041605010628700256,\n",
       "   0.05537930876016617,\n",
       "   -0.13247603178024292,\n",
       "   -0.032635897397994995,\n",
       "   -0.20362262427806854,\n",
       "   -0.3690440356731415,\n",
       "   0.06732721626758575,\n",
       "   0.04781554639339447,\n",
       "   -0.10531243681907654,\n",
       "   0.3012903332710266,\n",
       "   0.18954965472221375,\n",
       "   -0.07064705342054367,\n",
       "   -0.028474120423197746,\n",
       "   -0.45728909969329834,\n",
       "   0.08773909509181976,\n",
       "   -0.18031150102615356,\n",
       "   0.009878234006464481,\n",
       "   -0.10766518115997314,\n",
       "   -0.15344327688217163,\n",
       "   -0.1983681470155716,\n",
       "   0.04000325873494148,\n",
       "   0.11686043441295624,\n",
       "   -0.2781752645969391,\n",
       "   -0.2616516649723053,\n",
       "   -0.32242265343666077,\n",
       "   -0.11807259917259216,\n",
       "   -0.17440877854824066,\n",
       "   0.14054438471794128,\n",
       "   -0.020053938031196594,\n",
       "   0.05114232748746872,\n",
       "   -0.03128005564212799,\n",
       "   0.12229980528354645,\n",
       "   0.10087115317583084,\n",
       "   0.12934544682502747,\n",
       "   -0.24915212392807007,\n",
       "   -0.16720829904079437,\n",
       "   0.19397683441638947,\n",
       "   -0.052004143595695496,\n",
       "   -0.012171968817710876,\n",
       "   -0.3335229754447937,\n",
       "   -0.24970538914203644,\n",
       "   0.38905906677246094,\n",
       "   -0.06996995955705643,\n",
       "   -0.014193568378686905,\n",
       "   -0.214137002825737,\n",
       "   0.0942140445113182,\n",
       "   0.16697978973388672,\n",
       "   0.13987208902835846,\n",
       "   -0.24602244794368744,\n",
       "   0.21763962507247925,\n",
       "   -0.30447089672088623,\n",
       "   0.2614800035953522,\n",
       "   0.03844335302710533,\n",
       "   -0.2167222946882248,\n",
       "   0.39410242438316345,\n",
       "   0.22821258008480072,\n",
       "   -0.12224381417036057,\n",
       "   0.1697704941034317,\n",
       "   -0.24780596792697906,\n",
       "   0.16913461685180664,\n",
       "   -0.05900362506508827,\n",
       "   -0.23506365716457367,\n",
       "   0.07231324911117554,\n",
       "   0.484038382768631,\n",
       "   0.24679617583751678,\n",
       "   0.6211674213409424,\n",
       "   0.04898207634687424,\n",
       "   -0.013337538577616215,\n",
       "   -0.00010498862684471533,\n",
       "   -0.3113279938697815,\n",
       "   -0.1315823793411255,\n",
       "   -0.43118953704833984,\n",
       "   -0.07551377266645432,\n",
       "   0.20715612173080444,\n",
       "   -0.09913627058267593,\n",
       "   0.025744717568159103,\n",
       "   0.48657679557800293,\n",
       "   -0.08505593240261078,\n",
       "   -0.3330843150615692,\n",
       "   0.22217024862766266,\n",
       "   -0.17344295978546143,\n",
       "   0.049075789749622345,\n",
       "   -0.1999395489692688,\n",
       "   0.12732820212841034,\n",
       "   -0.09477365761995316,\n",
       "   0.05930560082197189,\n",
       "   0.001397682586684823,\n",
       "   0.40461063385009766,\n",
       "   -0.5339139699935913,\n",
       "   -0.14910313487052917,\n",
       "   0.237467423081398,\n",
       "   -0.278715580701828,\n",
       "   0.2525823712348938,\n",
       "   -0.25178980827331543,\n",
       "   -0.9455535411834717,\n",
       "   0.35129252076148987,\n",
       "   -0.8524883389472961,\n",
       "   0.09497607499361038,\n",
       "   0.016547590494155884,\n",
       "   0.22760266065597534,\n",
       "   0.04750778153538704,\n",
       "   -0.027061937376856804,\n",
       "   0.11630099266767502,\n",
       "   -0.0324515700340271,\n",
       "   0.1658131629228592,\n",
       "   -0.05380868911743164,\n",
       "   -0.08455506712198257,\n",
       "   0.23704971373081207,\n",
       "   -0.5604739785194397,\n",
       "   -0.11725746095180511,\n",
       "   -0.1456516981124878,\n",
       "   -0.0787852331995964,\n",
       "   0.19362659752368927,\n",
       "   0.5949996709823608,\n",
       "   0.7412646412849426,\n",
       "   -0.09443056583404541,\n",
       "   -0.2862771451473236,\n",
       "   -0.3504449427127838,\n",
       "   -0.04790673032402992,\n",
       "   0.06967751681804657,\n",
       "   0.03928457573056221,\n",
       "   -0.013841893523931503,\n",
       "   -0.10236921161413193,\n",
       "   -0.18080124258995056,\n",
       "   0.010817783884704113,\n",
       "   0.10237513482570648,\n",
       "   0.1822671741247177,\n",
       "   0.13342005014419556,\n",
       "   -0.15410974621772766,\n",
       "   -0.08755319565534592,\n",
       "   0.15052032470703125,\n",
       "   0.303697407245636,\n",
       "   0.2778022587299347,\n",
       "   0.24230678379535675,\n",
       "   -0.03876839950680733,\n",
       "   0.34377962350845337,\n",
       "   0.31737178564071655,\n",
       "   0.10557416826486588,\n",
       "   -0.11109750717878342,\n",
       "   -0.15754671394824982,\n",
       "   0.21815183758735657,\n",
       "   -0.36953026056289673,\n",
       "   0.0900227278470993,\n",
       "   0.09987177699804306,\n",
       "   0.06710685044527054,\n",
       "   0.16950532793998718,\n",
       "   -0.03232567757368088,\n",
       "   0.21382655203342438,\n",
       "   -0.269631952047348,\n",
       "   0.3467106521129608,\n",
       "   -0.19183030724525452,\n",
       "   0.3990117013454437,\n",
       "   0.5268697738647461,\n",
       "   0.13443592190742493,\n",
       "   -0.5008947253227234,\n",
       "   -0.3026987910270691,\n",
       "   0.07103808224201202,\n",
       "   0.5278068780899048,\n",
       "   -0.2671490013599396,\n",
       "   0.4369067847728729,\n",
       "   -0.4802764356136322,\n",
       "   -0.40487611293792725,\n",
       "   0.4786458909511566,\n",
       "   0.2622503340244293,\n",
       "   0.5893493890762329,\n",
       "   -0.00019733051885850728,\n",
       "   0.2131252884864807,\n",
       "   0.24673759937286377,\n",
       "   0.2820870280265808,\n",
       "   0.1871795356273651,\n",
       "   -0.4422435462474823,\n",
       "   -0.1283498853445053,\n",
       "   -0.2351832240819931,\n",
       "   0.21898217499256134,\n",
       "   0.04329977184534073,\n",
       "   -0.07354767620563507,\n",
       "   0.06810370832681656,\n",
       "   0.23929385840892792,\n",
       "   -0.11905814707279205,\n",
       "   -0.07074656337499619,\n",
       "   0.13437466323375702,\n",
       "   0.8829225301742554,\n",
       "   -0.013043608516454697,\n",
       "   0.30273404717445374,\n",
       "   0.4876132905483246,\n",
       "   -0.39955636858940125,\n",
       "   0.3429582715034485,\n",
       "   0.13938821852207184,\n",
       "   0.2241424024105072,\n",
       "   0.06270559877157211,\n",
       "   0.09403461962938309,\n",
       "   0.09735548496246338,\n",
       "   -0.06829363107681274,\n",
       "   -0.1550721526145935,\n",
       "   -0.2499285638332367,\n",
       "   -0.20255860686302185,\n",
       "   -0.5686984658241272,\n",
       "   0.297793447971344,\n",
       "   -0.20471440255641937,\n",
       "   0.03080056607723236,\n",
       "   0.0798010528087616,\n",
       "   0.14483775198459625,\n",
       "   0.04165840521454811,\n",
       "   0.24064281582832336,\n",
       "   0.006632594391703606,\n",
       "   0.2885669469833374,\n",
       "   0.21322667598724365,\n",
       "   0.33644169569015503,\n",
       "   0.2534501254558563,\n",
       "   -0.5096054077148438,\n",
       "   -0.19772875308990479,\n",
       "   -0.009806572459638119,\n",
       "   -0.015243478119373322,\n",
       "   0.16441352665424347,\n",
       "   -0.264480859041214,\n",
       "   -0.3185324966907501,\n",
       "   -0.642041802406311,\n",
       "   -0.045864131301641464,\n",
       "   0.3040487468242645,\n",
       "   -0.365604966878891,\n",
       "   0.01904495246708393,\n",
       "   0.10112268477678299,\n",
       "   -0.07926517724990845,\n",
       "   -0.19805803894996643,\n",
       "   0.08064141869544983,\n",
       "   -0.15343162417411804,\n",
       "   0.08537712693214417,\n",
       "   0.4080638289451599,\n",
       "   -0.07441261410713196,\n",
       "   -0.2806636393070221,\n",
       "   0.01849406771361828,\n",
       "   0.3022502660751343,\n",
       "   0.12154628336429596,\n",
       "   -0.21070410311222076,\n",
       "   0.4307021498680115,\n",
       "   -0.47031182050704956,\n",
       "   -0.2629356384277344,\n",
       "   -0.1035870686173439,\n",
       "   0.21543660759925842,\n",
       "   0.1363147497177124,\n",
       "   0.11987601220607758,\n",
       "   0.0926351323723793,\n",
       "   -0.13185419142246246,\n",
       "   -0.18322819471359253,\n",
       "   0.09514594823122025,\n",
       "   0.2641548216342926,\n",
       "   -0.12749508023262024,\n",
       "   -0.10990781337022781,\n",
       "   -0.1724201887845993,\n",
       "   -0.018341632559895515,\n",
       "   -0.30604371428489685,\n",
       "   -0.010590710677206516,\n",
       "   -0.05993127077817917,\n",
       "   0.09162306040525436,\n",
       "   0.09735386073589325,\n",
       "   0.13492833077907562,\n",
       "   -0.31854090094566345,\n",
       "   0.16975019872188568,\n",
       "   -0.2577683925628662,\n",
       "   -0.12103083729743958,\n",
       "   0.12591233849525452,\n",
       "   0.1538807600736618,\n",
       "   -0.20141109824180603,\n",
       "   -0.15866515040397644,\n",
       "   0.12080893665552139,\n",
       "   0.04116220772266388,\n",
       "   0.09976737946271896,\n",
       "   0.2575889229774475,\n",
       "   0.03509654477238655,\n",
       "   -0.2286946326494217,\n",
       "   -0.08349362015724182,\n",
       "   0.1424897164106369,\n",
       "   0.046630680561065674,\n",
       "   -0.36045771837234497,\n",
       "   0.0659409686923027,\n",
       "   0.1358204036951065,\n",
       "   -0.08454765379428864,\n",
       "   -0.5128017067909241,\n",
       "   0.20534539222717285,\n",
       "   0.17162436246871948,\n",
       "   0.013776835985481739,\n",
       "   -0.4263346493244171,\n",
       "   0.19677728414535522,\n",
       "   -0.00454443646594882,\n",
       "   -0.006976690609008074,\n",
       "   0.1540151685476303,\n",
       "   -0.04092022776603699,\n",
       "   0.01958843506872654,\n",
       "   0.10938426852226257,\n",
       "   0.2167302668094635,\n",
       "   0.12089190632104874,\n",
       "   -0.2039533406496048,\n",
       "   0.04365456476807594,\n",
       "   0.260488897562027,\n",
       "   0.3350318968296051,\n",
       "   0.17188453674316406,\n",
       "   0.011980189010500908,\n",
       "   -0.026033131405711174,\n",
       "   -0.0632946714758873,\n",
       "   0.16377726197242737,\n",
       "   -0.16997964680194855,\n",
       "   0.21267782151699066,\n",
       "   -0.22457045316696167,\n",
       "   -0.4430890381336212,\n",
       "   0.20653796195983887,\n",
       "   0.14772045612335205,\n",
       "   0.06795013695955276,\n",
       "   -0.23686666786670685,\n",
       "   0.4112255573272705,\n",
       "   -0.2698410749435425,\n",
       "   0.17054636776447296,\n",
       "   0.053706325590610504,\n",
       "   0.22349080443382263,\n",
       "   0.2558572590351105,\n",
       "   -0.21674513816833496,\n",
       "   -0.16781379282474518,\n",
       "   0.4580499529838562,\n",
       "   -0.18563003838062286,\n",
       "   0.09275659173727036,\n",
       "   0.11852141469717026,\n",
       "   -0.04643775150179863,\n",
       "   0.014365958981215954,\n",
       "   0.3236365020275116,\n",
       "   0.0896298736333847,\n",
       "   0.13182896375656128,\n",
       "   0.4207831621170044,\n",
       "   0.20650151371955872,\n",
       "   0.2570439279079437,\n",
       "   0.23929323256015778,\n",
       "   0.3731355369091034,\n",
       "   -0.18872395157814026,\n",
       "   -0.2324148565530777,\n",
       "   -0.057918813079595566,\n",
       "   0.48354026675224304,\n",
       "   -0.33757856488227844,\n",
       "   0.1452251672744751,\n",
       "   -0.0031017211731523275,\n",
       "   -0.009071816690266132,\n",
       "   0.015064068138599396,\n",
       "   -0.5577238202095032,\n",
       "   -0.03618993982672691,\n",
       "   0.3808557391166687,\n",
       "   -0.3738009035587311,\n",
       "   -0.2634641230106354,\n",
       "   -0.10397452861070633,\n",
       "   5.058732858742587e-05,\n",
       "   -0.0444149449467659,\n",
       "   0.35797321796417236,\n",
       "   -0.13069294393062592,\n",
       "   -0.04405190423130989,\n",
       "   -0.32288873195648193,\n",
       "   -0.10856419056653976,\n",
       "   -0.07013599574565887,\n",
       "   0.11162982881069183,\n",
       "   -0.25398290157318115,\n",
       "   -0.2345300018787384,\n",
       "   0.21848857402801514,\n",
       "   -0.21756045520305634,\n",
       "   0.23406225442886353,\n",
       "   -0.42404231429100037,\n",
       "   0.1349431276321411,\n",
       "   -0.055665355175733566,\n",
       "   0.3740638494491577,\n",
       "   0.37340253591537476,\n",
       "   0.27637284994125366,\n",
       "   -0.13791434466838837,\n",
       "   0.06044141948223114,\n",
       "   -0.0782148614525795,\n",
       "   0.09102971106767654,\n",
       "   0.03761163353919983,\n",
       "   0.14116156101226807,\n",
       "   -0.07311581820249557,\n",
       "   -0.32126879692077637,\n",
       "   0.1342654675245285,\n",
       "   0.004114176612347364,\n",
       "   -0.17982739210128784,\n",
       "   -0.35665178298950195,\n",
       "   0.032982487231492996,\n",
       "   -0.10908140242099762,\n",
       "   0.1966267079114914,\n",
       "   0.2209126055240631,\n",
       "   0.05098523572087288,\n",
       "   -0.1480569839477539,\n",
       "   0.11666103452444077,\n",
       "   0.06280186772346497,\n",
       "   0.08144927769899368,\n",
       "   -0.16012752056121826,\n",
       "   0.1489977389574051,\n",
       "   0.07930759340524673,\n",
       "   0.18264640867710114,\n",
       "   -0.08430466800928116,\n",
       "   0.07061045616865158,\n",
       "   0.03787612169981003,\n",
       "   0.4189380705356598,\n",
       "   0.027414662763476372,\n",
       "   -0.11204203218221664,\n",
       "   -0.37865301966667175,\n",
       "   -0.330770343542099,\n",
       "   -0.34418198466300964,\n",
       "   -0.08834484964609146,\n",
       "   -0.19330376386642456,\n",
       "   0.13738003373146057,\n",
       "   -0.0022714107763022184,\n",
       "   0.090537890791893,\n",
       "   0.1760575771331787,\n",
       "   0.19486218690872192,\n",
       "   0.20748470723628998,\n",
       "   0.12348554283380508,\n",
       "   -0.20832616090774536,\n",
       "   -0.09984999895095825,\n",
       "   -0.28510650992393494,\n",
       "   0.48169511556625366,\n",
       "   -0.1726652830839157,\n",
       "   0.03965532407164574,\n",
       "   0.006443111691623926,\n",
       "   -0.350665420293808,\n",
       "   0.42169368267059326,\n",
       "   0.016046149656176567,\n",
       "   0.13119332492351532,\n",
       "   -0.17603746056556702,\n",
       "   0.05233055725693703,\n",
       "   -0.021239815279841423,\n",
       "   0.09319642931222916,\n",
       "   0.6377808451652527,\n",
       "   0.16887150704860687,\n",
       "   0.06171935051679611,\n",
       "   -0.03467853367328644,\n",
       "   0.16214917600154877,\n",
       "   0.15414535999298096,\n",
       "   0.18101781606674194,\n",
       "   -0.2202601432800293,\n",
       "   0.06765672564506531,\n",
       "   -0.015729432925581932,\n",
       "   -0.0018752291798591614,\n",
       "   0.05821579322218895,\n",
       "   0.027588479220867157,\n",
       "   -0.07953577488660812,\n",
       "   -0.11201304942369461,\n",
       "   0.1741836965084076,\n",
       "   -0.09662455320358276,\n",
       "   -0.2279873937368393,\n",
       "   -0.09358847886323929,\n",
       "   0.007985237054526806,\n",
       "   0.12249404191970825,\n",
       "   -0.24133999645709991,\n",
       "   0.33198466897010803,\n",
       "   0.07341507822275162,\n",
       "   0.20806647837162018,\n",
       "   -0.39738771319389343,\n",
       "   -0.06145118176937103,\n",
       "   0.4318276047706604,\n",
       "   -0.6654883623123169,\n",
       "   -0.12014949321746826,\n",
       "   -0.26930949091911316,\n",
       "   -0.011783208698034286,\n",
       "   0.32862144708633423,\n",
       "   0.17322088778018951,\n",
       "   -0.10575208812952042,\n",
       "   -0.281788170337677,\n",
       "   0.5291827917098999,\n",
       "   0.027850594371557236,\n",
       "   -0.2813951075077057,\n",
       "   0.31744903326034546,\n",
       "   -0.06961582601070404,\n",
       "   -0.007653912529349327,\n",
       "   -0.03373267501592636,\n",
       "   0.38657301664352417,\n",
       "   -0.24105381965637207,\n",
       "   -0.18339745700359344,\n",
       "   -0.07387268543243408,\n",
       "   -0.0449591726064682],\n",
       "  [-0.4040469229221344,\n",
       "   -0.013007267378270626,\n",
       "   -0.10247044265270233,\n",
       "   0.10342935472726822,\n",
       "   0.1453051120042801,\n",
       "   -0.12255392223596573,\n",
       "   0.25505441427230835,\n",
       "   0.18908894062042236,\n",
       "   0.13067765533924103,\n",
       "   -0.00650223670527339,\n",
       "   -0.05422317981719971,\n",
       "   0.4988860785961151,\n",
       "   -0.17248110473155975,\n",
       "   -0.27275946736335754,\n",
       "   -0.1320430487394333,\n",
       "   0.06504266709089279,\n",
       "   0.219759002327919,\n",
       "   0.02241688407957554,\n",
       "   0.2416452020406723,\n",
       "   -0.03428642824292183,\n",
       "   -0.12187288701534271,\n",
       "   0.10230212658643723,\n",
       "   -0.17672573029994965,\n",
       "   0.24139969050884247,\n",
       "   -0.3152627646923065,\n",
       "   -0.07799308747053146,\n",
       "   0.0812312588095665,\n",
       "   -0.20963358879089355,\n",
       "   -0.0027562291361391544,\n",
       "   -0.568961501121521,\n",
       "   -0.2084297239780426,\n",
       "   0.15812459588050842,\n",
       "   0.020943094044923782,\n",
       "   0.5331247448921204,\n",
       "   -0.0001148736773757264,\n",
       "   0.04034265875816345,\n",
       "   -0.029163340106606483,\n",
       "   0.27314773201942444,\n",
       "   0.13471728563308716,\n",
       "   0.04436144605278969,\n",
       "   0.35295453667640686,\n",
       "   -0.19428537786006927,\n",
       "   -0.16858315467834473,\n",
       "   -0.22576645016670227,\n",
       "   -0.05417079105973244,\n",
       "   -0.2808632552623749,\n",
       "   -0.022122494876384735,\n",
       "   -0.22417743504047394,\n",
       "   0.06830833852291107,\n",
       "   0.030041778460144997,\n",
       "   0.1608586460351944,\n",
       "   0.11519497632980347,\n",
       "   -0.39643150568008423,\n",
       "   -0.059515759348869324,\n",
       "   -0.0024587970692664385,\n",
       "   -0.10422490537166595,\n",
       "   -0.09915456920862198,\n",
       "   0.014026980847120285,\n",
       "   0.07132960855960846,\n",
       "   -0.25518715381622314,\n",
       "   -0.21165646612644196,\n",
       "   0.19490495324134827,\n",
       "   -0.2130429893732071,\n",
       "   -0.14781391620635986,\n",
       "   0.2687234878540039,\n",
       "   0.2163694202899933,\n",
       "   0.03461211174726486,\n",
       "   -0.2425246387720108,\n",
       "   0.16808678209781647,\n",
       "   0.23163501918315887,\n",
       "   0.06701083481311798,\n",
       "   -0.02721273899078369,\n",
       "   -0.01712493970990181,\n",
       "   -0.3350793123245239,\n",
       "   -0.3706912398338318,\n",
       "   -0.028482357040047646,\n",
       "   0.1461227536201477,\n",
       "   -0.4334975779056549,\n",
       "   -0.12065141648054123,\n",
       "   -0.08960272371768951,\n",
       "   -0.4140651524066925,\n",
       "   0.20377342402935028,\n",
       "   0.26424267888069153,\n",
       "   -0.12239381670951843,\n",
       "   -0.2039308398962021,\n",
       "   0.43524429202079773,\n",
       "   0.09542801976203918,\n",
       "   0.14973458647727966,\n",
       "   0.1041521355509758,\n",
       "   -0.09975191205739975,\n",
       "   -0.28079167008399963,\n",
       "   -0.26437777280807495,\n",
       "   -0.15957693755626678,\n",
       "   0.37330421805381775,\n",
       "   -0.3415870666503906,\n",
       "   -0.17395517230033875,\n",
       "   0.269295334815979,\n",
       "   -0.3091347813606262,\n",
       "   -0.14078836143016815,\n",
       "   0.11428023129701614,\n",
       "   -0.14243130385875702,\n",
       "   0.19466497004032135,\n",
       "   0.20834127068519592,\n",
       "   -0.04711996391415596,\n",
       "   0.06630698591470718,\n",
       "   0.24600408971309662,\n",
       "   0.038447603583335876,\n",
       "   0.17384621500968933,\n",
       "   0.029690774157643318,\n",
       "   -0.48029252886772156,\n",
       "   -0.31605061888694763,\n",
       "   -0.09036099165678024,\n",
       "   -0.18887561559677124,\n",
       "   0.020214010030031204,\n",
       "   0.34587207436561584,\n",
       "   -0.24245859682559967,\n",
       "   -0.23708045482635498,\n",
       "   -0.0725833922624588,\n",
       "   -0.020596446469426155,\n",
       "   -0.1713072508573532,\n",
       "   -0.3799442946910858,\n",
       "   -0.09833759069442749,\n",
       "   0.06586087495088577,\n",
       "   0.1854548454284668,\n",
       "   0.058747224509716034,\n",
       "   0.061038099229335785,\n",
       "   -0.36479803919792175,\n",
       "   -0.2021666169166565,\n",
       "   -0.09283147007226944,\n",
       "   -0.076726034283638,\n",
       "   -0.12862756848335266,\n",
       "   -0.17200061678886414,\n",
       "   -0.1044916957616806,\n",
       "   0.25425776839256287,\n",
       "   -0.18827340006828308,\n",
       "   0.09130566567182541,\n",
       "   0.2601751685142517,\n",
       "   0.07899510860443115,\n",
       "   -0.11537614464759827,\n",
       "   0.21030858159065247,\n",
       "   0.21960484981536865,\n",
       "   -0.36432310938835144,\n",
       "   0.10472845286130905,\n",
       "   -0.11305434256792068,\n",
       "   0.0748009979724884,\n",
       "   0.43602681159973145,\n",
       "   -0.0055078305304050446,\n",
       "   0.0017840805230662227,\n",
       "   -0.2595288157463074,\n",
       "   0.2082756757736206,\n",
       "   -0.2649088203907013,\n",
       "   -0.22648879885673523,\n",
       "   0.2413855344057083,\n",
       "   0.1191600114107132,\n",
       "   -0.08314219862222672,\n",
       "   -0.12216080725193024,\n",
       "   -0.49422940611839294,\n",
       "   0.4385454058647156,\n",
       "   0.25392258167266846,\n",
       "   -0.11635340005159378,\n",
       "   -0.07491347938776016,\n",
       "   -0.18091346323490143,\n",
       "   -0.5298194289207458,\n",
       "   -0.21674206852912903,\n",
       "   0.26792478561401367,\n",
       "   -0.01877214014530182,\n",
       "   -0.07221256196498871,\n",
       "   -0.1937801092863083,\n",
       "   0.3418041467666626,\n",
       "   0.1921718418598175,\n",
       "   0.3103512227535248,\n",
       "   0.64305579662323,\n",
       "   -0.3864825367927551,\n",
       "   0.7953374981880188,\n",
       "   -0.12505000829696655,\n",
       "   0.7483616471290588,\n",
       "   0.25799399614334106,\n",
       "   -0.24770566821098328,\n",
       "   -0.37781792879104614,\n",
       "   0.11663863062858582,\n",
       "   -0.17846864461898804,\n",
       "   0.27222153544425964,\n",
       "   -0.28185170888900757,\n",
       "   -0.12006309628486633,\n",
       "   0.7026370167732239,\n",
       "   -0.2252788096666336,\n",
       "   0.34198451042175293,\n",
       "   0.10614461451768875,\n",
       "   0.24699543416500092,\n",
       "   0.3040957748889923,\n",
       "   -0.24837352335453033,\n",
       "   -0.04918995499610901,\n",
       "   0.550458550453186,\n",
       "   -0.08022262901067734,\n",
       "   -0.2242528200149536,\n",
       "   -0.1509813666343689,\n",
       "   -0.15340656042099,\n",
       "   0.11113347858190536,\n",
       "   -0.031554292887449265,\n",
       "   -0.12037328630685806,\n",
       "   -0.15540924668312073,\n",
       "   0.06173209100961685,\n",
       "   0.015111537650227547,\n",
       "   0.027055859565734863,\n",
       "   -0.06373577564954758,\n",
       "   0.21922646462917328,\n",
       "   0.14711281657218933,\n",
       "   0.21719422936439514,\n",
       "   0.07411728799343109,\n",
       "   0.06839253008365631,\n",
       "   0.4290308356285095,\n",
       "   -0.006116968579590321,\n",
       "   0.1055164784193039,\n",
       "   -0.06455086171627045,\n",
       "   -0.15043260157108307,\n",
       "   -0.14192801713943481,\n",
       "   0.1726527363061905,\n",
       "   0.1282206028699875,\n",
       "   0.06789840757846832,\n",
       "   0.08919261395931244,\n",
       "   0.23117290437221527,\n",
       "   -0.10697982460260391,\n",
       "   0.015444044955074787,\n",
       "   0.12702296674251556,\n",
       "   0.4437766373157501,\n",
       "   -0.30414214730262756,\n",
       "   0.14068694412708282,\n",
       "   -0.16028903424739838,\n",
       "   -0.10612241923809052,\n",
       "   -0.1936035454273224,\n",
       "   0.14087413251399994,\n",
       "   0.10873258113861084,\n",
       "   -0.18346592783927917,\n",
       "   0.1624327152967453,\n",
       "   0.1247907429933548,\n",
       "   0.309466689825058,\n",
       "   -0.173070028424263,\n",
       "   0.20741264522075653,\n",
       "   0.11455957591533661,\n",
       "   0.2654833495616913,\n",
       "   0.04486135393381119,\n",
       "   0.11170506477355957,\n",
       "   0.016418036073446274,\n",
       "   -0.1618008315563202,\n",
       "   -0.10114365816116333,\n",
       "   0.2487524300813675,\n",
       "   0.7665553092956543,\n",
       "   0.2934233546257019,\n",
       "   -0.0981309786438942,\n",
       "   -0.19185040891170502,\n",
       "   -0.00744950445368886,\n",
       "   0.20522291958332062,\n",
       "   0.1254594922065735,\n",
       "   0.09028635919094086,\n",
       "   -0.01721389777958393,\n",
       "   0.3012901544570923,\n",
       "   0.4247046113014221,\n",
       "   0.1712668240070343,\n",
       "   0.3508589267730713,\n",
       "   0.474333792924881,\n",
       "   0.5267605781555176,\n",
       "   -0.12619903683662415,\n",
       "   -0.21092325448989868,\n",
       "   0.11692432314157486,\n",
       "   -0.34138455986976624,\n",
       "   -0.4390028715133667,\n",
       "   -0.03245577588677406,\n",
       "   -0.4132477045059204,\n",
       "   0.16794642806053162,\n",
       "   0.10433095693588257,\n",
       "   0.22383278608322144,\n",
       "   -0.168983593583107,\n",
       "   -0.013874867931008339,\n",
       "   0.0025530215352773666,\n",
       "   0.27276208996772766,\n",
       "   0.23192831873893738,\n",
       "   -0.006252425257116556,\n",
       "   -0.03470802679657936,\n",
       "   0.16773183643817902,\n",
       "   0.235477477312088,\n",
       "   -0.3278358280658722,\n",
       "   -0.16834309697151184,\n",
       "   -0.017744487151503563,\n",
       "   0.24668414890766144,\n",
       "   -0.02020339109003544,\n",
       "   -0.08809307962656021,\n",
       "   0.06602860242128372,\n",
       "   0.1790386587381363,\n",
       "   0.4836558401584625,\n",
       "   -0.07303351163864136,\n",
       "   0.043879907578229904,\n",
       "   -0.23364877700805664,\n",
       "   -0.08682698756456375,\n",
       "   0.011286372318863869,\n",
       "   0.07300631701946259,\n",
       "   0.1429343819618225,\n",
       "   0.20160536468029022,\n",
       "   0.041605010628700256,\n",
       "   0.05537930876016617,\n",
       "   -0.13247603178024292,\n",
       "   -0.032635897397994995,\n",
       "   -0.20362262427806854,\n",
       "   -0.3690440356731415,\n",
       "   0.06732721626758575,\n",
       "   0.04781554639339447,\n",
       "   -0.10531243681907654,\n",
       "   0.3012903332710266,\n",
       "   0.18954965472221375,\n",
       "   -0.07064705342054367,\n",
       "   -0.028474120423197746,\n",
       "   -0.45728909969329834,\n",
       "   0.08773909509181976,\n",
       "   -0.18031150102615356,\n",
       "   0.009878234006464481,\n",
       "   -0.10766518115997314,\n",
       "   -0.15344327688217163,\n",
       "   -0.1983681470155716,\n",
       "   0.04000325873494148,\n",
       "   0.11686043441295624,\n",
       "   -0.2781752645969391,\n",
       "   -0.2616516649723053,\n",
       "   -0.32242265343666077,\n",
       "   -0.11807259917259216,\n",
       "   -0.17440877854824066,\n",
       "   0.14054438471794128,\n",
       "   -0.020053938031196594,\n",
       "   0.05114232748746872,\n",
       "   -0.03128005564212799,\n",
       "   0.12229980528354645,\n",
       "   0.10087115317583084,\n",
       "   0.12934544682502747,\n",
       "   -0.24915212392807007,\n",
       "   -0.16720829904079437,\n",
       "   0.19397683441638947,\n",
       "   -0.052004143595695496,\n",
       "   -0.012171968817710876,\n",
       "   -0.3335229754447937,\n",
       "   -0.24970538914203644,\n",
       "   0.38905906677246094,\n",
       "   -0.06996995955705643,\n",
       "   -0.014193568378686905,\n",
       "   -0.214137002825737,\n",
       "   0.0942140445113182,\n",
       "   0.16697978973388672,\n",
       "   0.13987208902835846,\n",
       "   -0.24602244794368744,\n",
       "   0.21763962507247925,\n",
       "   -0.30447089672088623,\n",
       "   0.2614800035953522,\n",
       "   0.03844335302710533,\n",
       "   -0.2167222946882248,\n",
       "   0.39410242438316345,\n",
       "   0.22821258008480072,\n",
       "   -0.12224381417036057,\n",
       "   0.1697704941034317,\n",
       "   -0.24780596792697906,\n",
       "   0.16913461685180664,\n",
       "   -0.05900362506508827,\n",
       "   -0.23506365716457367,\n",
       "   0.07231324911117554,\n",
       "   0.484038382768631,\n",
       "   0.24679617583751678,\n",
       "   0.6211674213409424,\n",
       "   0.04898207634687424,\n",
       "   -0.013337538577616215,\n",
       "   -0.00010498862684471533,\n",
       "   -0.3113279938697815,\n",
       "   -0.1315823793411255,\n",
       "   -0.43118953704833984,\n",
       "   -0.07551377266645432,\n",
       "   0.20715612173080444,\n",
       "   -0.09913627058267593,\n",
       "   0.025744717568159103,\n",
       "   0.48657679557800293,\n",
       "   -0.08505593240261078,\n",
       "   -0.3330843150615692,\n",
       "   0.22217024862766266,\n",
       "   -0.17344295978546143,\n",
       "   0.049075789749622345,\n",
       "   -0.1999395489692688,\n",
       "   0.12732820212841034,\n",
       "   -0.09477365761995316,\n",
       "   0.05930560082197189,\n",
       "   0.001397682586684823,\n",
       "   0.40461063385009766,\n",
       "   -0.5339139699935913,\n",
       "   -0.14910313487052917,\n",
       "   0.237467423081398,\n",
       "   -0.278715580701828,\n",
       "   0.2525823712348938,\n",
       "   -0.25178980827331543,\n",
       "   -0.9455535411834717,\n",
       "   0.35129252076148987,\n",
       "   -0.8524883389472961,\n",
       "   0.09497607499361038,\n",
       "   0.016547590494155884,\n",
       "   0.22760266065597534,\n",
       "   0.04750778153538704,\n",
       "   -0.027061937376856804,\n",
       "   0.11630099266767502,\n",
       "   -0.0324515700340271,\n",
       "   0.1658131629228592,\n",
       "   -0.05380868911743164,\n",
       "   -0.08455506712198257,\n",
       "   0.23704971373081207,\n",
       "   -0.5604739785194397,\n",
       "   -0.11725746095180511,\n",
       "   -0.1456516981124878,\n",
       "   -0.0787852331995964,\n",
       "   0.19362659752368927,\n",
       "   0.5949996709823608,\n",
       "   0.7412646412849426,\n",
       "   -0.09443056583404541,\n",
       "   -0.2862771451473236,\n",
       "   -0.3504449427127838,\n",
       "   -0.04790673032402992,\n",
       "   0.06967751681804657,\n",
       "   0.03928457573056221,\n",
       "   -0.013841893523931503,\n",
       "   -0.10236921161413193,\n",
       "   -0.18080124258995056,\n",
       "   0.010817783884704113,\n",
       "   0.10237513482570648,\n",
       "   0.1822671741247177,\n",
       "   0.13342005014419556,\n",
       "   -0.15410974621772766,\n",
       "   -0.08755319565534592,\n",
       "   0.15052032470703125,\n",
       "   0.303697407245636,\n",
       "   0.2778022587299347,\n",
       "   0.24230678379535675,\n",
       "   -0.03876839950680733,\n",
       "   0.34377962350845337,\n",
       "   0.31737178564071655,\n",
       "   0.10557416826486588,\n",
       "   -0.11109750717878342,\n",
       "   -0.15754671394824982,\n",
       "   0.21815183758735657,\n",
       "   -0.36953026056289673,\n",
       "   0.0900227278470993,\n",
       "   0.09987177699804306,\n",
       "   0.06710685044527054,\n",
       "   0.16950532793998718,\n",
       "   -0.03232567757368088,\n",
       "   0.21382655203342438,\n",
       "   -0.269631952047348,\n",
       "   0.3467106521129608,\n",
       "   -0.19183030724525452,\n",
       "   0.3990117013454437,\n",
       "   0.5268697738647461,\n",
       "   0.13443592190742493,\n",
       "   -0.5008947253227234,\n",
       "   -0.3026987910270691,\n",
       "   0.07103808224201202,\n",
       "   0.5278068780899048,\n",
       "   -0.2671490013599396,\n",
       "   0.4369067847728729,\n",
       "   -0.4802764356136322,\n",
       "   -0.40487611293792725,\n",
       "   0.4786458909511566,\n",
       "   0.2622503340244293,\n",
       "   0.5893493890762329,\n",
       "   -0.00019733051885850728,\n",
       "   0.2131252884864807,\n",
       "   0.24673759937286377,\n",
       "   0.2820870280265808,\n",
       "   0.1871795356273651,\n",
       "   -0.4422435462474823,\n",
       "   -0.1283498853445053,\n",
       "   -0.2351832240819931,\n",
       "   0.21898217499256134,\n",
       "   0.04329977184534073,\n",
       "   -0.07354767620563507,\n",
       "   0.06810370832681656,\n",
       "   0.23929385840892792,\n",
       "   -0.11905814707279205,\n",
       "   -0.07074656337499619,\n",
       "   0.13437466323375702,\n",
       "   0.8829225301742554,\n",
       "   -0.013043608516454697,\n",
       "   0.30273404717445374,\n",
       "   0.4876132905483246,\n",
       "   -0.39955636858940125,\n",
       "   0.3429582715034485,\n",
       "   0.13938821852207184,\n",
       "   0.2241424024105072,\n",
       "   0.06270559877157211,\n",
       "   0.09403461962938309,\n",
       "   0.09735548496246338,\n",
       "   -0.06829363107681274,\n",
       "   -0.1550721526145935,\n",
       "   -0.2499285638332367,\n",
       "   -0.20255860686302185,\n",
       "   -0.5686984658241272,\n",
       "   0.297793447971344,\n",
       "   -0.20471440255641937,\n",
       "   0.03080056607723236,\n",
       "   0.0798010528087616,\n",
       "   0.14483775198459625,\n",
       "   0.04165840521454811,\n",
       "   0.24064281582832336,\n",
       "   0.006632594391703606,\n",
       "   0.2885669469833374,\n",
       "   0.21322667598724365,\n",
       "   0.33644169569015503,\n",
       "   0.2534501254558563,\n",
       "   -0.5096054077148438,\n",
       "   -0.19772875308990479,\n",
       "   -0.009806572459638119,\n",
       "   -0.015243478119373322,\n",
       "   0.16441352665424347,\n",
       "   -0.264480859041214,\n",
       "   -0.3185324966907501,\n",
       "   -0.642041802406311,\n",
       "   -0.045864131301641464,\n",
       "   0.3040487468242645,\n",
       "   -0.365604966878891,\n",
       "   0.01904495246708393,\n",
       "   0.10112268477678299,\n",
       "   -0.07926517724990845,\n",
       "   -0.19805803894996643,\n",
       "   0.08064141869544983,\n",
       "   -0.15343162417411804,\n",
       "   0.08537712693214417,\n",
       "   0.4080638289451599,\n",
       "   -0.07441261410713196,\n",
       "   -0.2806636393070221,\n",
       "   0.01849406771361828,\n",
       "   0.3022502660751343,\n",
       "   0.12154628336429596,\n",
       "   -0.21070410311222076,\n",
       "   0.4307021498680115,\n",
       "   -0.47031182050704956,\n",
       "   -0.2629356384277344,\n",
       "   -0.1035870686173439,\n",
       "   0.21543660759925842,\n",
       "   0.1363147497177124,\n",
       "   0.11987601220607758,\n",
       "   0.0926351323723793,\n",
       "   -0.13185419142246246,\n",
       "   -0.18322819471359253,\n",
       "   0.09514594823122025,\n",
       "   0.2641548216342926,\n",
       "   -0.12749508023262024,\n",
       "   -0.10990781337022781,\n",
       "   -0.1724201887845993,\n",
       "   -0.018341632559895515,\n",
       "   -0.30604371428489685,\n",
       "   -0.010590710677206516,\n",
       "   -0.05993127077817917,\n",
       "   0.09162306040525436,\n",
       "   0.09735386073589325,\n",
       "   0.13492833077907562,\n",
       "   -0.31854090094566345,\n",
       "   0.16975019872188568,\n",
       "   -0.2577683925628662,\n",
       "   -0.12103083729743958,\n",
       "   0.12591233849525452,\n",
       "   0.1538807600736618,\n",
       "   -0.20141109824180603,\n",
       "   -0.15866515040397644,\n",
       "   0.12080893665552139,\n",
       "   0.04116220772266388,\n",
       "   0.09976737946271896,\n",
       "   0.2575889229774475,\n",
       "   0.03509654477238655,\n",
       "   -0.2286946326494217,\n",
       "   -0.08349362015724182,\n",
       "   0.1424897164106369,\n",
       "   0.046630680561065674,\n",
       "   -0.36045771837234497,\n",
       "   0.0659409686923027,\n",
       "   0.1358204036951065,\n",
       "   -0.08454765379428864,\n",
       "   -0.5128017067909241,\n",
       "   0.20534539222717285,\n",
       "   0.17162436246871948,\n",
       "   0.013776835985481739,\n",
       "   -0.4263346493244171,\n",
       "   0.19677728414535522,\n",
       "   -0.00454443646594882,\n",
       "   -0.006976690609008074,\n",
       "   0.1540151685476303,\n",
       "   -0.04092022776603699,\n",
       "   0.01958843506872654,\n",
       "   0.10938426852226257,\n",
       "   0.2167302668094635,\n",
       "   0.12089190632104874,\n",
       "   -0.2039533406496048,\n",
       "   0.04365456476807594,\n",
       "   0.260488897562027,\n",
       "   0.3350318968296051,\n",
       "   0.17188453674316406,\n",
       "   0.011980189010500908,\n",
       "   -0.026033131405711174,\n",
       "   -0.0632946714758873,\n",
       "   0.16377726197242737,\n",
       "   -0.16997964680194855,\n",
       "   0.21267782151699066,\n",
       "   -0.22457045316696167,\n",
       "   -0.4430890381336212,\n",
       "   0.20653796195983887,\n",
       "   0.14772045612335205,\n",
       "   0.06795013695955276,\n",
       "   -0.23686666786670685,\n",
       "   0.4112255573272705,\n",
       "   -0.2698410749435425,\n",
       "   0.17054636776447296,\n",
       "   0.053706325590610504,\n",
       "   0.22349080443382263,\n",
       "   0.2558572590351105,\n",
       "   -0.21674513816833496,\n",
       "   -0.16781379282474518,\n",
       "   0.4580499529838562,\n",
       "   -0.18563003838062286,\n",
       "   0.09275659173727036,\n",
       "   0.11852141469717026,\n",
       "   -0.04643775150179863,\n",
       "   0.014365958981215954,\n",
       "   0.3236365020275116,\n",
       "   0.0896298736333847,\n",
       "   0.13182896375656128,\n",
       "   0.4207831621170044,\n",
       "   0.20650151371955872,\n",
       "   0.2570439279079437,\n",
       "   0.23929323256015778,\n",
       "   0.3731355369091034,\n",
       "   -0.18872395157814026,\n",
       "   -0.2324148565530777,\n",
       "   -0.057918813079595566,\n",
       "   0.48354026675224304,\n",
       "   -0.33757856488227844,\n",
       "   0.1452251672744751,\n",
       "   -0.0031017211731523275,\n",
       "   -0.009071816690266132,\n",
       "   0.015064068138599396,\n",
       "   -0.5577238202095032,\n",
       "   -0.03618993982672691,\n",
       "   0.3808557391166687,\n",
       "   -0.3738009035587311,\n",
       "   -0.2634641230106354,\n",
       "   -0.10397452861070633,\n",
       "   5.058732858742587e-05,\n",
       "   -0.0444149449467659,\n",
       "   0.35797321796417236,\n",
       "   -0.13069294393062592,\n",
       "   -0.04405190423130989,\n",
       "   -0.32288873195648193,\n",
       "   -0.10856419056653976,\n",
       "   -0.07013599574565887,\n",
       "   0.11162982881069183,\n",
       "   -0.25398290157318115,\n",
       "   -0.2345300018787384,\n",
       "   0.21848857402801514,\n",
       "   -0.21756045520305634,\n",
       "   0.23406225442886353,\n",
       "   -0.42404231429100037,\n",
       "   0.1349431276321411,\n",
       "   -0.055665355175733566,\n",
       "   0.3740638494491577,\n",
       "   0.37340253591537476,\n",
       "   0.27637284994125366,\n",
       "   -0.13791434466838837,\n",
       "   0.06044141948223114,\n",
       "   -0.0782148614525795,\n",
       "   0.09102971106767654,\n",
       "   0.03761163353919983,\n",
       "   0.14116156101226807,\n",
       "   -0.07311581820249557,\n",
       "   -0.32126879692077637,\n",
       "   0.1342654675245285,\n",
       "   0.004114176612347364,\n",
       "   -0.17982739210128784,\n",
       "   -0.35665178298950195,\n",
       "   0.032982487231492996,\n",
       "   -0.10908140242099762,\n",
       "   0.1966267079114914,\n",
       "   0.2209126055240631,\n",
       "   0.05098523572087288,\n",
       "   -0.1480569839477539,\n",
       "   0.11666103452444077,\n",
       "   0.06280186772346497,\n",
       "   0.08144927769899368,\n",
       "   -0.16012752056121826,\n",
       "   0.1489977389574051,\n",
       "   0.07930759340524673,\n",
       "   0.18264640867710114,\n",
       "   -0.08430466800928116,\n",
       "   0.07061045616865158,\n",
       "   0.03787612169981003,\n",
       "   0.4189380705356598,\n",
       "   0.027414662763476372,\n",
       "   -0.11204203218221664,\n",
       "   -0.37865301966667175,\n",
       "   -0.330770343542099,\n",
       "   -0.34418198466300964,\n",
       "   -0.08834484964609146,\n",
       "   -0.19330376386642456,\n",
       "   0.13738003373146057,\n",
       "   -0.0022714107763022184,\n",
       "   0.090537890791893,\n",
       "   0.1760575771331787,\n",
       "   0.19486218690872192,\n",
       "   0.20748470723628998,\n",
       "   0.12348554283380508,\n",
       "   -0.20832616090774536,\n",
       "   -0.09984999895095825,\n",
       "   -0.28510650992393494,\n",
       "   0.48169511556625366,\n",
       "   -0.1726652830839157,\n",
       "   0.03965532407164574,\n",
       "   0.006443111691623926,\n",
       "   -0.350665420293808,\n",
       "   0.42169368267059326,\n",
       "   0.016046149656176567,\n",
       "   0.13119332492351532,\n",
       "   -0.17603746056556702,\n",
       "   0.05233055725693703,\n",
       "   -0.021239815279841423,\n",
       "   0.09319642931222916,\n",
       "   0.6377808451652527,\n",
       "   0.16887150704860687,\n",
       "   0.06171935051679611,\n",
       "   -0.03467853367328644,\n",
       "   0.16214917600154877,\n",
       "   0.15414535999298096,\n",
       "   0.18101781606674194,\n",
       "   -0.2202601432800293,\n",
       "   0.06765672564506531,\n",
       "   -0.015729432925581932,\n",
       "   -0.0018752291798591614,\n",
       "   0.05821579322218895,\n",
       "   0.027588479220867157,\n",
       "   -0.07953577488660812,\n",
       "   -0.11201304942369461,\n",
       "   0.1741836965084076,\n",
       "   -0.09662455320358276,\n",
       "   -0.2279873937368393,\n",
       "   -0.09358847886323929,\n",
       "   0.007985237054526806,\n",
       "   0.12249404191970825,\n",
       "   -0.24133999645709991,\n",
       "   0.33198466897010803,\n",
       "   0.07341507822275162,\n",
       "   0.20806647837162018,\n",
       "   -0.39738771319389343,\n",
       "   -0.06145118176937103,\n",
       "   0.4318276047706604,\n",
       "   -0.6654883623123169,\n",
       "   -0.12014949321746826,\n",
       "   -0.26930949091911316,\n",
       "   -0.011783208698034286,\n",
       "   0.32862144708633423,\n",
       "   0.17322088778018951,\n",
       "   -0.10575208812952042,\n",
       "   -0.281788170337677,\n",
       "   0.5291827917098999,\n",
       "   0.027850594371557236,\n",
       "   -0.2813951075077057,\n",
       "   0.31744903326034546,\n",
       "   -0.06961582601070404,\n",
       "   -0.007653912529349327,\n",
       "   -0.03373267501592636,\n",
       "   0.38657301664352417,\n",
       "   -0.24105381965637207,\n",
       "   -0.18339745700359344,\n",
       "   -0.07387268543243408,\n",
       "   -0.0449591726064682]]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Score: 41.92\n",
      "Comment: ['Tokenizer\\'s normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task \\n [This colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) implements a token classification input pipeline extending the logic from [this hugging example](https://huggingface.co/transformers/custom_datasets.html#tok-ner).\\r\\n\\r\\nThe pipeline works fine with most instance in different languages, but unfortunately, [the Japanese Kana ligature (a form of abbreviation? I don\\'t know Japanese well)](https://en.wikipedia.org/wiki/Kana_ligature) break the alignment of `return_offsets_mapping`:\\r\\n![image](https://user-images.githubusercontent.com/50871412/122904371-db192700-d382-11eb-8917-1775db76db69.png)\\r\\n\\r\\nWithout the try catch block, it riase `ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true`, example shown here [(another colab notebook)](https://colab.research.google.com/drive/1MmOqf3ppzzdKKyMWkn0bJy6DqzOO0SSm?usp=sharing)\\r\\n\\r\\nIt is clear that the normalizer is the process that break the alignment, as it is observed that `tokenizer._tokenizer.normalizer.normalize_str(\\'ヿ\\')` return \\'コト\\'.\\r\\n\\r\\nOne workaround is to include `tokenizer._tokenizer.normalizer.normalize_str` before the tokenizer preprocessing pipeline, which is also provided in the [first colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) with the name `udposTestDatasetWorkaround`.\\r\\n\\r\\nI guess similar logics should be included inside the tokenizer and the offsets_mapping generation process such that user don\\'t need to include them in their code. But I don\\'t understand the code of tokenizer well that I think I am not able to do this.\\r\\n\\r\\np.s.\\r\\n**I am using my own dataset building script in the provided example, but the script should be equivalent to the changes made by this [update](https://github.com/huggingface/datasets/pull/2466)**\\r\\n`get_dataset `is just a simple wrapping for `load_dataset`\\r\\nand the `tokenizer` is just `XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")` \\n Hi @jerryIsHere, thanks for reporting the issue. But are you sure this is a bug in HuggingFace **Datasets**?', 'Tokenizer\\'s normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task \\n [This colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) implements a token classification input pipeline extending the logic from [this hugging example](https://huggingface.co/transformers/custom_datasets.html#tok-ner).\\r\\n\\r\\nThe pipeline works fine with most instance in different languages, but unfortunately, [the Japanese Kana ligature (a form of abbreviation? I don\\'t know Japanese well)](https://en.wikipedia.org/wiki/Kana_ligature) break the alignment of `return_offsets_mapping`:\\r\\n![image](https://user-images.githubusercontent.com/50871412/122904371-db192700-d382-11eb-8917-1775db76db69.png)\\r\\n\\r\\nWithout the try catch block, it riase `ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true`, example shown here [(another colab notebook)](https://colab.research.google.com/drive/1MmOqf3ppzzdKKyMWkn0bJy6DqzOO0SSm?usp=sharing)\\r\\n\\r\\nIt is clear that the normalizer is the process that break the alignment, as it is observed that `tokenizer._tokenizer.normalizer.normalize_str(\\'ヿ\\')` return \\'コト\\'.\\r\\n\\r\\nOne workaround is to include `tokenizer._tokenizer.normalizer.normalize_str` before the tokenizer preprocessing pipeline, which is also provided in the [first colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) with the name `udposTestDatasetWorkaround`.\\r\\n\\r\\nI guess similar logics should be included inside the tokenizer and the offsets_mapping generation process such that user don\\'t need to include them in their code. But I don\\'t understand the code of tokenizer well that I think I am not able to do this.\\r\\n\\r\\np.s.\\r\\n**I am using my own dataset building script in the provided example, but the script should be equivalent to the changes made by this [update](https://github.com/huggingface/datasets/pull/2466)**\\r\\n`get_dataset `is just a simple wrapping for `load_dataset`\\r\\nand the `tokenizer` is just `XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")` \\n > Hi @jerryIsHere, thanks for reporting the issue. But are you sure this is a bug in HuggingFace **Datasets**?\\r\\n\\r\\nOh, I am sorry\\r\\nI would reopen the post on huggingface/transformers', '❓ On-the-fly tokenization with datasets, tokenizers, and torch Datasets and Dataloaders \\n Hi there,\\r\\n\\r\\nI have a question regarding \"on-the-fly\" tokenization. This question was elicited by reading the \"How to train a new language model from scratch using Transformers and Tokenizers\" [here](https://huggingface.co/blog/how-to-train). Towards the end there is this sentence: \"If your dataset is very large, you can opt to load and tokenize examples on the fly, rather than as a preprocessing step\". I\\'ve tried coming up with a solution that would combine both `datasets` and `tokenizers`, but did not manage to find a good pattern.\\r\\n\\r\\nI guess the solution would entail wrapping a dataset into a Pytorch dataset.\\r\\n\\r\\nAs a concrete example from the [docs](https://huggingface.co/transformers/custom_datasets.html)\\r\\n\\r\\n```python\\r\\nimport torch\\r\\n\\r\\nclass SquadDataset(torch.utils.data.Dataset):\\r\\n    def __init__(self, encodings):\\r\\n        # instead of doing this beforehand, I\\'d like to do tokenization on the fly\\r\\n        self.encodings = encodings \\r\\n\\r\\n    def __getitem__(self, idx):\\r\\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.encodings.input_ids)\\r\\n\\r\\ntrain_dataset = SquadDataset(train_encodings)\\r\\n```\\r\\n\\r\\nHow would one implement this with \"on-the-fly\" tokenization exploiting the vectorized capabilities of tokenizers?\\r\\n\\r\\n\\r\\n----\\r\\n\\r\\nEdit: I have come up with this solution. It does what I want, but I feel it\\'s not very elegant\\r\\n\\r\\n```python\\r\\nclass CustomPytorchDataset(Dataset):\\r\\n    def __init__(self):\\r\\n        self.dataset = some_hf_dataset(...)\\r\\n        self.tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\\r\\n\\r\\n    def __getitem__(self, batch_idx):\\r\\n        instance = self.dataset[text_col][batch_idx]\\r\\n        tokenized_text = self.tokenizer(instance, truncation=True, padding=True)\\r\\n        return tokenized_text\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.dataset)\\r\\n\\r\\n    @staticmethod\\r\\n    def collate_fn(batch):\\r\\n        # batch is a list, however it will always contain 1 item because we should not use the\\r\\n        # batch_size argument as batch_size is controlled by the sampler\\r\\n        return {k: torch.tensor(v) for k, v in batch[0].items()}\\r\\n\\r\\ntorch_ds = CustomPytorchDataset()\\r\\n\\r\\n# NOTE: batch_sampler returns list of integers and since here we have SequentialSampler\\r\\n# it returns: [1, 2, 3], [4, 5, 6], etc. - check calling `list(batch_sampler)`\\r\\nbatch_sampler = BatchSampler(SequentialSampler(torch_ds), batch_size=3, drop_last=True)\\r\\n\\r\\n# NOTE: no `batch_size` as now the it is controlled by the sampler!\\r\\ndl = DataLoader(dataset=torch_ds, sampler=batch_sampler, collate_fn=torch_ds.collate_fn)\\r\\n``` \\n We\\'re working on adding on-the-fly transforms in datasets.\\r\\nCurrently the only on-the-fly functions that can be applied are in `set_format` in which we transform the data in either numpy/torch/tf tensors or pandas.\\r\\nFor example\\r\\n```python\\r\\ndataset.set_format(\"torch\")\\r\\n```\\r\\napplies `torch.Tensor` to the dataset entries on-the-fly.\\r\\n\\r\\nWe plan to extend this to user-defined formatting transforms.\\r\\nFor example\\r\\n```python\\r\\ndataset.set_format(transform=tokenize)\\r\\n```\\r\\n\\r\\nWhat do you think ?', 'using map on loaded Tokenizer 10x - 100x slower than default Tokenizer? \\n This could total relate to me misunderstanding particular call functions, but I added words to a GPT2Tokenizer, and saved it to disk (note I\\'m only showing snippets but I can share more) and the map function ran much slower: \\r\\n\\r\\n````\\r\\ndef save_tokenizer(original_tokenizer,text,path=\"simpledata/tokenizer\"):\\r\\n    words_unique = set(text.split(\" \"))\\r\\n    for i in words_unique:\\r\\n        original_tokenizer.add_tokens(i)\\r\\n    original_tokenizer.save_pretrained(path)\\r\\n\\r\\ntokenizer2 = GPT2Tokenizer.from_pretrained(os.path.join(experiment_path,experiment_name,\"tokenizer_squad\"))\\r\\n\\r\\ntrain_set_baby=Dataset.from_dict({\"text\":[train_set[\"text\"][0][0:50]]})\\r\\n````\\r\\n\\r\\nI then applied the dataset map function on a fairly small set of text:\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\n\\r\\n\\r\\nThe run time for train_set_baby.map was 6 seconds, and the batch itself was 2.6 seconds\\r\\n\\r\\n**100% 1/1 [00:02<00:00, 2.60s/ba] CPU times: user 5.96 s, sys: 36 ms, total: 5.99 s Wall time: 5.99 s**\\r\\n\\r\\nIn comparison using (even after adding additional tokens): \\r\\n`\\r\\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")`\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\nThe time is \\r\\n**100% 1/1 [00:00<00:00, 34.09ba/s] CPU times: user 68.1 ms, sys: 16 µs, total: 68.1 ms Wall time: 62.9 ms**\\r\\n\\r\\nIt seems this might relate to the tokenizer save or load function, however, the issue appears to come up when I apply the loaded tokenizer  to the map function. \\r\\n\\r\\nI should also add that playing around with the amount of words I add to the tokenizer before I save it to disk and load it into memory  appears to impact the time it takes to run the map function. \\r\\n\\r\\n\\r\\n \\n Hi @wumpusman \\r\\n`datasets` has a caching mechanism that allows to cache the results of `.map` so that when you want to re-run it later it doesn\\'t recompute it again.\\r\\nSo when you do `.map`, what actually happens is:\\r\\n1. compute the hash used to identify your `map` for the cache\\r\\n2. apply your function on every batch\\r\\n\\r\\nThis can explain the time difference between your different experiments.\\r\\n\\r\\nThe hash computation time depends of how complex your function is. For a tokenizer, the hash computation scans the lists of the words in the tokenizer to identify this tokenizer. Usually it takes 2-3 seconds.\\r\\n\\r\\nAlso note that you can disable caching though using\\r\\n```python\\r\\nimport datasets\\r\\n\\r\\ndatasets.set_caching_enabled(False)\\r\\n```', 'using map on loaded Tokenizer 10x - 100x slower than default Tokenizer? \\n This could total relate to me misunderstanding particular call functions, but I added words to a GPT2Tokenizer, and saved it to disk (note I\\'m only showing snippets but I can share more) and the map function ran much slower: \\r\\n\\r\\n````\\r\\ndef save_tokenizer(original_tokenizer,text,path=\"simpledata/tokenizer\"):\\r\\n    words_unique = set(text.split(\" \"))\\r\\n    for i in words_unique:\\r\\n        original_tokenizer.add_tokens(i)\\r\\n    original_tokenizer.save_pretrained(path)\\r\\n\\r\\ntokenizer2 = GPT2Tokenizer.from_pretrained(os.path.join(experiment_path,experiment_name,\"tokenizer_squad\"))\\r\\n\\r\\ntrain_set_baby=Dataset.from_dict({\"text\":[train_set[\"text\"][0][0:50]]})\\r\\n````\\r\\n\\r\\nI then applied the dataset map function on a fairly small set of text:\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\n\\r\\n\\r\\nThe run time for train_set_baby.map was 6 seconds, and the batch itself was 2.6 seconds\\r\\n\\r\\n**100% 1/1 [00:02<00:00, 2.60s/ba] CPU times: user 5.96 s, sys: 36 ms, total: 5.99 s Wall time: 5.99 s**\\r\\n\\r\\nIn comparison using (even after adding additional tokens): \\r\\n`\\r\\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")`\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\nThe time is \\r\\n**100% 1/1 [00:00<00:00, 34.09ba/s] CPU times: user 68.1 ms, sys: 16 µs, total: 68.1 ms Wall time: 62.9 ms**\\r\\n\\r\\nIt seems this might relate to the tokenizer save or load function, however, the issue appears to come up when I apply the loaded tokenizer  to the map function. \\r\\n\\r\\nI should also add that playing around with the amount of words I add to the tokenizer before I save it to disk and load it into memory  appears to impact the time it takes to run the map function. \\r\\n\\r\\n\\r\\n \\n Hi @lhoestq ,\\r\\n\\r\\nThanks for the reply. It\\'s entirely possible that is the issue. Since it\\'s a side project I won\\'t be looking at it till later this week, but, I\\'ll verify it by disabling caching and hopefully I\\'ll see the same runtime. \\r\\n\\r\\nAppreciate the reference,\\r\\n\\r\\nMichael']\n",
      "URL: ['https://github.com/huggingface/datasets/issues/2532', 'https://github.com/huggingface/datasets/issues/2532', 'https://github.com/huggingface/datasets/issues/1167', 'https://github.com/huggingface/datasets/issues/1830', 'https://github.com/huggingface/datasets/issues/1830']\n",
      "\n",
      "2. Score: 41.92\n",
      "Comment: ['Tokenizer\\'s normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task \\n [This colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) implements a token classification input pipeline extending the logic from [this hugging example](https://huggingface.co/transformers/custom_datasets.html#tok-ner).\\r\\n\\r\\nThe pipeline works fine with most instance in different languages, but unfortunately, [the Japanese Kana ligature (a form of abbreviation? I don\\'t know Japanese well)](https://en.wikipedia.org/wiki/Kana_ligature) break the alignment of `return_offsets_mapping`:\\r\\n![image](https://user-images.githubusercontent.com/50871412/122904371-db192700-d382-11eb-8917-1775db76db69.png)\\r\\n\\r\\nWithout the try catch block, it riase `ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true`, example shown here [(another colab notebook)](https://colab.research.google.com/drive/1MmOqf3ppzzdKKyMWkn0bJy6DqzOO0SSm?usp=sharing)\\r\\n\\r\\nIt is clear that the normalizer is the process that break the alignment, as it is observed that `tokenizer._tokenizer.normalizer.normalize_str(\\'ヿ\\')` return \\'コト\\'.\\r\\n\\r\\nOne workaround is to include `tokenizer._tokenizer.normalizer.normalize_str` before the tokenizer preprocessing pipeline, which is also provided in the [first colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) with the name `udposTestDatasetWorkaround`.\\r\\n\\r\\nI guess similar logics should be included inside the tokenizer and the offsets_mapping generation process such that user don\\'t need to include them in their code. But I don\\'t understand the code of tokenizer well that I think I am not able to do this.\\r\\n\\r\\np.s.\\r\\n**I am using my own dataset building script in the provided example, but the script should be equivalent to the changes made by this [update](https://github.com/huggingface/datasets/pull/2466)**\\r\\n`get_dataset `is just a simple wrapping for `load_dataset`\\r\\nand the `tokenizer` is just `XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")` \\n Hi @jerryIsHere, thanks for reporting the issue. But are you sure this is a bug in HuggingFace **Datasets**?', 'Tokenizer\\'s normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task \\n [This colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) implements a token classification input pipeline extending the logic from [this hugging example](https://huggingface.co/transformers/custom_datasets.html#tok-ner).\\r\\n\\r\\nThe pipeline works fine with most instance in different languages, but unfortunately, [the Japanese Kana ligature (a form of abbreviation? I don\\'t know Japanese well)](https://en.wikipedia.org/wiki/Kana_ligature) break the alignment of `return_offsets_mapping`:\\r\\n![image](https://user-images.githubusercontent.com/50871412/122904371-db192700-d382-11eb-8917-1775db76db69.png)\\r\\n\\r\\nWithout the try catch block, it riase `ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true`, example shown here [(another colab notebook)](https://colab.research.google.com/drive/1MmOqf3ppzzdKKyMWkn0bJy6DqzOO0SSm?usp=sharing)\\r\\n\\r\\nIt is clear that the normalizer is the process that break the alignment, as it is observed that `tokenizer._tokenizer.normalizer.normalize_str(\\'ヿ\\')` return \\'コト\\'.\\r\\n\\r\\nOne workaround is to include `tokenizer._tokenizer.normalizer.normalize_str` before the tokenizer preprocessing pipeline, which is also provided in the [first colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) with the name `udposTestDatasetWorkaround`.\\r\\n\\r\\nI guess similar logics should be included inside the tokenizer and the offsets_mapping generation process such that user don\\'t need to include them in their code. But I don\\'t understand the code of tokenizer well that I think I am not able to do this.\\r\\n\\r\\np.s.\\r\\n**I am using my own dataset building script in the provided example, but the script should be equivalent to the changes made by this [update](https://github.com/huggingface/datasets/pull/2466)**\\r\\n`get_dataset `is just a simple wrapping for `load_dataset`\\r\\nand the `tokenizer` is just `XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")` \\n > Hi @jerryIsHere, thanks for reporting the issue. But are you sure this is a bug in HuggingFace **Datasets**?\\r\\n\\r\\nOh, I am sorry\\r\\nI would reopen the post on huggingface/transformers', '❓ On-the-fly tokenization with datasets, tokenizers, and torch Datasets and Dataloaders \\n Hi there,\\r\\n\\r\\nI have a question regarding \"on-the-fly\" tokenization. This question was elicited by reading the \"How to train a new language model from scratch using Transformers and Tokenizers\" [here](https://huggingface.co/blog/how-to-train). Towards the end there is this sentence: \"If your dataset is very large, you can opt to load and tokenize examples on the fly, rather than as a preprocessing step\". I\\'ve tried coming up with a solution that would combine both `datasets` and `tokenizers`, but did not manage to find a good pattern.\\r\\n\\r\\nI guess the solution would entail wrapping a dataset into a Pytorch dataset.\\r\\n\\r\\nAs a concrete example from the [docs](https://huggingface.co/transformers/custom_datasets.html)\\r\\n\\r\\n```python\\r\\nimport torch\\r\\n\\r\\nclass SquadDataset(torch.utils.data.Dataset):\\r\\n    def __init__(self, encodings):\\r\\n        # instead of doing this beforehand, I\\'d like to do tokenization on the fly\\r\\n        self.encodings = encodings \\r\\n\\r\\n    def __getitem__(self, idx):\\r\\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.encodings.input_ids)\\r\\n\\r\\ntrain_dataset = SquadDataset(train_encodings)\\r\\n```\\r\\n\\r\\nHow would one implement this with \"on-the-fly\" tokenization exploiting the vectorized capabilities of tokenizers?\\r\\n\\r\\n\\r\\n----\\r\\n\\r\\nEdit: I have come up with this solution. It does what I want, but I feel it\\'s not very elegant\\r\\n\\r\\n```python\\r\\nclass CustomPytorchDataset(Dataset):\\r\\n    def __init__(self):\\r\\n        self.dataset = some_hf_dataset(...)\\r\\n        self.tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\\r\\n\\r\\n    def __getitem__(self, batch_idx):\\r\\n        instance = self.dataset[text_col][batch_idx]\\r\\n        tokenized_text = self.tokenizer(instance, truncation=True, padding=True)\\r\\n        return tokenized_text\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.dataset)\\r\\n\\r\\n    @staticmethod\\r\\n    def collate_fn(batch):\\r\\n        # batch is a list, however it will always contain 1 item because we should not use the\\r\\n        # batch_size argument as batch_size is controlled by the sampler\\r\\n        return {k: torch.tensor(v) for k, v in batch[0].items()}\\r\\n\\r\\ntorch_ds = CustomPytorchDataset()\\r\\n\\r\\n# NOTE: batch_sampler returns list of integers and since here we have SequentialSampler\\r\\n# it returns: [1, 2, 3], [4, 5, 6], etc. - check calling `list(batch_sampler)`\\r\\nbatch_sampler = BatchSampler(SequentialSampler(torch_ds), batch_size=3, drop_last=True)\\r\\n\\r\\n# NOTE: no `batch_size` as now the it is controlled by the sampler!\\r\\ndl = DataLoader(dataset=torch_ds, sampler=batch_sampler, collate_fn=torch_ds.collate_fn)\\r\\n``` \\n We\\'re working on adding on-the-fly transforms in datasets.\\r\\nCurrently the only on-the-fly functions that can be applied are in `set_format` in which we transform the data in either numpy/torch/tf tensors or pandas.\\r\\nFor example\\r\\n```python\\r\\ndataset.set_format(\"torch\")\\r\\n```\\r\\napplies `torch.Tensor` to the dataset entries on-the-fly.\\r\\n\\r\\nWe plan to extend this to user-defined formatting transforms.\\r\\nFor example\\r\\n```python\\r\\ndataset.set_format(transform=tokenize)\\r\\n```\\r\\n\\r\\nWhat do you think ?', 'using map on loaded Tokenizer 10x - 100x slower than default Tokenizer? \\n This could total relate to me misunderstanding particular call functions, but I added words to a GPT2Tokenizer, and saved it to disk (note I\\'m only showing snippets but I can share more) and the map function ran much slower: \\r\\n\\r\\n````\\r\\ndef save_tokenizer(original_tokenizer,text,path=\"simpledata/tokenizer\"):\\r\\n    words_unique = set(text.split(\" \"))\\r\\n    for i in words_unique:\\r\\n        original_tokenizer.add_tokens(i)\\r\\n    original_tokenizer.save_pretrained(path)\\r\\n\\r\\ntokenizer2 = GPT2Tokenizer.from_pretrained(os.path.join(experiment_path,experiment_name,\"tokenizer_squad\"))\\r\\n\\r\\ntrain_set_baby=Dataset.from_dict({\"text\":[train_set[\"text\"][0][0:50]]})\\r\\n````\\r\\n\\r\\nI then applied the dataset map function on a fairly small set of text:\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\n\\r\\n\\r\\nThe run time for train_set_baby.map was 6 seconds, and the batch itself was 2.6 seconds\\r\\n\\r\\n**100% 1/1 [00:02<00:00, 2.60s/ba] CPU times: user 5.96 s, sys: 36 ms, total: 5.99 s Wall time: 5.99 s**\\r\\n\\r\\nIn comparison using (even after adding additional tokens): \\r\\n`\\r\\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")`\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\nThe time is \\r\\n**100% 1/1 [00:00<00:00, 34.09ba/s] CPU times: user 68.1 ms, sys: 16 µs, total: 68.1 ms Wall time: 62.9 ms**\\r\\n\\r\\nIt seems this might relate to the tokenizer save or load function, however, the issue appears to come up when I apply the loaded tokenizer  to the map function. \\r\\n\\r\\nI should also add that playing around with the amount of words I add to the tokenizer before I save it to disk and load it into memory  appears to impact the time it takes to run the map function. \\r\\n\\r\\n\\r\\n \\n Hi @wumpusman \\r\\n`datasets` has a caching mechanism that allows to cache the results of `.map` so that when you want to re-run it later it doesn\\'t recompute it again.\\r\\nSo when you do `.map`, what actually happens is:\\r\\n1. compute the hash used to identify your `map` for the cache\\r\\n2. apply your function on every batch\\r\\n\\r\\nThis can explain the time difference between your different experiments.\\r\\n\\r\\nThe hash computation time depends of how complex your function is. For a tokenizer, the hash computation scans the lists of the words in the tokenizer to identify this tokenizer. Usually it takes 2-3 seconds.\\r\\n\\r\\nAlso note that you can disable caching though using\\r\\n```python\\r\\nimport datasets\\r\\n\\r\\ndatasets.set_caching_enabled(False)\\r\\n```', 'using map on loaded Tokenizer 10x - 100x slower than default Tokenizer? \\n This could total relate to me misunderstanding particular call functions, but I added words to a GPT2Tokenizer, and saved it to disk (note I\\'m only showing snippets but I can share more) and the map function ran much slower: \\r\\n\\r\\n````\\r\\ndef save_tokenizer(original_tokenizer,text,path=\"simpledata/tokenizer\"):\\r\\n    words_unique = set(text.split(\" \"))\\r\\n    for i in words_unique:\\r\\n        original_tokenizer.add_tokens(i)\\r\\n    original_tokenizer.save_pretrained(path)\\r\\n\\r\\ntokenizer2 = GPT2Tokenizer.from_pretrained(os.path.join(experiment_path,experiment_name,\"tokenizer_squad\"))\\r\\n\\r\\ntrain_set_baby=Dataset.from_dict({\"text\":[train_set[\"text\"][0][0:50]]})\\r\\n````\\r\\n\\r\\nI then applied the dataset map function on a fairly small set of text:\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\n\\r\\n\\r\\nThe run time for train_set_baby.map was 6 seconds, and the batch itself was 2.6 seconds\\r\\n\\r\\n**100% 1/1 [00:02<00:00, 2.60s/ba] CPU times: user 5.96 s, sys: 36 ms, total: 5.99 s Wall time: 5.99 s**\\r\\n\\r\\nIn comparison using (even after adding additional tokens): \\r\\n`\\r\\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")`\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\nThe time is \\r\\n**100% 1/1 [00:00<00:00, 34.09ba/s] CPU times: user 68.1 ms, sys: 16 µs, total: 68.1 ms Wall time: 62.9 ms**\\r\\n\\r\\nIt seems this might relate to the tokenizer save or load function, however, the issue appears to come up when I apply the loaded tokenizer  to the map function. \\r\\n\\r\\nI should also add that playing around with the amount of words I add to the tokenizer before I save it to disk and load it into memory  appears to impact the time it takes to run the map function. \\r\\n\\r\\n\\r\\n \\n Hi @lhoestq ,\\r\\n\\r\\nThanks for the reply. It\\'s entirely possible that is the issue. Since it\\'s a side project I won\\'t be looking at it till later this week, but, I\\'ll verify it by disabling caching and hopefully I\\'ll see the same runtime. \\r\\n\\r\\nAppreciate the reference,\\r\\n\\r\\nMichael']\n",
      "URL: ['https://github.com/huggingface/datasets/issues/2532', 'https://github.com/huggingface/datasets/issues/2532', 'https://github.com/huggingface/datasets/issues/1167', 'https://github.com/huggingface/datasets/issues/1830', 'https://github.com/huggingface/datasets/issues/1830']\n",
      "\n",
      "3. Score: 42.69\n",
      "Comment: ['Tokenizer\\'s normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task \\n [This colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) implements a token classification input pipeline extending the logic from [this hugging example](https://huggingface.co/transformers/custom_datasets.html#tok-ner).\\r\\n\\r\\nThe pipeline works fine with most instance in different languages, but unfortunately, [the Japanese Kana ligature (a form of abbreviation? I don\\'t know Japanese well)](https://en.wikipedia.org/wiki/Kana_ligature) break the alignment of `return_offsets_mapping`:\\r\\n![image](https://user-images.githubusercontent.com/50871412/122904371-db192700-d382-11eb-8917-1775db76db69.png)\\r\\n\\r\\nWithout the try catch block, it riase `ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true`, example shown here [(another colab notebook)](https://colab.research.google.com/drive/1MmOqf3ppzzdKKyMWkn0bJy6DqzOO0SSm?usp=sharing)\\r\\n\\r\\nIt is clear that the normalizer is the process that break the alignment, as it is observed that `tokenizer._tokenizer.normalizer.normalize_str(\\'ヿ\\')` return \\'コト\\'.\\r\\n\\r\\nOne workaround is to include `tokenizer._tokenizer.normalizer.normalize_str` before the tokenizer preprocessing pipeline, which is also provided in the [first colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) with the name `udposTestDatasetWorkaround`.\\r\\n\\r\\nI guess similar logics should be included inside the tokenizer and the offsets_mapping generation process such that user don\\'t need to include them in their code. But I don\\'t understand the code of tokenizer well that I think I am not able to do this.\\r\\n\\r\\np.s.\\r\\n**I am using my own dataset building script in the provided example, but the script should be equivalent to the changes made by this [update](https://github.com/huggingface/datasets/pull/2466)**\\r\\n`get_dataset `is just a simple wrapping for `load_dataset`\\r\\nand the `tokenizer` is just `XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")` \\n Hi @jerryIsHere, thanks for reporting the issue. But are you sure this is a bug in HuggingFace **Datasets**?', 'Tokenizer\\'s normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task \\n [This colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) implements a token classification input pipeline extending the logic from [this hugging example](https://huggingface.co/transformers/custom_datasets.html#tok-ner).\\r\\n\\r\\nThe pipeline works fine with most instance in different languages, but unfortunately, [the Japanese Kana ligature (a form of abbreviation? I don\\'t know Japanese well)](https://en.wikipedia.org/wiki/Kana_ligature) break the alignment of `return_offsets_mapping`:\\r\\n![image](https://user-images.githubusercontent.com/50871412/122904371-db192700-d382-11eb-8917-1775db76db69.png)\\r\\n\\r\\nWithout the try catch block, it riase `ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true`, example shown here [(another colab notebook)](https://colab.research.google.com/drive/1MmOqf3ppzzdKKyMWkn0bJy6DqzOO0SSm?usp=sharing)\\r\\n\\r\\nIt is clear that the normalizer is the process that break the alignment, as it is observed that `tokenizer._tokenizer.normalizer.normalize_str(\\'ヿ\\')` return \\'コト\\'.\\r\\n\\r\\nOne workaround is to include `tokenizer._tokenizer.normalizer.normalize_str` before the tokenizer preprocessing pipeline, which is also provided in the [first colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) with the name `udposTestDatasetWorkaround`.\\r\\n\\r\\nI guess similar logics should be included inside the tokenizer and the offsets_mapping generation process such that user don\\'t need to include them in their code. But I don\\'t understand the code of tokenizer well that I think I am not able to do this.\\r\\n\\r\\np.s.\\r\\n**I am using my own dataset building script in the provided example, but the script should be equivalent to the changes made by this [update](https://github.com/huggingface/datasets/pull/2466)**\\r\\n`get_dataset `is just a simple wrapping for `load_dataset`\\r\\nand the `tokenizer` is just `XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")` \\n > Hi @jerryIsHere, thanks for reporting the issue. But are you sure this is a bug in HuggingFace **Datasets**?\\r\\n\\r\\nOh, I am sorry\\r\\nI would reopen the post on huggingface/transformers', '❓ On-the-fly tokenization with datasets, tokenizers, and torch Datasets and Dataloaders \\n Hi there,\\r\\n\\r\\nI have a question regarding \"on-the-fly\" tokenization. This question was elicited by reading the \"How to train a new language model from scratch using Transformers and Tokenizers\" [here](https://huggingface.co/blog/how-to-train). Towards the end there is this sentence: \"If your dataset is very large, you can opt to load and tokenize examples on the fly, rather than as a preprocessing step\". I\\'ve tried coming up with a solution that would combine both `datasets` and `tokenizers`, but did not manage to find a good pattern.\\r\\n\\r\\nI guess the solution would entail wrapping a dataset into a Pytorch dataset.\\r\\n\\r\\nAs a concrete example from the [docs](https://huggingface.co/transformers/custom_datasets.html)\\r\\n\\r\\n```python\\r\\nimport torch\\r\\n\\r\\nclass SquadDataset(torch.utils.data.Dataset):\\r\\n    def __init__(self, encodings):\\r\\n        # instead of doing this beforehand, I\\'d like to do tokenization on the fly\\r\\n        self.encodings = encodings \\r\\n\\r\\n    def __getitem__(self, idx):\\r\\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.encodings.input_ids)\\r\\n\\r\\ntrain_dataset = SquadDataset(train_encodings)\\r\\n```\\r\\n\\r\\nHow would one implement this with \"on-the-fly\" tokenization exploiting the vectorized capabilities of tokenizers?\\r\\n\\r\\n\\r\\n----\\r\\n\\r\\nEdit: I have come up with this solution. It does what I want, but I feel it\\'s not very elegant\\r\\n\\r\\n```python\\r\\nclass CustomPytorchDataset(Dataset):\\r\\n    def __init__(self):\\r\\n        self.dataset = some_hf_dataset(...)\\r\\n        self.tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\\r\\n\\r\\n    def __getitem__(self, batch_idx):\\r\\n        instance = self.dataset[text_col][batch_idx]\\r\\n        tokenized_text = self.tokenizer(instance, truncation=True, padding=True)\\r\\n        return tokenized_text\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.dataset)\\r\\n\\r\\n    @staticmethod\\r\\n    def collate_fn(batch):\\r\\n        # batch is a list, however it will always contain 1 item because we should not use the\\r\\n        # batch_size argument as batch_size is controlled by the sampler\\r\\n        return {k: torch.tensor(v) for k, v in batch[0].items()}\\r\\n\\r\\ntorch_ds = CustomPytorchDataset()\\r\\n\\r\\n# NOTE: batch_sampler returns list of integers and since here we have SequentialSampler\\r\\n# it returns: [1, 2, 3], [4, 5, 6], etc. - check calling `list(batch_sampler)`\\r\\nbatch_sampler = BatchSampler(SequentialSampler(torch_ds), batch_size=3, drop_last=True)\\r\\n\\r\\n# NOTE: no `batch_size` as now the it is controlled by the sampler!\\r\\ndl = DataLoader(dataset=torch_ds, sampler=batch_sampler, collate_fn=torch_ds.collate_fn)\\r\\n``` \\n We\\'re working on adding on-the-fly transforms in datasets.\\r\\nCurrently the only on-the-fly functions that can be applied are in `set_format` in which we transform the data in either numpy/torch/tf tensors or pandas.\\r\\nFor example\\r\\n```python\\r\\ndataset.set_format(\"torch\")\\r\\n```\\r\\napplies `torch.Tensor` to the dataset entries on-the-fly.\\r\\n\\r\\nWe plan to extend this to user-defined formatting transforms.\\r\\nFor example\\r\\n```python\\r\\ndataset.set_format(transform=tokenize)\\r\\n```\\r\\n\\r\\nWhat do you think ?', 'using map on loaded Tokenizer 10x - 100x slower than default Tokenizer? \\n This could total relate to me misunderstanding particular call functions, but I added words to a GPT2Tokenizer, and saved it to disk (note I\\'m only showing snippets but I can share more) and the map function ran much slower: \\r\\n\\r\\n````\\r\\ndef save_tokenizer(original_tokenizer,text,path=\"simpledata/tokenizer\"):\\r\\n    words_unique = set(text.split(\" \"))\\r\\n    for i in words_unique:\\r\\n        original_tokenizer.add_tokens(i)\\r\\n    original_tokenizer.save_pretrained(path)\\r\\n\\r\\ntokenizer2 = GPT2Tokenizer.from_pretrained(os.path.join(experiment_path,experiment_name,\"tokenizer_squad\"))\\r\\n\\r\\ntrain_set_baby=Dataset.from_dict({\"text\":[train_set[\"text\"][0][0:50]]})\\r\\n````\\r\\n\\r\\nI then applied the dataset map function on a fairly small set of text:\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\n\\r\\n\\r\\nThe run time for train_set_baby.map was 6 seconds, and the batch itself was 2.6 seconds\\r\\n\\r\\n**100% 1/1 [00:02<00:00, 2.60s/ba] CPU times: user 5.96 s, sys: 36 ms, total: 5.99 s Wall time: 5.99 s**\\r\\n\\r\\nIn comparison using (even after adding additional tokens): \\r\\n`\\r\\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")`\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\nThe time is \\r\\n**100% 1/1 [00:00<00:00, 34.09ba/s] CPU times: user 68.1 ms, sys: 16 µs, total: 68.1 ms Wall time: 62.9 ms**\\r\\n\\r\\nIt seems this might relate to the tokenizer save or load function, however, the issue appears to come up when I apply the loaded tokenizer  to the map function. \\r\\n\\r\\nI should also add that playing around with the amount of words I add to the tokenizer before I save it to disk and load it into memory  appears to impact the time it takes to run the map function. \\r\\n\\r\\n\\r\\n \\n Hi @wumpusman \\r\\n`datasets` has a caching mechanism that allows to cache the results of `.map` so that when you want to re-run it later it doesn\\'t recompute it again.\\r\\nSo when you do `.map`, what actually happens is:\\r\\n1. compute the hash used to identify your `map` for the cache\\r\\n2. apply your function on every batch\\r\\n\\r\\nThis can explain the time difference between your different experiments.\\r\\n\\r\\nThe hash computation time depends of how complex your function is. For a tokenizer, the hash computation scans the lists of the words in the tokenizer to identify this tokenizer. Usually it takes 2-3 seconds.\\r\\n\\r\\nAlso note that you can disable caching though using\\r\\n```python\\r\\nimport datasets\\r\\n\\r\\ndatasets.set_caching_enabled(False)\\r\\n```', 'using map on loaded Tokenizer 10x - 100x slower than default Tokenizer? \\n This could total relate to me misunderstanding particular call functions, but I added words to a GPT2Tokenizer, and saved it to disk (note I\\'m only showing snippets but I can share more) and the map function ran much slower: \\r\\n\\r\\n````\\r\\ndef save_tokenizer(original_tokenizer,text,path=\"simpledata/tokenizer\"):\\r\\n    words_unique = set(text.split(\" \"))\\r\\n    for i in words_unique:\\r\\n        original_tokenizer.add_tokens(i)\\r\\n    original_tokenizer.save_pretrained(path)\\r\\n\\r\\ntokenizer2 = GPT2Tokenizer.from_pretrained(os.path.join(experiment_path,experiment_name,\"tokenizer_squad\"))\\r\\n\\r\\ntrain_set_baby=Dataset.from_dict({\"text\":[train_set[\"text\"][0][0:50]]})\\r\\n````\\r\\n\\r\\nI then applied the dataset map function on a fairly small set of text:\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\n\\r\\n\\r\\nThe run time for train_set_baby.map was 6 seconds, and the batch itself was 2.6 seconds\\r\\n\\r\\n**100% 1/1 [00:02<00:00, 2.60s/ba] CPU times: user 5.96 s, sys: 36 ms, total: 5.99 s Wall time: 5.99 s**\\r\\n\\r\\nIn comparison using (even after adding additional tokens): \\r\\n`\\r\\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")`\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\nThe time is \\r\\n**100% 1/1 [00:00<00:00, 34.09ba/s] CPU times: user 68.1 ms, sys: 16 µs, total: 68.1 ms Wall time: 62.9 ms**\\r\\n\\r\\nIt seems this might relate to the tokenizer save or load function, however, the issue appears to come up when I apply the loaded tokenizer  to the map function. \\r\\n\\r\\nI should also add that playing around with the amount of words I add to the tokenizer before I save it to disk and load it into memory  appears to impact the time it takes to run the map function. \\r\\n\\r\\n\\r\\n \\n Hi @lhoestq ,\\r\\n\\r\\nThanks for the reply. It\\'s entirely possible that is the issue. Since it\\'s a side project I won\\'t be looking at it till later this week, but, I\\'ll verify it by disabling caching and hopefully I\\'ll see the same runtime. \\r\\n\\r\\nAppreciate the reference,\\r\\n\\r\\nMichael']\n",
      "URL: ['https://github.com/huggingface/datasets/issues/2532', 'https://github.com/huggingface/datasets/issues/2532', 'https://github.com/huggingface/datasets/issues/1167', 'https://github.com/huggingface/datasets/issues/1830', 'https://github.com/huggingface/datasets/issues/1830']\n",
      "\n",
      "4. Score: 43.96\n",
      "Comment: ['Tokenizer\\'s normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task \\n [This colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) implements a token classification input pipeline extending the logic from [this hugging example](https://huggingface.co/transformers/custom_datasets.html#tok-ner).\\r\\n\\r\\nThe pipeline works fine with most instance in different languages, but unfortunately, [the Japanese Kana ligature (a form of abbreviation? I don\\'t know Japanese well)](https://en.wikipedia.org/wiki/Kana_ligature) break the alignment of `return_offsets_mapping`:\\r\\n![image](https://user-images.githubusercontent.com/50871412/122904371-db192700-d382-11eb-8917-1775db76db69.png)\\r\\n\\r\\nWithout the try catch block, it riase `ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true`, example shown here [(another colab notebook)](https://colab.research.google.com/drive/1MmOqf3ppzzdKKyMWkn0bJy6DqzOO0SSm?usp=sharing)\\r\\n\\r\\nIt is clear that the normalizer is the process that break the alignment, as it is observed that `tokenizer._tokenizer.normalizer.normalize_str(\\'ヿ\\')` return \\'コト\\'.\\r\\n\\r\\nOne workaround is to include `tokenizer._tokenizer.normalizer.normalize_str` before the tokenizer preprocessing pipeline, which is also provided in the [first colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) with the name `udposTestDatasetWorkaround`.\\r\\n\\r\\nI guess similar logics should be included inside the tokenizer and the offsets_mapping generation process such that user don\\'t need to include them in their code. But I don\\'t understand the code of tokenizer well that I think I am not able to do this.\\r\\n\\r\\np.s.\\r\\n**I am using my own dataset building script in the provided example, but the script should be equivalent to the changes made by this [update](https://github.com/huggingface/datasets/pull/2466)**\\r\\n`get_dataset `is just a simple wrapping for `load_dataset`\\r\\nand the `tokenizer` is just `XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")` \\n Hi @jerryIsHere, thanks for reporting the issue. But are you sure this is a bug in HuggingFace **Datasets**?', 'Tokenizer\\'s normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task \\n [This colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) implements a token classification input pipeline extending the logic from [this hugging example](https://huggingface.co/transformers/custom_datasets.html#tok-ner).\\r\\n\\r\\nThe pipeline works fine with most instance in different languages, but unfortunately, [the Japanese Kana ligature (a form of abbreviation? I don\\'t know Japanese well)](https://en.wikipedia.org/wiki/Kana_ligature) break the alignment of `return_offsets_mapping`:\\r\\n![image](https://user-images.githubusercontent.com/50871412/122904371-db192700-d382-11eb-8917-1775db76db69.png)\\r\\n\\r\\nWithout the try catch block, it riase `ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true`, example shown here [(another colab notebook)](https://colab.research.google.com/drive/1MmOqf3ppzzdKKyMWkn0bJy6DqzOO0SSm?usp=sharing)\\r\\n\\r\\nIt is clear that the normalizer is the process that break the alignment, as it is observed that `tokenizer._tokenizer.normalizer.normalize_str(\\'ヿ\\')` return \\'コト\\'.\\r\\n\\r\\nOne workaround is to include `tokenizer._tokenizer.normalizer.normalize_str` before the tokenizer preprocessing pipeline, which is also provided in the [first colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) with the name `udposTestDatasetWorkaround`.\\r\\n\\r\\nI guess similar logics should be included inside the tokenizer and the offsets_mapping generation process such that user don\\'t need to include them in their code. But I don\\'t understand the code of tokenizer well that I think I am not able to do this.\\r\\n\\r\\np.s.\\r\\n**I am using my own dataset building script in the provided example, but the script should be equivalent to the changes made by this [update](https://github.com/huggingface/datasets/pull/2466)**\\r\\n`get_dataset `is just a simple wrapping for `load_dataset`\\r\\nand the `tokenizer` is just `XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")` \\n > Hi @jerryIsHere, thanks for reporting the issue. But are you sure this is a bug in HuggingFace **Datasets**?\\r\\n\\r\\nOh, I am sorry\\r\\nI would reopen the post on huggingface/transformers', '❓ On-the-fly tokenization with datasets, tokenizers, and torch Datasets and Dataloaders \\n Hi there,\\r\\n\\r\\nI have a question regarding \"on-the-fly\" tokenization. This question was elicited by reading the \"How to train a new language model from scratch using Transformers and Tokenizers\" [here](https://huggingface.co/blog/how-to-train). Towards the end there is this sentence: \"If your dataset is very large, you can opt to load and tokenize examples on the fly, rather than as a preprocessing step\". I\\'ve tried coming up with a solution that would combine both `datasets` and `tokenizers`, but did not manage to find a good pattern.\\r\\n\\r\\nI guess the solution would entail wrapping a dataset into a Pytorch dataset.\\r\\n\\r\\nAs a concrete example from the [docs](https://huggingface.co/transformers/custom_datasets.html)\\r\\n\\r\\n```python\\r\\nimport torch\\r\\n\\r\\nclass SquadDataset(torch.utils.data.Dataset):\\r\\n    def __init__(self, encodings):\\r\\n        # instead of doing this beforehand, I\\'d like to do tokenization on the fly\\r\\n        self.encodings = encodings \\r\\n\\r\\n    def __getitem__(self, idx):\\r\\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.encodings.input_ids)\\r\\n\\r\\ntrain_dataset = SquadDataset(train_encodings)\\r\\n```\\r\\n\\r\\nHow would one implement this with \"on-the-fly\" tokenization exploiting the vectorized capabilities of tokenizers?\\r\\n\\r\\n\\r\\n----\\r\\n\\r\\nEdit: I have come up with this solution. It does what I want, but I feel it\\'s not very elegant\\r\\n\\r\\n```python\\r\\nclass CustomPytorchDataset(Dataset):\\r\\n    def __init__(self):\\r\\n        self.dataset = some_hf_dataset(...)\\r\\n        self.tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\\r\\n\\r\\n    def __getitem__(self, batch_idx):\\r\\n        instance = self.dataset[text_col][batch_idx]\\r\\n        tokenized_text = self.tokenizer(instance, truncation=True, padding=True)\\r\\n        return tokenized_text\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.dataset)\\r\\n\\r\\n    @staticmethod\\r\\n    def collate_fn(batch):\\r\\n        # batch is a list, however it will always contain 1 item because we should not use the\\r\\n        # batch_size argument as batch_size is controlled by the sampler\\r\\n        return {k: torch.tensor(v) for k, v in batch[0].items()}\\r\\n\\r\\ntorch_ds = CustomPytorchDataset()\\r\\n\\r\\n# NOTE: batch_sampler returns list of integers and since here we have SequentialSampler\\r\\n# it returns: [1, 2, 3], [4, 5, 6], etc. - check calling `list(batch_sampler)`\\r\\nbatch_sampler = BatchSampler(SequentialSampler(torch_ds), batch_size=3, drop_last=True)\\r\\n\\r\\n# NOTE: no `batch_size` as now the it is controlled by the sampler!\\r\\ndl = DataLoader(dataset=torch_ds, sampler=batch_sampler, collate_fn=torch_ds.collate_fn)\\r\\n``` \\n We\\'re working on adding on-the-fly transforms in datasets.\\r\\nCurrently the only on-the-fly functions that can be applied are in `set_format` in which we transform the data in either numpy/torch/tf tensors or pandas.\\r\\nFor example\\r\\n```python\\r\\ndataset.set_format(\"torch\")\\r\\n```\\r\\napplies `torch.Tensor` to the dataset entries on-the-fly.\\r\\n\\r\\nWe plan to extend this to user-defined formatting transforms.\\r\\nFor example\\r\\n```python\\r\\ndataset.set_format(transform=tokenize)\\r\\n```\\r\\n\\r\\nWhat do you think ?', 'using map on loaded Tokenizer 10x - 100x slower than default Tokenizer? \\n This could total relate to me misunderstanding particular call functions, but I added words to a GPT2Tokenizer, and saved it to disk (note I\\'m only showing snippets but I can share more) and the map function ran much slower: \\r\\n\\r\\n````\\r\\ndef save_tokenizer(original_tokenizer,text,path=\"simpledata/tokenizer\"):\\r\\n    words_unique = set(text.split(\" \"))\\r\\n    for i in words_unique:\\r\\n        original_tokenizer.add_tokens(i)\\r\\n    original_tokenizer.save_pretrained(path)\\r\\n\\r\\ntokenizer2 = GPT2Tokenizer.from_pretrained(os.path.join(experiment_path,experiment_name,\"tokenizer_squad\"))\\r\\n\\r\\ntrain_set_baby=Dataset.from_dict({\"text\":[train_set[\"text\"][0][0:50]]})\\r\\n````\\r\\n\\r\\nI then applied the dataset map function on a fairly small set of text:\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\n\\r\\n\\r\\nThe run time for train_set_baby.map was 6 seconds, and the batch itself was 2.6 seconds\\r\\n\\r\\n**100% 1/1 [00:02<00:00, 2.60s/ba] CPU times: user 5.96 s, sys: 36 ms, total: 5.99 s Wall time: 5.99 s**\\r\\n\\r\\nIn comparison using (even after adding additional tokens): \\r\\n`\\r\\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")`\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\nThe time is \\r\\n**100% 1/1 [00:00<00:00, 34.09ba/s] CPU times: user 68.1 ms, sys: 16 µs, total: 68.1 ms Wall time: 62.9 ms**\\r\\n\\r\\nIt seems this might relate to the tokenizer save or load function, however, the issue appears to come up when I apply the loaded tokenizer  to the map function. \\r\\n\\r\\nI should also add that playing around with the amount of words I add to the tokenizer before I save it to disk and load it into memory  appears to impact the time it takes to run the map function. \\r\\n\\r\\n\\r\\n \\n Hi @wumpusman \\r\\n`datasets` has a caching mechanism that allows to cache the results of `.map` so that when you want to re-run it later it doesn\\'t recompute it again.\\r\\nSo when you do `.map`, what actually happens is:\\r\\n1. compute the hash used to identify your `map` for the cache\\r\\n2. apply your function on every batch\\r\\n\\r\\nThis can explain the time difference between your different experiments.\\r\\n\\r\\nThe hash computation time depends of how complex your function is. For a tokenizer, the hash computation scans the lists of the words in the tokenizer to identify this tokenizer. Usually it takes 2-3 seconds.\\r\\n\\r\\nAlso note that you can disable caching though using\\r\\n```python\\r\\nimport datasets\\r\\n\\r\\ndatasets.set_caching_enabled(False)\\r\\n```', 'using map on loaded Tokenizer 10x - 100x slower than default Tokenizer? \\n This could total relate to me misunderstanding particular call functions, but I added words to a GPT2Tokenizer, and saved it to disk (note I\\'m only showing snippets but I can share more) and the map function ran much slower: \\r\\n\\r\\n````\\r\\ndef save_tokenizer(original_tokenizer,text,path=\"simpledata/tokenizer\"):\\r\\n    words_unique = set(text.split(\" \"))\\r\\n    for i in words_unique:\\r\\n        original_tokenizer.add_tokens(i)\\r\\n    original_tokenizer.save_pretrained(path)\\r\\n\\r\\ntokenizer2 = GPT2Tokenizer.from_pretrained(os.path.join(experiment_path,experiment_name,\"tokenizer_squad\"))\\r\\n\\r\\ntrain_set_baby=Dataset.from_dict({\"text\":[train_set[\"text\"][0][0:50]]})\\r\\n````\\r\\n\\r\\nI then applied the dataset map function on a fairly small set of text:\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\n\\r\\n\\r\\nThe run time for train_set_baby.map was 6 seconds, and the batch itself was 2.6 seconds\\r\\n\\r\\n**100% 1/1 [00:02<00:00, 2.60s/ba] CPU times: user 5.96 s, sys: 36 ms, total: 5.99 s Wall time: 5.99 s**\\r\\n\\r\\nIn comparison using (even after adding additional tokens): \\r\\n`\\r\\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")`\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\nThe time is \\r\\n**100% 1/1 [00:00<00:00, 34.09ba/s] CPU times: user 68.1 ms, sys: 16 µs, total: 68.1 ms Wall time: 62.9 ms**\\r\\n\\r\\nIt seems this might relate to the tokenizer save or load function, however, the issue appears to come up when I apply the loaded tokenizer  to the map function. \\r\\n\\r\\nI should also add that playing around with the amount of words I add to the tokenizer before I save it to disk and load it into memory  appears to impact the time it takes to run the map function. \\r\\n\\r\\n\\r\\n \\n Hi @lhoestq ,\\r\\n\\r\\nThanks for the reply. It\\'s entirely possible that is the issue. Since it\\'s a side project I won\\'t be looking at it till later this week, but, I\\'ll verify it by disabling caching and hopefully I\\'ll see the same runtime. \\r\\n\\r\\nAppreciate the reference,\\r\\n\\r\\nMichael']\n",
      "URL: ['https://github.com/huggingface/datasets/issues/2532', 'https://github.com/huggingface/datasets/issues/2532', 'https://github.com/huggingface/datasets/issues/1167', 'https://github.com/huggingface/datasets/issues/1830', 'https://github.com/huggingface/datasets/issues/1830']\n",
      "\n",
      "5. Score: 43.96\n",
      "Comment: ['Tokenizer\\'s normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task \\n [This colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) implements a token classification input pipeline extending the logic from [this hugging example](https://huggingface.co/transformers/custom_datasets.html#tok-ner).\\r\\n\\r\\nThe pipeline works fine with most instance in different languages, but unfortunately, [the Japanese Kana ligature (a form of abbreviation? I don\\'t know Japanese well)](https://en.wikipedia.org/wiki/Kana_ligature) break the alignment of `return_offsets_mapping`:\\r\\n![image](https://user-images.githubusercontent.com/50871412/122904371-db192700-d382-11eb-8917-1775db76db69.png)\\r\\n\\r\\nWithout the try catch block, it riase `ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true`, example shown here [(another colab notebook)](https://colab.research.google.com/drive/1MmOqf3ppzzdKKyMWkn0bJy6DqzOO0SSm?usp=sharing)\\r\\n\\r\\nIt is clear that the normalizer is the process that break the alignment, as it is observed that `tokenizer._tokenizer.normalizer.normalize_str(\\'ヿ\\')` return \\'コト\\'.\\r\\n\\r\\nOne workaround is to include `tokenizer._tokenizer.normalizer.normalize_str` before the tokenizer preprocessing pipeline, which is also provided in the [first colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) with the name `udposTestDatasetWorkaround`.\\r\\n\\r\\nI guess similar logics should be included inside the tokenizer and the offsets_mapping generation process such that user don\\'t need to include them in their code. But I don\\'t understand the code of tokenizer well that I think I am not able to do this.\\r\\n\\r\\np.s.\\r\\n**I am using my own dataset building script in the provided example, but the script should be equivalent to the changes made by this [update](https://github.com/huggingface/datasets/pull/2466)**\\r\\n`get_dataset `is just a simple wrapping for `load_dataset`\\r\\nand the `tokenizer` is just `XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")` \\n Hi @jerryIsHere, thanks for reporting the issue. But are you sure this is a bug in HuggingFace **Datasets**?', 'Tokenizer\\'s normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task \\n [This colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) implements a token classification input pipeline extending the logic from [this hugging example](https://huggingface.co/transformers/custom_datasets.html#tok-ner).\\r\\n\\r\\nThe pipeline works fine with most instance in different languages, but unfortunately, [the Japanese Kana ligature (a form of abbreviation? I don\\'t know Japanese well)](https://en.wikipedia.org/wiki/Kana_ligature) break the alignment of `return_offsets_mapping`:\\r\\n![image](https://user-images.githubusercontent.com/50871412/122904371-db192700-d382-11eb-8917-1775db76db69.png)\\r\\n\\r\\nWithout the try catch block, it riase `ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true`, example shown here [(another colab notebook)](https://colab.research.google.com/drive/1MmOqf3ppzzdKKyMWkn0bJy6DqzOO0SSm?usp=sharing)\\r\\n\\r\\nIt is clear that the normalizer is the process that break the alignment, as it is observed that `tokenizer._tokenizer.normalizer.normalize_str(\\'ヿ\\')` return \\'コト\\'.\\r\\n\\r\\nOne workaround is to include `tokenizer._tokenizer.normalizer.normalize_str` before the tokenizer preprocessing pipeline, which is also provided in the [first colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) with the name `udposTestDatasetWorkaround`.\\r\\n\\r\\nI guess similar logics should be included inside the tokenizer and the offsets_mapping generation process such that user don\\'t need to include them in their code. But I don\\'t understand the code of tokenizer well that I think I am not able to do this.\\r\\n\\r\\np.s.\\r\\n**I am using my own dataset building script in the provided example, but the script should be equivalent to the changes made by this [update](https://github.com/huggingface/datasets/pull/2466)**\\r\\n`get_dataset `is just a simple wrapping for `load_dataset`\\r\\nand the `tokenizer` is just `XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")` \\n > Hi @jerryIsHere, thanks for reporting the issue. But are you sure this is a bug in HuggingFace **Datasets**?\\r\\n\\r\\nOh, I am sorry\\r\\nI would reopen the post on huggingface/transformers', '❓ On-the-fly tokenization with datasets, tokenizers, and torch Datasets and Dataloaders \\n Hi there,\\r\\n\\r\\nI have a question regarding \"on-the-fly\" tokenization. This question was elicited by reading the \"How to train a new language model from scratch using Transformers and Tokenizers\" [here](https://huggingface.co/blog/how-to-train). Towards the end there is this sentence: \"If your dataset is very large, you can opt to load and tokenize examples on the fly, rather than as a preprocessing step\". I\\'ve tried coming up with a solution that would combine both `datasets` and `tokenizers`, but did not manage to find a good pattern.\\r\\n\\r\\nI guess the solution would entail wrapping a dataset into a Pytorch dataset.\\r\\n\\r\\nAs a concrete example from the [docs](https://huggingface.co/transformers/custom_datasets.html)\\r\\n\\r\\n```python\\r\\nimport torch\\r\\n\\r\\nclass SquadDataset(torch.utils.data.Dataset):\\r\\n    def __init__(self, encodings):\\r\\n        # instead of doing this beforehand, I\\'d like to do tokenization on the fly\\r\\n        self.encodings = encodings \\r\\n\\r\\n    def __getitem__(self, idx):\\r\\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.encodings.input_ids)\\r\\n\\r\\ntrain_dataset = SquadDataset(train_encodings)\\r\\n```\\r\\n\\r\\nHow would one implement this with \"on-the-fly\" tokenization exploiting the vectorized capabilities of tokenizers?\\r\\n\\r\\n\\r\\n----\\r\\n\\r\\nEdit: I have come up with this solution. It does what I want, but I feel it\\'s not very elegant\\r\\n\\r\\n```python\\r\\nclass CustomPytorchDataset(Dataset):\\r\\n    def __init__(self):\\r\\n        self.dataset = some_hf_dataset(...)\\r\\n        self.tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\\r\\n\\r\\n    def __getitem__(self, batch_idx):\\r\\n        instance = self.dataset[text_col][batch_idx]\\r\\n        tokenized_text = self.tokenizer(instance, truncation=True, padding=True)\\r\\n        return tokenized_text\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.dataset)\\r\\n\\r\\n    @staticmethod\\r\\n    def collate_fn(batch):\\r\\n        # batch is a list, however it will always contain 1 item because we should not use the\\r\\n        # batch_size argument as batch_size is controlled by the sampler\\r\\n        return {k: torch.tensor(v) for k, v in batch[0].items()}\\r\\n\\r\\ntorch_ds = CustomPytorchDataset()\\r\\n\\r\\n# NOTE: batch_sampler returns list of integers and since here we have SequentialSampler\\r\\n# it returns: [1, 2, 3], [4, 5, 6], etc. - check calling `list(batch_sampler)`\\r\\nbatch_sampler = BatchSampler(SequentialSampler(torch_ds), batch_size=3, drop_last=True)\\r\\n\\r\\n# NOTE: no `batch_size` as now the it is controlled by the sampler!\\r\\ndl = DataLoader(dataset=torch_ds, sampler=batch_sampler, collate_fn=torch_ds.collate_fn)\\r\\n``` \\n We\\'re working on adding on-the-fly transforms in datasets.\\r\\nCurrently the only on-the-fly functions that can be applied are in `set_format` in which we transform the data in either numpy/torch/tf tensors or pandas.\\r\\nFor example\\r\\n```python\\r\\ndataset.set_format(\"torch\")\\r\\n```\\r\\napplies `torch.Tensor` to the dataset entries on-the-fly.\\r\\n\\r\\nWe plan to extend this to user-defined formatting transforms.\\r\\nFor example\\r\\n```python\\r\\ndataset.set_format(transform=tokenize)\\r\\n```\\r\\n\\r\\nWhat do you think ?', 'using map on loaded Tokenizer 10x - 100x slower than default Tokenizer? \\n This could total relate to me misunderstanding particular call functions, but I added words to a GPT2Tokenizer, and saved it to disk (note I\\'m only showing snippets but I can share more) and the map function ran much slower: \\r\\n\\r\\n````\\r\\ndef save_tokenizer(original_tokenizer,text,path=\"simpledata/tokenizer\"):\\r\\n    words_unique = set(text.split(\" \"))\\r\\n    for i in words_unique:\\r\\n        original_tokenizer.add_tokens(i)\\r\\n    original_tokenizer.save_pretrained(path)\\r\\n\\r\\ntokenizer2 = GPT2Tokenizer.from_pretrained(os.path.join(experiment_path,experiment_name,\"tokenizer_squad\"))\\r\\n\\r\\ntrain_set_baby=Dataset.from_dict({\"text\":[train_set[\"text\"][0][0:50]]})\\r\\n````\\r\\n\\r\\nI then applied the dataset map function on a fairly small set of text:\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\n\\r\\n\\r\\nThe run time for train_set_baby.map was 6 seconds, and the batch itself was 2.6 seconds\\r\\n\\r\\n**100% 1/1 [00:02<00:00, 2.60s/ba] CPU times: user 5.96 s, sys: 36 ms, total: 5.99 s Wall time: 5.99 s**\\r\\n\\r\\nIn comparison using (even after adding additional tokens): \\r\\n`\\r\\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")`\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\nThe time is \\r\\n**100% 1/1 [00:00<00:00, 34.09ba/s] CPU times: user 68.1 ms, sys: 16 µs, total: 68.1 ms Wall time: 62.9 ms**\\r\\n\\r\\nIt seems this might relate to the tokenizer save or load function, however, the issue appears to come up when I apply the loaded tokenizer  to the map function. \\r\\n\\r\\nI should also add that playing around with the amount of words I add to the tokenizer before I save it to disk and load it into memory  appears to impact the time it takes to run the map function. \\r\\n\\r\\n\\r\\n \\n Hi @wumpusman \\r\\n`datasets` has a caching mechanism that allows to cache the results of `.map` so that when you want to re-run it later it doesn\\'t recompute it again.\\r\\nSo when you do `.map`, what actually happens is:\\r\\n1. compute the hash used to identify your `map` for the cache\\r\\n2. apply your function on every batch\\r\\n\\r\\nThis can explain the time difference between your different experiments.\\r\\n\\r\\nThe hash computation time depends of how complex your function is. For a tokenizer, the hash computation scans the lists of the words in the tokenizer to identify this tokenizer. Usually it takes 2-3 seconds.\\r\\n\\r\\nAlso note that you can disable caching though using\\r\\n```python\\r\\nimport datasets\\r\\n\\r\\ndatasets.set_caching_enabled(False)\\r\\n```', 'using map on loaded Tokenizer 10x - 100x slower than default Tokenizer? \\n This could total relate to me misunderstanding particular call functions, but I added words to a GPT2Tokenizer, and saved it to disk (note I\\'m only showing snippets but I can share more) and the map function ran much slower: \\r\\n\\r\\n````\\r\\ndef save_tokenizer(original_tokenizer,text,path=\"simpledata/tokenizer\"):\\r\\n    words_unique = set(text.split(\" \"))\\r\\n    for i in words_unique:\\r\\n        original_tokenizer.add_tokens(i)\\r\\n    original_tokenizer.save_pretrained(path)\\r\\n\\r\\ntokenizer2 = GPT2Tokenizer.from_pretrained(os.path.join(experiment_path,experiment_name,\"tokenizer_squad\"))\\r\\n\\r\\ntrain_set_baby=Dataset.from_dict({\"text\":[train_set[\"text\"][0][0:50]]})\\r\\n````\\r\\n\\r\\nI then applied the dataset map function on a fairly small set of text:\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\n\\r\\n\\r\\nThe run time for train_set_baby.map was 6 seconds, and the batch itself was 2.6 seconds\\r\\n\\r\\n**100% 1/1 [00:02<00:00, 2.60s/ba] CPU times: user 5.96 s, sys: 36 ms, total: 5.99 s Wall time: 5.99 s**\\r\\n\\r\\nIn comparison using (even after adding additional tokens): \\r\\n`\\r\\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")`\\r\\n\\r\\n```\\r\\n%%time\\r\\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\\r\\n\\r\\n```\\r\\nThe time is \\r\\n**100% 1/1 [00:00<00:00, 34.09ba/s] CPU times: user 68.1 ms, sys: 16 µs, total: 68.1 ms Wall time: 62.9 ms**\\r\\n\\r\\nIt seems this might relate to the tokenizer save or load function, however, the issue appears to come up when I apply the loaded tokenizer  to the map function. \\r\\n\\r\\nI should also add that playing around with the amount of words I add to the tokenizer before I save it to disk and load it into memory  appears to impact the time it takes to run the map function. \\r\\n\\r\\n\\r\\n \\n Hi @lhoestq ,\\r\\n\\r\\nThanks for the reply. It\\'s entirely possible that is the issue. Since it\\'s a side project I won\\'t be looking at it till later this week, but, I\\'ll verify it by disabling caching and hopefully I\\'ll see the same runtime. \\r\\n\\r\\nAppreciate the reference,\\r\\n\\r\\nMichael']\n",
      "URL: ['https://github.com/huggingface/datasets/issues/2532', 'https://github.com/huggingface/datasets/issues/2532', 'https://github.com/huggingface/datasets/issues/1167', 'https://github.com/huggingface/datasets/issues/1830', 'https://github.com/huggingface/datasets/issues/1830']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the results\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"{i+1}. Score: {score:.2f}\")\n",
    "    print(f\"Comment: {retrieved_examples['text']}\")\n",
    "    print(f\"URL: {retrieved_examples['html_url']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's collect the results in a dataframe\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame.from_dict(retrieved_examples)\n",
    "results_df[\"scores\"] = scores\n",
    "results_df.sort_values(by=\"scores\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment: Hi! :)\n",
      "\n",
      "I believe that should work unless `dataset_infos.json` isn't actually a dataset. For Hugging Face datasets, there is usually a file named `dataset_infos.json` which contains metadata about the dataset (eg. the dataset citation, license, description, etc). Can you double-check that `dataset_infos.json` isn't just metadata please?\n",
      "score: 35.43\n",
      "Title: load_dataset can't load local dataset,Unable to find ...\n",
      "URL: https://github.com/huggingface/datasets/issues/4192\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Comment: Hi @ahf876828330, \n",
      "\n",
      "As @stevhliu pointed out, the proper way to load a dataset is not trying to load its metadata file.\n",
      "\n",
      "In your case, as the dataset script is local, you should better point to your local loading script:\n",
      "```python\n",
      "dataset = load_dataset(\"dataset/opus_books.py\")\n",
      "```\n",
      "\n",
      "Please, feel free to re-open this issue if the previous code snippet does not work for you.\n",
      "score: 35.43\n",
      "Title: load_dataset can't load local dataset,Unable to find ...\n",
      "URL: https://github.com/huggingface/datasets/issues/4192\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Comment: Hi @SaulLu, thanks for reporting.\n",
      "\n",
      "I think offline mode is not supported for datasets containing only data files (without any loading script). I'm having a look into this...\n",
      "score: 34.86\n",
      "Title: Issue with offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/4760\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Comment: Thanks for your feedback! \n",
      "\n",
      "To give you a little more info, if you don't set the offline mode flag, the script will load the cache. I first noticed this behavior with the `evaluate` library, and while trying to understand the downloading flow I realized that I had a similar error with datasets.\n",
      "score: 34.86\n",
      "Title: Issue with offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/4760\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Comment: #5331 will be helpful to fix this, as it updates the cache directory template to be aligned with the other datasets\n",
      "score: 34.86\n",
      "Title: Issue with offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/4760\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the results\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"Comment: {row.comments}\")\n",
    "    print(f\"score: {row.scores:.2f}\")\n",
    "    print(f\"Title: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"+\" * 50)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
