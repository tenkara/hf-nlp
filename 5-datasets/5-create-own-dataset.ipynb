{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Github issues corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\raj\\repos\\hf-nlp\\.venv\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\raj\\repos\\hf-nlp\\.venv\\lib\\site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\raj\\repos\\hf-nlp\\.venv\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\raj\\repos\\hf-nlp\\.venv\\lib\\site-packages (from requests) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raj\\repos\\hf-nlp\\.venv\\lib\\site-packages (from requests) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "# install requests package if not installed\n",
    "! pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests package\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the github issues from dataset repository\n",
    "# first let's get the first issue on first page\n",
    "url = 'https://api.github.com/repos/huggingface/datasets/issues/1'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://api.github.com/repos/huggingface/datasets/issues/1',\n",
       " 'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       " 'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/1/labels{/name}',\n",
       " 'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/1/comments',\n",
       " 'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/1/events',\n",
       " 'html_url': 'https://github.com/huggingface/datasets/pull/1',\n",
       " 'id': 599457467,\n",
       " 'node_id': 'MDExOlB1bGxSZXF1ZXN0NDAzMDk1NDYw',\n",
       " 'number': 1,\n",
       " 'title': 'changing nlp.bool to nlp.bool_',\n",
       " 'user': {'login': 'mariamabarham',\n",
       "  'id': 38249783,\n",
       "  'node_id': 'MDQ6VXNlcjM4MjQ5Nzgz',\n",
       "  'avatar_url': 'https://avatars.githubusercontent.com/u/38249783?v=4',\n",
       "  'gravatar_id': '',\n",
       "  'url': 'https://api.github.com/users/mariamabarham',\n",
       "  'html_url': 'https://github.com/mariamabarham',\n",
       "  'followers_url': 'https://api.github.com/users/mariamabarham/followers',\n",
       "  'following_url': 'https://api.github.com/users/mariamabarham/following{/other_user}',\n",
       "  'gists_url': 'https://api.github.com/users/mariamabarham/gists{/gist_id}',\n",
       "  'starred_url': 'https://api.github.com/users/mariamabarham/starred{/owner}{/repo}',\n",
       "  'subscriptions_url': 'https://api.github.com/users/mariamabarham/subscriptions',\n",
       "  'organizations_url': 'https://api.github.com/users/mariamabarham/orgs',\n",
       "  'repos_url': 'https://api.github.com/users/mariamabarham/repos',\n",
       "  'events_url': 'https://api.github.com/users/mariamabarham/events{/privacy}',\n",
       "  'received_events_url': 'https://api.github.com/users/mariamabarham/received_events',\n",
       "  'type': 'User',\n",
       "  'site_admin': False},\n",
       " 'labels': [],\n",
       " 'state': 'closed',\n",
       " 'locked': False,\n",
       " 'assignee': None,\n",
       " 'assignees': [],\n",
       " 'milestone': None,\n",
       " 'comments': 0,\n",
       " 'created_at': '2020-04-14T10:18:02Z',\n",
       " 'updated_at': '2022-10-04T09:31:40Z',\n",
       " 'closed_at': '2020-04-14T12:01:40Z',\n",
       " 'author_association': 'CONTRIBUTOR',\n",
       " 'active_lock_reason': None,\n",
       " 'draft': False,\n",
       " 'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/1',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/1',\n",
       "  'diff_url': 'https://github.com/huggingface/datasets/pull/1.diff',\n",
       "  'patch_url': 'https://github.com/huggingface/datasets/pull/1.patch',\n",
       "  'merged_at': '2020-04-14T12:01:40Z'},\n",
       " 'body': '',\n",
       " 'closed_by': {'login': 'thomwolf',\n",
       "  'id': 7353373,\n",
       "  'node_id': 'MDQ6VXNlcjczNTMzNzM=',\n",
       "  'avatar_url': 'https://avatars.githubusercontent.com/u/7353373?v=4',\n",
       "  'gravatar_id': '',\n",
       "  'url': 'https://api.github.com/users/thomwolf',\n",
       "  'html_url': 'https://github.com/thomwolf',\n",
       "  'followers_url': 'https://api.github.com/users/thomwolf/followers',\n",
       "  'following_url': 'https://api.github.com/users/thomwolf/following{/other_user}',\n",
       "  'gists_url': 'https://api.github.com/users/thomwolf/gists{/gist_id}',\n",
       "  'starred_url': 'https://api.github.com/users/thomwolf/starred{/owner}{/repo}',\n",
       "  'subscriptions_url': 'https://api.github.com/users/thomwolf/subscriptions',\n",
       "  'organizations_url': 'https://api.github.com/users/thomwolf/orgs',\n",
       "  'repos_url': 'https://api.github.com/users/thomwolf/repos',\n",
       "  'events_url': 'https://api.github.com/users/thomwolf/events{/privacy}',\n",
       "  'received_events_url': 'https://api.github.com/users/thomwolf/received_events',\n",
       "  'type': 'User',\n",
       "  'site_admin': False},\n",
       " 'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/1/reactions',\n",
       "  'total_count': 0,\n",
       "  '+1': 0,\n",
       "  '-1': 0,\n",
       "  'laugh': 0,\n",
       "  'hooray': 0,\n",
       "  'confused': 0,\n",
       "  'heart': 0,\n",
       "  'rocket': 0,\n",
       "  'eyes': 0},\n",
       " 'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/1/timeline',\n",
       " 'performed_via_github_app': None,\n",
       " 'state_reason': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\raj\\repos\\hf-nlp\\.venv\\lib\\site-packages (1.0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the github token from env file\n",
    "! pip install python-dotenv\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "headers = {'Authorization': f'token {GITHUB_TOKEN}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the github issues from dataset repository\n",
    "\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get the github issues from dataset repository\n",
    "all_issues = [] # debugging\n",
    "\n",
    "def fetch_issues(\n",
    "        owner='huggingface',\n",
    "        repo='datasets',\n",
    "        num_issues=10_000,\n",
    "        rate_limit=5000,\n",
    "        issues_path=Path(\"../data\"),\n",
    "):\n",
    "    if not issues_path.is_dir():\n",
    "        issues_path.mkdir(exist_ok=True)\n",
    "\n",
    "    batch = []\n",
    "    # all_issues = []\n",
    "    per_page = 100 # number of issues to return per page\n",
    "    num_pages = math.ceil(num_issues / per_page) # number of pages to request\n",
    "    base_url = \"https://api.github.com/repos\"\n",
    "\n",
    "    for page in tqdm(range(1, num_pages)):\n",
    "        # Query with state=all to get both open and closed issues\n",
    "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
    "        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
    "        batch.extend(issues.json())\n",
    "\n",
    "        # if we reach the rate limit, save the batch and wait until we can make more requests\n",
    "        if len(batch) >= rate_limit and len(all_issues) < num_issues:\n",
    "            all_issues.extend(batch)\n",
    "            batch = [] # flush batch for next iteration\n",
    "            print(f\"Reached Github rate limit. Sleeping for 1 min...\")\n",
    "            time.sleep(60) # sleep for 1 minute \n",
    "\n",
    "    # save the remaining issues\n",
    "    all_issues.extend(batch)\n",
    "    df = pd.Dataframe.from_records(all_issues)\n",
    "    df.to_json(f\"{issues_path}/{repo}-issues.jsonl, orient='records', lines=True\")\n",
    "    print (f\"Downloaded all the issues for {repo}! Dataset saved at {issues_path}/{repo}-issues.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/99 [01:00<00:56,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached Github rate limit. Sleeping for 1 min...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [02:23<00:00,  1.45s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'Dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# fetch the issues from the dataset repository\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m fetch_issues()\n",
      "Cell \u001b[1;32mIn[18], line 35\u001b[0m, in \u001b[0;36mfetch_issues\u001b[1;34m(owner, repo, num_issues, rate_limit, issues_path)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39m# save the remaining issues\u001b[39;00m\n\u001b[0;32m     34\u001b[0m all_issues\u001b[39m.\u001b[39mextend(batch)\n\u001b[1;32m---> 35\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataframe\u001b[39m.\u001b[39mfrom_records(all_issues)\n\u001b[0;32m     36\u001b[0m df\u001b[39m.\u001b[39mto_json(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00missues_path\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrepo\u001b[39m}\u001b[39;00m\u001b[39m-issues.jsonl, orient=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrecords\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, lines=True\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloaded all the issues for \u001b[39m\u001b[39m{\u001b[39;00mrepo\u001b[39m}\u001b[39;00m\u001b[39m! Dataset saved at \u001b[39m\u001b[39m{\u001b[39;00missues_path\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrepo\u001b[39m}\u001b[39;00m\u001b[39m-issues.jsonl\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'Dataframe'"
     ]
    }
   ],
   "source": [
    "# fetch the issues from the dataset repository\n",
    "fetch_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5841"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_issues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>repository_url</th>\n",
       "      <th>labels_url</th>\n",
       "      <th>comments_url</th>\n",
       "      <th>events_url</th>\n",
       "      <th>html_url</th>\n",
       "      <th>id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>closed_at</th>\n",
       "      <th>author_association</th>\n",
       "      <th>active_lock_reason</th>\n",
       "      <th>body</th>\n",
       "      <th>reactions</th>\n",
       "      <th>timeline_url</th>\n",
       "      <th>performed_via_github_app</th>\n",
       "      <th>state_reason</th>\n",
       "      <th>draft</th>\n",
       "      <th>pull_request</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>1754359316</td>\n",
       "      <td>I_kwDODunzps5okWYU</td>\n",
       "      <td>5947</td>\n",
       "      <td>Return the audio filename when decoding fails ...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NONE</td>\n",
       "      <td>None</td>\n",
       "      <td>### Feature request\\r\\n\\r\\nReturn the audio fi...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>1754234469</td>\n",
       "      <td>I_kwDODunzps5oj35l</td>\n",
       "      <td>5946</td>\n",
       "      <td>IndexError Not Solving -&gt; IndexError: Invalid ...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NONE</td>\n",
       "      <td>None</td>\n",
       "      <td>### Describe the bug\\n\\nin &lt;cell line: 1&gt;:1   ...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>1754084577</td>\n",
       "      <td>I_kwDODunzps5ojTTh</td>\n",
       "      <td>5945</td>\n",
       "      <td>Failing to upload dataset to the hub</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NONE</td>\n",
       "      <td>None</td>\n",
       "      <td>### Describe the bug\\n\\nTrying to upload a dat...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/5944</td>\n",
       "      <td>1752882200</td>\n",
       "      <td>PR_kwDODunzps5Sx7O4</td>\n",
       "      <td>5944</td>\n",
       "      <td>Arrow dataset builder to be able to load and s...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>None</td>\n",
       "      <td>This adds a Arrow dataset builder to be able t...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>1752824336</td>\n",
       "      <td>I_kwDODunzps5oefoQ</td>\n",
       "      <td>5943</td>\n",
       "      <td>Language `lzh` is not shown on the web interface</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NONE</td>\n",
       "      <td>None</td>\n",
       "      <td>### Describe the bug\\r\\n\\r\\nDespite its popula...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                      repository_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datasets   \n",
       "1  https://api.github.com/repos/huggingface/datasets   \n",
       "2  https://api.github.com/repos/huggingface/datasets   \n",
       "3  https://api.github.com/repos/huggingface/datasets   \n",
       "4  https://api.github.com/repos/huggingface/datasets   \n",
       "\n",
       "                                          labels_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                        comments_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                          events_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                            html_url          id  \\\n",
       "0  https://github.com/huggingface/datasets/issues...  1754359316   \n",
       "1  https://github.com/huggingface/datasets/issues...  1754234469   \n",
       "2  https://github.com/huggingface/datasets/issues...  1754084577   \n",
       "3  https://github.com/huggingface/datasets/pull/5944  1752882200   \n",
       "4  https://github.com/huggingface/datasets/issues...  1752824336   \n",
       "\n",
       "               node_id  number  \\\n",
       "0   I_kwDODunzps5okWYU    5947   \n",
       "1   I_kwDODunzps5oj35l    5946   \n",
       "2   I_kwDODunzps5ojTTh    5945   \n",
       "3  PR_kwDODunzps5Sx7O4    5944   \n",
       "4   I_kwDODunzps5oefoQ    5943   \n",
       "\n",
       "                                               title  ... closed_at  \\\n",
       "0  Return the audio filename when decoding fails ...  ...      None   \n",
       "1  IndexError Not Solving -> IndexError: Invalid ...  ...      None   \n",
       "2               Failing to upload dataset to the hub  ...      None   \n",
       "3  Arrow dataset builder to be able to load and s...  ...      None   \n",
       "4   Language `lzh` is not shown on the web interface  ...      None   \n",
       "\n",
       "  author_association active_lock_reason  \\\n",
       "0               NONE               None   \n",
       "1               NONE               None   \n",
       "2               NONE               None   \n",
       "3        CONTRIBUTOR               None   \n",
       "4               NONE               None   \n",
       "\n",
       "                                                body  \\\n",
       "0  ### Feature request\\r\\n\\r\\nReturn the audio fi...   \n",
       "1  ### Describe the bug\\n\\nin <cell line: 1>:1   ...   \n",
       "2  ### Describe the bug\\n\\nTrying to upload a dat...   \n",
       "3  This adds a Arrow dataset builder to be able t...   \n",
       "4  ### Describe the bug\\r\\n\\r\\nDespite its popula...   \n",
       "\n",
       "                                           reactions  \\\n",
       "0  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "1  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "2  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "3  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "4  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "\n",
       "                                        timeline_url performed_via_github_app  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...                     None   \n",
       "1  https://api.github.com/repos/huggingface/datas...                     None   \n",
       "2  https://api.github.com/repos/huggingface/datas...                     None   \n",
       "3  https://api.github.com/repos/huggingface/datas...                     None   \n",
       "4  https://api.github.com/repos/huggingface/datas...                     None   \n",
       "\n",
       "   state_reason  draft                                       pull_request  \n",
       "0          None    NaN                                                NaN  \n",
       "1          None    NaN                                                NaN  \n",
       "2          None    NaN                                                NaN  \n",
       "3          None  False  {'url': 'https://api.github.com/repos/huggingf...  \n",
       "4          None    NaN                                                NaN  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe from all_issues list\n",
    "df = pd.DataFrame.from_records(all_issues)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>repository_url</th>\n",
       "      <th>labels_url</th>\n",
       "      <th>comments_url</th>\n",
       "      <th>events_url</th>\n",
       "      <th>html_url</th>\n",
       "      <th>id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>closed_at</th>\n",
       "      <th>author_association</th>\n",
       "      <th>active_lock_reason</th>\n",
       "      <th>body</th>\n",
       "      <th>reactions</th>\n",
       "      <th>timeline_url</th>\n",
       "      <th>performed_via_github_app</th>\n",
       "      <th>state_reason</th>\n",
       "      <th>draft</th>\n",
       "      <th>pull_request</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5836</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues/5</td>\n",
       "      <td>600295889</td>\n",
       "      <td>MDU6SXNzdWU2MDAyOTU4ODk=</td>\n",
       "      <td>5</td>\n",
       "      <td>ValueError when a split is empty</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-04-29T09:23:05Z</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>None</td>\n",
       "      <td>When a split is empty either TEST, VALIDATION ...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>completed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5837</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues/4</td>\n",
       "      <td>600185417</td>\n",
       "      <td>MDU6SXNzdWU2MDAxODU0MTc=</td>\n",
       "      <td>4</td>\n",
       "      <td>[Feature] Keep the list of labels of a dataset...</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-05-04T06:11:57Z</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>None</td>\n",
       "      <td>It would be useful to keep the list of the lab...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>completed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5838</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues/3</td>\n",
       "      <td>600180050</td>\n",
       "      <td>MDU6SXNzdWU2MDAxODAwNTA=</td>\n",
       "      <td>3</td>\n",
       "      <td>[Feature] More dataset outputs</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-05-04T06:12:27Z</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>None</td>\n",
       "      <td>Add the following dataset outputs:\\r\\n\\r\\n- Sp...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>completed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5839</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2</td>\n",
       "      <td>599767671</td>\n",
       "      <td>MDU6SXNzdWU1OTk3Njc2NzE=</td>\n",
       "      <td>2</td>\n",
       "      <td>Issue to read a local dataset</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-05-11T18:55:22Z</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>None</td>\n",
       "      <td>Hello,\\r\\n\\r\\nAs proposed by @thomwolf, I open...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>completed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5840</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/1</td>\n",
       "      <td>599457467</td>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NDAzMDk1NDYw</td>\n",
       "      <td>1</td>\n",
       "      <td>changing nlp.bool to nlp.bool_</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-04-14T12:01:40Z</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url  \\\n",
       "5836  https://api.github.com/repos/huggingface/datas...   \n",
       "5837  https://api.github.com/repos/huggingface/datas...   \n",
       "5838  https://api.github.com/repos/huggingface/datas...   \n",
       "5839  https://api.github.com/repos/huggingface/datas...   \n",
       "5840  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                         repository_url  \\\n",
       "5836  https://api.github.com/repos/huggingface/datasets   \n",
       "5837  https://api.github.com/repos/huggingface/datasets   \n",
       "5838  https://api.github.com/repos/huggingface/datasets   \n",
       "5839  https://api.github.com/repos/huggingface/datasets   \n",
       "5840  https://api.github.com/repos/huggingface/datasets   \n",
       "\n",
       "                                             labels_url  \\\n",
       "5836  https://api.github.com/repos/huggingface/datas...   \n",
       "5837  https://api.github.com/repos/huggingface/datas...   \n",
       "5838  https://api.github.com/repos/huggingface/datas...   \n",
       "5839  https://api.github.com/repos/huggingface/datas...   \n",
       "5840  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                           comments_url  \\\n",
       "5836  https://api.github.com/repos/huggingface/datas...   \n",
       "5837  https://api.github.com/repos/huggingface/datas...   \n",
       "5838  https://api.github.com/repos/huggingface/datas...   \n",
       "5839  https://api.github.com/repos/huggingface/datas...   \n",
       "5840  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                             events_url  \\\n",
       "5836  https://api.github.com/repos/huggingface/datas...   \n",
       "5837  https://api.github.com/repos/huggingface/datas...   \n",
       "5838  https://api.github.com/repos/huggingface/datas...   \n",
       "5839  https://api.github.com/repos/huggingface/datas...   \n",
       "5840  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                              html_url         id  \\\n",
       "5836  https://github.com/huggingface/datasets/issues/5  600295889   \n",
       "5837  https://github.com/huggingface/datasets/issues/4  600185417   \n",
       "5838  https://github.com/huggingface/datasets/issues/3  600180050   \n",
       "5839  https://github.com/huggingface/datasets/issues/2  599767671   \n",
       "5840    https://github.com/huggingface/datasets/pull/1  599457467   \n",
       "\n",
       "                               node_id  number  \\\n",
       "5836          MDU6SXNzdWU2MDAyOTU4ODk=       5   \n",
       "5837          MDU6SXNzdWU2MDAxODU0MTc=       4   \n",
       "5838          MDU6SXNzdWU2MDAxODAwNTA=       3   \n",
       "5839          MDU6SXNzdWU1OTk3Njc2NzE=       2   \n",
       "5840  MDExOlB1bGxSZXF1ZXN0NDAzMDk1NDYw       1   \n",
       "\n",
       "                                                  title  ...  \\\n",
       "5836                   ValueError when a split is empty  ...   \n",
       "5837  [Feature] Keep the list of labels of a dataset...  ...   \n",
       "5838                     [Feature] More dataset outputs  ...   \n",
       "5839                      Issue to read a local dataset  ...   \n",
       "5840                     changing nlp.bool to nlp.bool_  ...   \n",
       "\n",
       "                 closed_at author_association active_lock_reason  \\\n",
       "5836  2020-04-29T09:23:05Z        CONTRIBUTOR               None   \n",
       "5837  2020-05-04T06:11:57Z        CONTRIBUTOR               None   \n",
       "5838  2020-05-04T06:12:27Z        CONTRIBUTOR               None   \n",
       "5839  2020-05-11T18:55:22Z        CONTRIBUTOR               None   \n",
       "5840  2020-04-14T12:01:40Z        CONTRIBUTOR               None   \n",
       "\n",
       "                                                   body  \\\n",
       "5836  When a split is empty either TEST, VALIDATION ...   \n",
       "5837  It would be useful to keep the list of the lab...   \n",
       "5838  Add the following dataset outputs:\\r\\n\\r\\n- Sp...   \n",
       "5839  Hello,\\r\\n\\r\\nAs proposed by @thomwolf, I open...   \n",
       "5840                                                      \n",
       "\n",
       "                                              reactions  \\\n",
       "5836  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "5837  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "5838  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "5839  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "5840  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "\n",
       "                                           timeline_url  \\\n",
       "5836  https://api.github.com/repos/huggingface/datas...   \n",
       "5837  https://api.github.com/repos/huggingface/datas...   \n",
       "5838  https://api.github.com/repos/huggingface/datas...   \n",
       "5839  https://api.github.com/repos/huggingface/datas...   \n",
       "5840  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "     performed_via_github_app  state_reason  draft  \\\n",
       "5836                     None     completed    NaN   \n",
       "5837                     None     completed    NaN   \n",
       "5838                     None     completed    NaN   \n",
       "5839                     None     completed    NaN   \n",
       "5840                     None          None  False   \n",
       "\n",
       "                                           pull_request  \n",
       "5836                                                NaN  \n",
       "5837                                                NaN  \n",
       "5838                                                NaN  \n",
       "5839                                                NaN  \n",
       "5840  {'url': 'https://api.github.com/repos/huggingf...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded all the issues for datasets! Dataset saved at ..\\data/datasets-issues.jsonl\n"
     ]
    }
   ],
   "source": [
    "owner='huggingface'\n",
    "repo='datasets'\n",
    "num_issues=10_000\n",
    "rate_limit=5000\n",
    "issues_path=Path(\"../data\")\n",
    "df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient='records', lines=True)\n",
    "print (f\"Downloaded all the issues for {repo}! Dataset saved at {issues_path}/{repo}-issues.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:/Users/Raj/.cache/huggingface/datasets/json/default-db71a5ded63c27a6/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91ff88ce87949788f343a929b5c61dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3f50bfaa034960bb080591c02dc6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a448bf28abe541eda4e3afeeedda0f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\builder.py:1873\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[0;32m   1866\u001b[0m     writer \u001b[39m=\u001b[39m writer_class(\n\u001b[0;32m   1867\u001b[0m         features\u001b[39m=\u001b[39mwriter\u001b[39m.\u001b[39m_features,\n\u001b[0;32m   1868\u001b[0m         path\u001b[39m=\u001b[39mfpath\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mSSSSS\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mshard_id\u001b[39m:\u001b[39;00m\u001b[39m05d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mJJJJJ\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mjob_id\u001b[39m:\u001b[39;00m\u001b[39m05d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1871\u001b[0m         embed_local_files\u001b[39m=\u001b[39membed_local_files,\n\u001b[0;32m   1872\u001b[0m     )\n\u001b[1;32m-> 1873\u001b[0m writer\u001b[39m.\u001b[39;49mwrite_table(table)\n\u001b[0;32m   1874\u001b[0m num_examples_progress_update \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(table)\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\arrow_writer.py:568\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[1;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[0;32m    567\u001b[0m pa_table \u001b[39m=\u001b[39m pa_table\u001b[39m.\u001b[39mcombine_chunks()\n\u001b[1;32m--> 568\u001b[0m pa_table \u001b[39m=\u001b[39m table_cast(pa_table, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_schema)\n\u001b[0;32m    569\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_local_files:\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\table.py:2290\u001b[0m, in \u001b[0;36mtable_cast\u001b[1;34m(table, schema)\u001b[0m\n\u001b[0;32m   2289\u001b[0m \u001b[39mif\u001b[39;00m table\u001b[39m.\u001b[39mschema \u001b[39m!=\u001b[39m schema:\n\u001b[1;32m-> 2290\u001b[0m     \u001b[39mreturn\u001b[39;00m cast_table_to_schema(table, schema)\n\u001b[0;32m   2291\u001b[0m \u001b[39melif\u001b[39;00m table\u001b[39m.\u001b[39mschema\u001b[39m.\u001b[39mmetadata \u001b[39m!=\u001b[39m schema\u001b[39m.\u001b[39mmetadata:\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\table.py:2249\u001b[0m, in \u001b[0;36mcast_table_to_schema\u001b[1;34m(table, schema)\u001b[0m\n\u001b[0;32m   2248\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt cast\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtable\u001b[39m.\u001b[39mschema\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mto\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfeatures\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mbecause column names don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2249\u001b[0m arrays \u001b[39m=\u001b[39m [cast_array_to_feature(table[name], feature) \u001b[39mfor\u001b[39;49;00m name, feature \u001b[39min\u001b[39;49;00m features\u001b[39m.\u001b[39;49mitems()]\n\u001b[0;32m   2250\u001b[0m \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_arrays(arrays, schema\u001b[39m=\u001b[39mschema)\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\table.py:2249\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2248\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt cast\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtable\u001b[39m.\u001b[39mschema\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mto\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfeatures\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mbecause column names don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2249\u001b[0m arrays \u001b[39m=\u001b[39m [cast_array_to_feature(table[name], feature) \u001b[39mfor\u001b[39;00m name, feature \u001b[39min\u001b[39;00m features\u001b[39m.\u001b[39mitems()]\n\u001b[0;32m   2250\u001b[0m \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_arrays(arrays, schema\u001b[39m=\u001b[39mschema)\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\table.py:1817\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[1;34m(array, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1816\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(array, pa\u001b[39m.\u001b[39mChunkedArray):\n\u001b[1;32m-> 1817\u001b[0m     \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mchunked_array([func(chunk, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m array\u001b[39m.\u001b[39;49mchunks])\n\u001b[0;32m   1818\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\table.py:1817\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1816\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(array, pa\u001b[39m.\u001b[39mChunkedArray):\n\u001b[1;32m-> 1817\u001b[0m     \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mchunked_array([func(chunk, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m array\u001b[39m.\u001b[39mchunks])\n\u001b[0;32m   1818\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\table.py:2043\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[1;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[0;32m   2042\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(feature, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m {field\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m field \u001b[39min\u001b[39;00m array\u001b[39m.\u001b[39mtype} \u001b[39m==\u001b[39m \u001b[39mset\u001b[39m(feature):\n\u001b[1;32m-> 2043\u001b[0m     arrays \u001b[39m=\u001b[39m [_c(array\u001b[39m.\u001b[39;49mfield(name), subfeature) \u001b[39mfor\u001b[39;49;00m name, subfeature \u001b[39min\u001b[39;49;00m feature\u001b[39m.\u001b[39;49mitems()]\n\u001b[0;32m   2044\u001b[0m     \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mStructArray\u001b[39m.\u001b[39mfrom_arrays(arrays, names\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(feature), mask\u001b[39m=\u001b[39marray\u001b[39m.\u001b[39mis_null())\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\table.py:2043\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2042\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(feature, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m {field\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m field \u001b[39min\u001b[39;00m array\u001b[39m.\u001b[39mtype} \u001b[39m==\u001b[39m \u001b[39mset\u001b[39m(feature):\n\u001b[1;32m-> 2043\u001b[0m     arrays \u001b[39m=\u001b[39m [_c(array\u001b[39m.\u001b[39;49mfield(name), subfeature) \u001b[39mfor\u001b[39;00m name, subfeature \u001b[39min\u001b[39;00m feature\u001b[39m.\u001b[39mitems()]\n\u001b[0;32m   2044\u001b[0m     \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mStructArray\u001b[39m.\u001b[39mfrom_arrays(arrays, names\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(feature), mask\u001b[39m=\u001b[39marray\u001b[39m.\u001b[39mis_null())\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\table.py:1819\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[1;34m(array, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1818\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1819\u001b[0m     \u001b[39mreturn\u001b[39;00m func(array, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\table.py:2109\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[1;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[0;32m   2108\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(feature, (Sequence, \u001b[39mdict\u001b[39m, \u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m-> 2109\u001b[0m     \u001b[39mreturn\u001b[39;00m array_cast(array, feature(), allow_number_to_str\u001b[39m=\u001b[39;49mallow_number_to_str)\n\u001b[0;32m   2110\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt cast array of type\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00marray\u001b[39m.\u001b[39mtype\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mto\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfeature\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\table.py:1819\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[1;34m(array, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1818\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1819\u001b[0m     \u001b[39mreturn\u001b[39;00m func(array, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\table.py:1998\u001b[0m, in \u001b[0;36marray_cast\u001b[1;34m(array, pa_type, allow_number_to_str)\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[39mif\u001b[39;00m pa\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_null(pa_type) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m pa\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_null(array\u001b[39m.\u001b[39mtype):\n\u001b[1;32m-> 1998\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt cast array of type \u001b[39m\u001b[39m{\u001b[39;00marray\u001b[39m.\u001b[39mtype\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mpa_type\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1999\u001b[0m \u001b[39mreturn\u001b[39;00m array\u001b[39m.\u001b[39mcast(pa_type)\n",
      "\u001b[1;31mTypeError\u001b[0m: Couldn't cast array of type timestamp[s] to null",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# load the issues from local file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m issues_dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39;49m\u001b[39mjson\u001b[39;49m\u001b[39m'\u001b[39;49m, data_files\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../data/datasets-issues.jsonl\u001b[39;49m\u001b[39m'\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\load.py:1797\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1794\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[0;32m   1796\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 1797\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[0;32m   1798\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   1799\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   1800\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[0;32m   1801\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[0;32m   1802\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m   1803\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   1804\u001b[0m )\n\u001b[0;32m   1806\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[0;32m   1808\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[0;32m   1809\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\builder.py:890\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    889\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[1;32m--> 890\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[0;32m    891\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[0;32m    892\u001b[0m         verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[0;32m    893\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs,\n\u001b[0;32m    894\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs,\n\u001b[0;32m    895\u001b[0m     )\n\u001b[0;32m    896\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\builder.py:985\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    981\u001b[0m split_dict\u001b[39m.\u001b[39madd(split_generator\u001b[39m.\u001b[39msplit_info)\n\u001b[0;32m    983\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    984\u001b[0m     \u001b[39m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[1;32m--> 985\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_split(split_generator, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs)\n\u001b[0;32m    986\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    987\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    988\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot find data file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    989\u001b[0m         \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_download_instructions \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    990\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mOriginal error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[0;32m    992\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\builder.py:1746\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[1;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[0;32m   1744\u001b[0m job_id \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   1745\u001b[0m \u001b[39mwith\u001b[39;00m pbar:\n\u001b[1;32m-> 1746\u001b[0m     \u001b[39mfor\u001b[39;00m job_id, done, content \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_split_single(\n\u001b[0;32m   1747\u001b[0m         gen_kwargs\u001b[39m=\u001b[39mgen_kwargs, job_id\u001b[39m=\u001b[39mjob_id, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_prepare_split_args\n\u001b[0;32m   1748\u001b[0m     ):\n\u001b[0;32m   1749\u001b[0m         \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m   1750\u001b[0m             result \u001b[39m=\u001b[39m content\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\builder.py:1891\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[0;32m   1889\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, SchemaInferenceError) \u001b[39mand\u001b[39;00m e\u001b[39m.\u001b[39m__context__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1890\u001b[0m         e \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39m__context__\n\u001b[1;32m-> 1891\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetGenerationError(\u001b[39m\"\u001b[39m\u001b[39mAn error occurred while generating the dataset\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1893\u001b[0m \u001b[39myield\u001b[39;00m job_id, \u001b[39mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[39m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "# load the issues from local file\n",
    "issues_dataset = load_dataset('json', data_files='../data/datasets-issues.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'issues_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m issues_dataset\n",
      "\u001b[1;31mNameError\u001b[0m: name 'issues_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "issues_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
