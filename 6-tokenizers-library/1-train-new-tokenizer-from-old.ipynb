{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4462dec6738149a3a05e39ce31f84c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f5bea166be47f68740a31e19e5c40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/18.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56fad325b81f4ecf97a272cefc68cf72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/12.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset code_search_net/python to C:/Users/Raj/.cache/huggingface/datasets/code_search_net/python/1.0.0/8f2524e6b62f65af5f5d65c53715c654db7b08dc93e0b7bcce2ab2f286a75be1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8effcefdfc43f1ab69d498b200f839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8c617f60f940c3b02b18b3defa70f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/941M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7225ad97885a4d5d8b788230e710cdbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4675521ff463424894386e1f8f572feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb263b7f87434fbcb06484635f73e0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c60e223e8cd411f8a33421ab1e3839d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c92f5a34a3647db9ce6c9e09023ba91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset code_search_net downloaded and prepared to C:/Users/Raj/.cache/huggingface/datasets/code_search_net/python/1.0.0/8f2524e6b62f65af5f5d65c53715c654db7b08dc93e0b7bcce2ab2f286a75be1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0dde158fa1b40a5aae743e9739e6a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\n",
      "    \"\"\"\n",
      "    Trains a k-nearest neighbors classifier for face recognition.\n",
      "\n",
      "    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n",
      "\n",
      "     (View in source code to see train_dir example tree structure)\n",
      "\n",
      "     Structure:\n",
      "        <train_dir>/\n",
      "        ├── <person1>/\n",
      "        │   ├── <somename1>.jpeg\n",
      "        │   ├── <somename2>.jpeg\n",
      "        │   ├── ...\n",
      "        ├── <person2>/\n",
      "        │   ├── <somename1>.jpeg\n",
      "        │   └── <somename2>.jpeg\n",
      "        └── ...\n",
      "\n",
      "    :param model_save_path: (optional) path to save model on disk\n",
      "    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n",
      "    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n",
      "    :param verbose: verbosity of training\n",
      "    :return: returns knn classifier that was trained on the given data.\n",
      "    \"\"\"\n",
      "    X = []\n",
      "    y = []\n",
      "\n",
      "    # Loop through each person in the training set\n",
      "    for class_dir in os.listdir(train_dir):\n",
      "        if not os.path.isdir(os.path.join(train_dir, class_dir)):\n",
      "            continue\n",
      "\n",
      "        # Loop through each training image for the current person\n",
      "        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n",
      "            image = face_recognition.load_image_file(img_path)\n",
      "            face_bounding_boxes = face_recognition.face_locations(image)\n",
      "\n",
      "            if len(face_bounding_boxes) != 1:\n",
      "                # If there are no people (or too many people) in a training image, skip the image.\n",
      "                if verbose:\n",
      "                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn't find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\n",
      "            else:\n",
      "                # Add face encoding for current image to the training set\n",
      "                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\n",
      "                y.append(class_dir)\n",
      "\n",
      "    # Determine how many neighbors to use for weighting in the KNN classifier\n",
      "    if n_neighbors is None:\n",
      "        n_neighbors = int(round(math.sqrt(len(X))))\n",
      "        if verbose:\n",
      "            print(\"Chose n_neighbors automatically:\", n_neighbors)\n",
      "\n",
      "    # Create and train the KNN classifier\n",
      "    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')\n",
      "    knn_clf.fit(X, y)\n",
      "\n",
      "    # Save the trained KNN classifier\n",
      "    if model_save_path is not None:\n",
      "        with open(model_save_path, 'wb') as f:\n",
      "            pickle.dump(knn_clf, f)\n",
      "\n",
      "    return knn_clf\n"
     ]
    }
   ],
   "source": [
    "# print whole function string\n",
    "print(raw_datasets[\"train\"][0][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not use this unless your dataset is small\n",
    "# training_corpus = [raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a python generator, we can  avoid loading anything into memory until it is actually necessary. To create such a generator, you just need to replace the brackets with parantheses\n",
    "training_corpus = (\n",
    "    raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"] \n",
    "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# but the problem with generator object is that it can only be iterated over once\n",
    "gen_nums = (i for i in range(10))\n",
    "print(list(gen_nums))\n",
    "print(list(gen_nums)) # empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can use a function to create a new generator\n",
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also define a generator inside a for loop by using the yield keyword\n",
    "# this will produce the same result as the function above and allows you to use the generator multiple times as well as use more complex logic than can be done through a list comprehension\n",
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        samples = dataset[i: i + 1000][\"whole_func_string\"]\n",
    "        yield samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
