{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84a2ce4267041ddb70086123058e192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/9.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849775ba67b94c629aa5e0bf87c54c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/3.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c909eb404beb416ca9bde440183283c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset conll2003/conll2003 to C:/Users/Raj/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a8980c00854d6d8d25e691930c6ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc12baace8948dcb5a0bab6770e9133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffe5e9dd5d145aea0c0c81cba462b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39be1b8f918c4a238e4dcae6ce767add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset conll2003 downloaded and prepared to C:/Users/Raj/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fee75d942f49d48b20b003684a820d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the dataset\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the first element of the dataset\n",
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the ner tags\n",
    "label_names = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU    rejects German call to boycott British lamb . \n",
      "B-ORG O       B-MISC O    O  O       B-MISC  O    O \n"
     ]
    }
   ],
   "source": [
    "# decoding the labels gives us the following\n",
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"',\n",
       " \"''\",\n",
       " '#',\n",
       " '$',\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '.',\n",
       " ':',\n",
       " '``',\n",
       " 'CC',\n",
       " 'CD',\n",
       " 'DT',\n",
       " 'EX',\n",
       " 'FW',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'JJR',\n",
       " 'JJS',\n",
       " 'LS',\n",
       " 'MD',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'NNPS',\n",
       " 'NNS',\n",
       " 'NN|SYM',\n",
       " 'PDT',\n",
       " 'POS',\n",
       " 'PRP',\n",
       " 'PRP$',\n",
       " 'RB',\n",
       " 'RBR',\n",
       " 'RBS',\n",
       " 'RP',\n",
       " 'SYM',\n",
       " 'TO',\n",
       " 'UH',\n",
       " 'VB',\n",
       " 'VBD',\n",
       " 'VBG',\n",
       " 'VBN',\n",
       " 'VBP',\n",
       " 'VBZ',\n",
       " 'WDT',\n",
       " 'WP',\n",
       " 'WP$',\n",
       " 'WRB']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print pos_tags labels\n",
    "label_names = raw_datasets[\"train\"].features[\"pos_tags\"].feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU  rejects German call to boycott British lamb . \n",
      "NNP VBZ     JJ     NN   TO VB      JJ      NN   . \n"
     ]
    }
   ],
   "source": [
    "# decoding the labels gives us the following\n",
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"pos_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-ADJP',\n",
       " 'I-ADJP',\n",
       " 'B-ADVP',\n",
       " 'I-ADVP',\n",
       " 'B-CONJP',\n",
       " 'I-CONJP',\n",
       " 'B-INTJ',\n",
       " 'I-INTJ',\n",
       " 'B-LST',\n",
       " 'I-LST',\n",
       " 'B-NP',\n",
       " 'I-NP',\n",
       " 'B-PP',\n",
       " 'I-PP',\n",
       " 'B-PRT',\n",
       " 'I-PRT',\n",
       " 'B-SBAR',\n",
       " 'I-SBAR',\n",
       " 'B-UCP',\n",
       " 'I-UCP',\n",
       " 'B-VP',\n",
       " 'I-VP']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print chunk_tags labels\n",
    "label_names = raw_datasets[\"train\"].features[\"chunk_tags\"].feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU   rejects German call to   boycott British lamb . \n",
      "B-NP B-VP    B-NP   I-NP B-VP I-VP    B-NP    I-NP O \n"
     ]
    }
   ],
   "source": [
    "# decoding the labels gives us the following\n",
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"chunk_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '4',\n",
       " 'tokens': ['Germany',\n",
       "  \"'s\",\n",
       "  'representative',\n",
       "  'to',\n",
       "  'the',\n",
       "  'European',\n",
       "  'Union',\n",
       "  \"'s\",\n",
       "  'veterinary',\n",
       "  'committee',\n",
       "  'Werner',\n",
       "  'Zwingmann',\n",
       "  'said',\n",
       "  'on',\n",
       "  'Wednesday',\n",
       "  'consumers',\n",
       "  'should',\n",
       "  'buy',\n",
       "  'sheepmeat',\n",
       "  'from',\n",
       "  'countries',\n",
       "  'other',\n",
       "  'than',\n",
       "  'Britain',\n",
       "  'until',\n",
       "  'the',\n",
       "  'scientific',\n",
       "  'advice',\n",
       "  'was',\n",
       "  'clearer',\n",
       "  '.'],\n",
       " 'pos_tags': [22,\n",
       "  27,\n",
       "  21,\n",
       "  35,\n",
       "  12,\n",
       "  22,\n",
       "  22,\n",
       "  27,\n",
       "  16,\n",
       "  21,\n",
       "  22,\n",
       "  22,\n",
       "  38,\n",
       "  15,\n",
       "  22,\n",
       "  24,\n",
       "  20,\n",
       "  37,\n",
       "  21,\n",
       "  15,\n",
       "  24,\n",
       "  16,\n",
       "  15,\n",
       "  22,\n",
       "  15,\n",
       "  12,\n",
       "  16,\n",
       "  21,\n",
       "  38,\n",
       "  17,\n",
       "  7],\n",
       " 'chunk_tags': [11,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  21,\n",
       "  13,\n",
       "  11,\n",
       "  12,\n",
       "  21,\n",
       "  22,\n",
       "  11,\n",
       "  13,\n",
       "  11,\n",
       "  1,\n",
       "  13,\n",
       "  11,\n",
       "  17,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  21,\n",
       "  1,\n",
       "  0],\n",
       " 'ner_tags': [5,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the second element of the dataset\n",
    "raw_datasets[\"train\"][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A big difference in the case of token classification tasks is that we have pre-tokenized inputs. Fortunately, the tokenizer API can deal with that pretty easily. We just need to warn the tokenizer that weâ€™re passing it pre-tokenized inputs by setting is_split_into_words=True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our tokenizer object\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the tokenizer is fast\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 'rejects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize a pre-tokenized input by add the parameter is_split_into_words=True\n",
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the fast tokenizer super power to map each token to its corresponding word\n",
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align label with the token that contains the word\n",
    "# 1. create a mapping from each label to its corresponding token\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
      "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "# Let's try it on our first sentence\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To preprocess our whole dataset, we need to tokenize all the inputs and apply align_labels_with_tokens to all the labels.\n",
    "# We can do this by applying a function to all the elements of a dataset using the map method.\n",
    "# And batch the result using the batch method.\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "        \n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e199ade77c6c4343853b578e1689f1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175a7872076f4b3c82a6d6e895fa5c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c581f8e1803434a8dd5319e3a83b28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# apply the function to all the elements of the dataset\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels, \n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine tuning the model with the Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use data collator to pad the inputs and labels\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  7270, 22961,  1528,  1840,  1106, 21423,  1418,  2495, 12913,\n",
       "            119,   102],\n",
       "         [  101,  1943, 14428,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
       "         [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the data collator on a few samples\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"input_ids\"], batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n",
      "[-100, 1, 2, -100]\n"
     ]
    }
   ],
   "source": [
    "# compare this to the original dataset\n",
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see, the second set of labels has been padded to the length of the first one using -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "                                              0.0/43.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.6/43.6 kB ? eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\raj\\repos\\hf-nlp\\.venv\\lib\\site-packages (from seqeval) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\raj\\repos\\hf-nlp\\.venv\\lib\\site-packages (from seqeval) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\raj\\repos\\hf-nlp\\.venv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\raj\\repos\\hf-nlp\\.venv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\raj\\repos\\hf-nlp\\.venv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (pyproject.toml): started\n",
      "  Building wheel for seqeval (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16190 sha256=00cad1bfd5fd8eda8470a70e58fa23ce921753b6f39d9c14b4b883d6e4a36d27\n",
      "  Stored in directory: c:\\users\\raj\\appdata\\local\\pip\\cache\\wheels\\bc\\92\\f0\\243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n"
     ]
    }
   ],
   "source": [
    "# The traditional framework used to evaluate token classification prediction is seqeval. To use this metric we need to install the seqeval library.\n",
    "! pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599a89784b7546cd860cebb5e79295bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can then load it via the evaluate.load() function\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This metric does not behave like the standard accuracy: it will actually take the list of labels as strings, not integers. So we need to fully decode the predictions and labels before passing them to the metric.\n",
    "# First we get the labels for our first training example\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "# label_names = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "labels = [label_names[label] for label in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MISC': {'precision': 1.0,\n",
       "  'recall': 0.5,\n",
       "  'f1': 0.6666666666666666,\n",
       "  'number': 2},\n",
       " 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 0.6666666666666666,\n",
       " 'overall_f1': 0.8,\n",
       " 'overall_accuracy': 0.8888888888888888}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can create fake predictions for those by just changing the value at index 2\n",
    "predictions = labels.copy()\n",
    "predictions[2] = \"O\"\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute metrics function\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are working on a token classification task, we use the AutoModelForTokenClassification class.\n",
    "# The main thing to remember when defining this model is to pass along the number of labels to the model using the num_labels argument.\n",
    "# But if we want a nice inference widget working, it's better to set the correct lable correspondences instead of just passing the number of labels.\n",
    "\n",
    "# We can do this by passing the label2id dictionary to the model config.\n",
    "# We can get this dictionary from the features of our dataset.\n",
    "# We also need to set the id2label dictionary to the model config to get nice labels when using the inference widget.\n",
    "# We can get this dictionary by inverting the label2id dictionary.\n",
    "# We can then pass the model config to the AutoModelForTokenClassification class.\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# we can then pass the above dictionaries to the model config, saved properly and upload to the hub\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Make sure the right number of labels are used in the model. Otherwise, will get an obscure error when calling the Trainer.train() method later on. Something like \"CUDA error: device-side assert triggered\". This is the number one cause of bugs reported by users for such errors.\n",
    "print(model.num_labels)\n",
    "print(len(label_names))\n",
    "assert model.num_labels == len(label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two things to do before we define our Trainer\n",
    "# 1. Log in to Hugging Face\n",
    "# 2. Define training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f425589a524925ba86a8782ea031a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training arguments, hyperparameters and save the model and evaluate it at the end of each epoch, upload results to the hub\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raj\\repos\\hf-nlp\\7-main-nlp-tasks\\bert-finetuned-ner is already a clone of https://huggingface.co/RajkNakka/bert-finetuned-ner. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "# Finally we just pass everything to the Trainer and start training\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = tokenized_datasets[\"train\"],\n",
    "    eval_dataset = tokenized_datasets[\"validation\"],\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e613cbf27045f2897bf76bd0e6ac02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2632, 'learning_rate': 1.810174639331815e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0986, 'learning_rate': 1.6203492786636296e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0851, 'learning_rate': 1.4305239179954442e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9aa059df76b44c19d353e0f87c80544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06431853026151657, 'eval_precision': 0.9149076517150396, 'eval_recall': 0.9336923594749242, 'eval_f1': 0.9242045643844745, 'eval_accuracy': 0.9827220815918055, 'eval_runtime': 4.0787, 'eval_samples_per_second': 796.828, 'eval_steps_per_second': 99.787, 'epoch': 1.0}\n",
      "{'loss': 0.0596, 'learning_rate': 1.240698557327259e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0464, 'learning_rate': 1.0508731966590738e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0434, 'learning_rate': 8.610478359908885e-06, 'epoch': 1.71}\n",
      "{'loss': 0.0354, 'learning_rate': 6.712224753227031e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc2a6001409410ea0e20656404b9051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.059338293969631195, 'eval_precision': 0.9275936005277915, 'eval_recall': 0.9464826657691013, 'eval_f1': 0.9369429404414827, 'eval_accuracy': 0.9858568316948254, 'eval_runtime': 4.1024, 'eval_samples_per_second': 792.212, 'eval_steps_per_second': 99.209, 'epoch': 2.0}\n",
      "{'loss': 0.0213, 'learning_rate': 4.8139711465451785e-06, 'epoch': 2.28}\n",
      "{'loss': 0.0233, 'learning_rate': 2.9157175398633257e-06, 'epoch': 2.56}\n",
      "{'loss': 0.0176, 'learning_rate': 1.0174639331814731e-06, 'epoch': 2.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753b228878744a2897b575cfd28ff296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06247725710272789, 'eval_precision': 0.9346891534391535, 'eval_recall': 0.9513631773813531, 'eval_f1': 0.942952460383653, 'eval_accuracy': 0.9863866486136458, 'eval_runtime': 4.0862, 'eval_samples_per_second': 795.354, 'eval_steps_per_second': 99.603, 'epoch': 3.0}\n",
      "{'train_runtime': 194.9061, 'train_samples_per_second': 216.119, 'train_steps_per_second': 27.028, 'train_loss': 0.0670818526724929, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.0670818526724929, metrics={'train_runtime': 194.9061, 'train_samples_per_second': 216.119, 'train_steps_per_second': 27.028, 'train_loss': 0.0670818526724929, 'epoch': 3.0})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start training\n",
    "# while the training happens, each time the model is saved (here, very epoch), it will be uploaded to the hub in the background. This way, you will be able to resume training from any checkpoint by just loading it from the hub or from another machine.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the final model to the hub\n",
    "trainer.push_to_hub(commit_message=\"finetuned on conll2003\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training using HF Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to build the Dataloaders from our datasets\n",
    "# we will reuse our data_collator as collate_fn and shuffle the training dataset but not the validation dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], \n",
    "    shuffle=True, \n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# we need to reinstate the model to make sure we are not continuing fine tuning from before but starting from the BERT pretrained model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need an optimizer. We will use the classic AdamW optimizer but with a fix in the way the weight decay is computed (decoupled weight decay).\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we have all these objects, we can send them to the accelerator.prepare method to make sure they are on the right device.\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the scheduler using the get_scheduler method from the transformers library.\n",
    "# we can use the length of the training dataloader to compute the total number of training steps.\n",
    "# Remember to always do this after preparing the dataloader as that method will change its length.\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "max_training_steps = num_update_steps_per_epoch * num_train_epochs\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", # linear scheduler with no warmup steps\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=max_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RajkNakka/bert-finetuned-ner-accelerate'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lastly, to push the model to the Hub, we will need to create a Repository object in a working folder.\n",
    "# First, log into HF if not logged in already.\n",
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"bert-finetuned-ner-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raj\\repos\\hf-nlp\\7-main-nlp-tasks\\../../bert-finetuned-ner-accelerate is already a clone of https://huggingface.co/RajkNakka/bert-finetuned-ner-accelerate. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "# We clone that repository locally in a local folder. If it already exists, this local folder should be an existing clone of the repository we are working with.\n",
    "output_dir = \"../../bert-finetuned-ner-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name, use_auth_token=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now ready to write our full training loop.\n",
    "# To simplify its evaluation part, we define this postprocess() function that will decode the predictions and labels and compute the metrics.\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can write the training loop. After defining a progress bar to follow how training goes, the loop has three parts:\n",
    "\n",
    "The training in itself, which is the classic iteration over the train_dataloader, forward pass through the model, then backward pass and optimizer step.\n",
    "\n",
    "The evaluation, in which there is a novelty after getting the outputs of our model on a batch: since two processes may have padded the inputs and labels to different shapes, we need to use accelerator.pad_across_processes() to make the predictions and labels the same shape before calling the gather() method. If we donâ€™t do this, the evaluation will either error out or hang forever. Then we send the results to metric.add_batch() and call metric.compute() once the evaluation loop is over.\n",
    "\n",
    "Saving and uploading, where we first save the model and the tokenizer, then call repo.push_to_hub(). Notice that we use the argument blocking=False to tell the ðŸ¤— Hub library to push in an asynchronous process. This way, training continues normally and this (long) instruction is executed in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'precision': 0.9010738691832086, 'recall': 0.9320094244362168, 'f1': 0.9162806088682992, 'accuracy': 0.9827809501383411}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (3) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: {'precision': 0.9241027329601581, 'recall': 0.9446314372265231, 'f1': 0.9342543275632491, 'accuracy': 0.9848266321304527}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (4) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: {'precision': 0.9322873658133773, 'recall': 0.9500168293503871, 'f1': 0.9410686004834542, 'accuracy': 0.9862247601106728}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (5) will be pushed upstream.\n"
     ]
    }
   ],
   "source": [
    "# We can now write our training loop.\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(max_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_labels, true_predictions = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "\n",
    "    print(f\"epoch {epoch}:\",\n",
    "          {\n",
    "              key: results[f\"overall_{key}\"]\n",
    "                for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "          },\n",
    "    )\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c83e336a1154941ac80d337c1d30d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f487a05757c84e4e8caaed08faeb1cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83805a1d457745ea8fa52d45c8900c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/315 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7341b8617544188e37441525e6419d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61101c32ee6c48258e7fa8796e195de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/669k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08403828430a44afbd392965877d8dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ORG',\n",
       "  'score': 0.9980264,\n",
       "  'word': 'Hugging Face Inc.',\n",
       "  'start': 0,\n",
       "  'end': 17},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9990112,\n",
       "  'word': 'New York City',\n",
       "  'start': 40,\n",
       "  'end': 53},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9958806,\n",
       "  'word': 'DUMBO',\n",
       "  'start': 79,\n",
       "  'end': 84},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9933764,\n",
       "  'word': 'Manhattan Bridge',\n",
       "  'start': 113,\n",
       "  'end': 129}]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# load the model from the hub\n",
    "model_checkpoint = \"RajkNakka/bert-finetuned-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# test the model on a few samples\n",
    "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\" \\\n",
    "              \"close to the Manhattan Bridge which is visible from the window.\"\n",
    "\n",
    "token_classifier(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9924819,\n",
       "  'word': 'Raj',\n",
       "  'start': 11,\n",
       "  'end': 14},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9989982,\n",
       "  'word': 'Colorado',\n",
       "  'start': 39,\n",
       "  'end': 47}]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"My name is Raj and I live in beautiful Colorado.\"\n",
    "\n",
    "token_classifier(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ArithmeticError',\n",
       " 'AssertionError',\n",
       " 'AttributeError',\n",
       " 'BaseException',\n",
       " 'BaseExceptionGroup',\n",
       " 'BlockingIOError',\n",
       " 'BrokenPipeError',\n",
       " 'BufferError',\n",
       " 'BytesWarning',\n",
       " 'ChildProcessError',\n",
       " 'ConnectionAbortedError',\n",
       " 'ConnectionError',\n",
       " 'ConnectionRefusedError',\n",
       " 'ConnectionResetError',\n",
       " 'DeprecationWarning',\n",
       " 'EOFError',\n",
       " 'Ellipsis',\n",
       " 'EncodingWarning',\n",
       " 'EnvironmentError',\n",
       " 'Exception',\n",
       " 'ExceptionGroup',\n",
       " 'False',\n",
       " 'FileExistsError',\n",
       " 'FileNotFoundError',\n",
       " 'FloatingPointError',\n",
       " 'FutureWarning',\n",
       " 'GeneratorExit',\n",
       " 'IOError',\n",
       " 'ImportError',\n",
       " 'ImportWarning',\n",
       " 'IndentationError',\n",
       " 'IndexError',\n",
       " 'InterruptedError',\n",
       " 'IsADirectoryError',\n",
       " 'KeyError',\n",
       " 'KeyboardInterrupt',\n",
       " 'LookupError',\n",
       " 'MemoryError',\n",
       " 'ModuleNotFoundError',\n",
       " 'NameError',\n",
       " 'None',\n",
       " 'NotADirectoryError',\n",
       " 'NotImplemented',\n",
       " 'NotImplementedError',\n",
       " 'OSError',\n",
       " 'OverflowError',\n",
       " 'PendingDeprecationWarning',\n",
       " 'PermissionError',\n",
       " 'ProcessLookupError',\n",
       " 'RecursionError',\n",
       " 'ReferenceError',\n",
       " 'ResourceWarning',\n",
       " 'RuntimeError',\n",
       " 'RuntimeWarning',\n",
       " 'StopAsyncIteration',\n",
       " 'StopIteration',\n",
       " 'SyntaxError',\n",
       " 'SyntaxWarning',\n",
       " 'SystemError',\n",
       " 'SystemExit',\n",
       " 'TabError',\n",
       " 'TimeoutError',\n",
       " 'True',\n",
       " 'TypeError',\n",
       " 'UnboundLocalError',\n",
       " 'UnicodeDecodeError',\n",
       " 'UnicodeEncodeError',\n",
       " 'UnicodeError',\n",
       " 'UnicodeTranslateError',\n",
       " 'UnicodeWarning',\n",
       " 'UserWarning',\n",
       " 'ValueError',\n",
       " 'Warning',\n",
       " 'WindowsError',\n",
       " 'ZeroDivisionError',\n",
       " '__IPYTHON__',\n",
       " '__build_class__',\n",
       " '__debug__',\n",
       " '__doc__',\n",
       " '__import__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__pybind11_internals_v4_mingw_libstdcpp_cxxabi1014__',\n",
       " '__pybind11_internals_v4_msvc__',\n",
       " '__spec__',\n",
       " 'abs',\n",
       " 'aiter',\n",
       " 'all',\n",
       " 'anext',\n",
       " 'any',\n",
       " 'ascii',\n",
       " 'bin',\n",
       " 'bool',\n",
       " 'breakpoint',\n",
       " 'bytearray',\n",
       " 'bytes',\n",
       " 'callable',\n",
       " 'chr',\n",
       " 'classmethod',\n",
       " 'compile',\n",
       " 'complex',\n",
       " 'copyright',\n",
       " 'credits',\n",
       " 'delattr',\n",
       " 'dict',\n",
       " 'dir',\n",
       " 'display',\n",
       " 'divmod',\n",
       " 'enumerate',\n",
       " 'eval',\n",
       " 'exec',\n",
       " 'execfile',\n",
       " 'filter',\n",
       " 'float',\n",
       " 'format',\n",
       " 'frozenset',\n",
       " 'get_ipython',\n",
       " 'getattr',\n",
       " 'globals',\n",
       " 'hasattr',\n",
       " 'hash',\n",
       " 'help',\n",
       " 'hex',\n",
       " 'id',\n",
       " 'input',\n",
       " 'int',\n",
       " 'isinstance',\n",
       " 'issubclass',\n",
       " 'iter',\n",
       " 'len',\n",
       " 'license',\n",
       " 'list',\n",
       " 'locals',\n",
       " 'map',\n",
       " 'max',\n",
       " 'memoryview',\n",
       " 'min',\n",
       " 'next',\n",
       " 'object',\n",
       " 'oct',\n",
       " 'open',\n",
       " 'ord',\n",
       " 'pow',\n",
       " 'print',\n",
       " 'property',\n",
       " 'range',\n",
       " 'repr',\n",
       " 'reversed',\n",
       " 'round',\n",
       " 'runfile',\n",
       " 'set',\n",
       " 'setattr',\n",
       " 'slice',\n",
       " 'sorted',\n",
       " 'staticmethod',\n",
       " 'str',\n",
       " 'sum',\n",
       " 'super',\n",
       " 'tuple',\n",
       " 'type',\n",
       " 'vars',\n",
       " 'zip']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(__builtins__) # to see all the built-in functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class zip in module builtins:\n",
      "\n",
      "class zip(object)\n",
      " |  zip(*iterables, strict=False) --> Yield tuples until an input is exhausted.\n",
      " |  \n",
      " |     >>> list(zip('abcdefg', range(3), range(4)))\n",
      " |     [('a', 0, 0), ('b', 1, 1), ('c', 2, 2)]\n",
      " |  \n",
      " |  The zip object yields n-length tuples, where n is the number of iterables\n",
      " |  passed as positional arguments to zip().  The i-th element in every tuple\n",
      " |  comes from the i-th iterable argument to zip().  This continues until the\n",
      " |  shortest argument is exhausted.\n",
      " |  \n",
      " |  If strict is true and one of the arguments is exhausted before the others,\n",
      " |  raise a ValueError.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __next__(self, /)\n",
      " |      Implement next(self).\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Return state information for pickling.\n",
      " |  \n",
      " |  __setstate__(...)\n",
      " |      Set state information for unpickling.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(zip) # to see the documentation of a built-in function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
