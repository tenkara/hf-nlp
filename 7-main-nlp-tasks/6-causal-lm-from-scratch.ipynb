{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a filter function to to avoid downloading the data and instead use the streaming feature to filter it on the fly\n",
    "\n",
    "def any_keyword_in_string(string, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in string:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "#let's test it on two examples\n",
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "example_1 = \"import pandas as pd\"\n",
    "example_2 = \"import numpy as np\"\n",
    "print(any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this to create a function that will stream the dataset and filter the elements we want\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "def filter_streaming_dataset(dataset, filters):\n",
    "    filtered_dict = defaultdict(list)\n",
    "    total = 0\n",
    "    for sample in tqdm(iter(dataset)):\n",
    "        total += 1\n",
    "        if any_keyword_in_string(sample[\"content\"], filters):\n",
    "            for k, v in sample.items():\n",
    "                filtered_dict[k].append(v)\n",
    "    print(f\"{len(filtered_dict['content'])} out of {total:.2%} samples were filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2c8623682241a38748cd044c15f488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "486999it [07:29, 1572.59it/s]Got disconnected from remote data host. Retrying in 5sec [1/20]\n",
      "486999it [07:40, 1572.59it/s]Failed to read file 'gzip://file-000000000004.json::https://huggingface.co/datasets/transformersbook/codeparrot-train/resolve/0933803eb0f5956b2da9d2d7b6805fa31b18a6c8/file-000000000004.json.gz' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Invalid value. in row 0\n",
      "487815it [08:31, 953.89it/s] \n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "JSON parse error: Invalid value. in row 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\packaged_modules\\json\\json.py:134\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 134\u001b[0m         dataset \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mload(f)\n\u001b[0;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39ma JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39mkwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39;49mread(),\n\u001b[0;32m    294\u001b[0m     \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m, object_hook\u001b[39m=\u001b[39;49mobject_hook,\n\u001b[0;32m    295\u001b[0m     parse_float\u001b[39m=\u001b[39;49mparse_float, parse_int\u001b[39m=\u001b[39;49mparse_int,\n\u001b[0;32m    296\u001b[0m     parse_constant\u001b[39m=\u001b[39;49mparse_constant, object_pairs_hook\u001b[39m=\u001b[39;49mobject_pairs_hook, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n\u001b[1;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExtra data\u001b[39m\u001b[39m\"\u001b[39m, s, end)\n\u001b[0;32m    341\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 7016)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m filters \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mpandas\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msklearn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmatplotlib\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mseaborn\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m data \u001b[39m=\u001b[39m load_dataset(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtransformersbook/codeparrot-\u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, split\u001b[39m=\u001b[39msplit, streaming\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m filtered_data \u001b[39m=\u001b[39m filter_streaming_dataset(data, filters)\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36mfilter_streaming_dataset\u001b[1;34m(dataset, filters)\u001b[0m\n\u001b[0;32m      8\u001b[0m filtered_dict \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n\u001b[0;32m      9\u001b[0m total \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m tqdm(\u001b[39miter\u001b[39m(dataset)):\n\u001b[0;32m     11\u001b[0m     total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m any_keyword_in_string(sample[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m], filters):\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:981\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    978\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_pytorch(ex_iterable)\n\u001b[0;32m    979\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m \u001b[39mfor\u001b[39;00m key, example \u001b[39min\u001b[39;00m ex_iterable:\n\u001b[0;32m    982\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures:\n\u001b[0;32m    983\u001b[0m         \u001b[39m# `IterableDataset` automatically fills missing columns with None.\u001b[39;00m\n\u001b[0;32m    984\u001b[0m         \u001b[39m# This is done with `_apply_feature_types_on_example`.\u001b[39;00m\n\u001b[0;32m    985\u001b[0m         \u001b[39myield\u001b[39;00m _apply_feature_types_on_example(\n\u001b[0;32m    986\u001b[0m             example, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures, token_per_repo_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_token_per_repo_id\n\u001b[0;32m    987\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:116\u001b[0m, in \u001b[0;36mExamplesIterable.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 116\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_examples_fn(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:801\u001b[0m, in \u001b[0;36m_generate_examples_from_tables_wrapper.<locals>.wrapper\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    800\u001b[0m     python_formatter \u001b[39m=\u001b[39m PythonFormatter()\n\u001b[1;32m--> 801\u001b[0m     \u001b[39mfor\u001b[39;00m key, table \u001b[39min\u001b[39;00m generate_tables_fn(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    802\u001b[0m         batch \u001b[39m=\u001b[39m python_formatter\u001b[39m.\u001b[39mformat_batch(table)\n\u001b[0;32m    803\u001b[0m         \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(_batch_to_examples(batch)):\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\packaged_modules\\json\\json.py:137\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError:\n\u001b[0;32m    136\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to read file \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m with error \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    138\u001b[0m \u001b[39m# If possible, parse the file as a list of json objects and exit the loop\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dataset, \u001b[39mlist\u001b[39m):  \u001b[39m# list is the only sequence type supported in JSON\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\datasets\\packaged_modules\\json\\json.py:113\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m         pa_table \u001b[39m=\u001b[39m paj\u001b[39m.\u001b[39;49mread_json(\n\u001b[0;32m    114\u001b[0m             io\u001b[39m.\u001b[39;49mBytesIO(batch), read_options\u001b[39m=\u001b[39;49mpaj\u001b[39m.\u001b[39;49mReadOptions(block_size\u001b[39m=\u001b[39;49mblock_size)\n\u001b[0;32m    115\u001b[0m         )\n\u001b[0;32m    116\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[39mexcept\u001b[39;00m (pa\u001b[39m.\u001b[39mArrowInvalid, pa\u001b[39m.\u001b[39mArrowNotImplementedError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\pyarrow\\_json.pyx:258\u001b[0m, in \u001b[0;36mpyarrow._json.read_json\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\pyarrow\\error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\pyarrow\\error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: JSON parse error: Invalid value. in row 0"
     ]
    }
   ],
   "source": [
    "# Apply this function to the streaming dataset\n",
    "# This cell will take a very long time to execute, so we will skip it for now. You can run it later if you want to train the model on the full dataset.\n",
    "from datasets import load_dataset\n",
    "split = \"train\" # \"valid\"\n",
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "\n",
    "data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n",
    "filtered_data = filter_streaming_dataset(data, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/huggingface-course--codeparrot-ds-train to C:/Users/Raj/.cache/huggingface/datasets/huggingface-course___json/huggingface-course--codeparrot-ds-train-7e9fc5dfe436a81a/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d59d374dd44101980dff6f1bda274c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03eb65b475ea4d1fad3ed0db8a9a2da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2fde0908c04d27b6a292b26e558ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a9938445974ac19270ac697b0a34f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/Raj/.cache/huggingface/datasets/huggingface-course___json/huggingface-course--codeparrot-ds-train-7e9fc5dfe436a81a/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset json/huggingface-course--codeparrot-ds-valid to C:/Users/Raj/.cache/huggingface/datasets/huggingface-course___json/huggingface-course--codeparrot-ds-valid-65557c3279496c87/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e7d31739db4241a0eea2282c974b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7158f60b6ab4cbf8317f211dfdf981f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/46.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d604d712c48d4f8aabea1fe2f256c2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81605c57d58949aa847f2e561ddc8ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/Raj/.cache/huggingface/datasets/huggingface-course___json/huggingface-course--codeparrot-ds-valid-65557c3279496c87/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# To speed up getting the dataset, we can reuse the filtered dataset from HF hub\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train, # .shuffle().select(range(50000))\n",
    "        \"validation\": ds_valid, # .shuffle().select(range(500))\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretraining the language model will take a while. So just run the training loop on a sample of the data and make sure the training successfully completes and the models are stored. Nothing is more frustrating than a training run failing at the last step because you forgot to create a folder or becauase there is a typo at the end of the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/Raj/.cache/huggingface/datasets/huggingface-course___json/huggingface-course--codeparrot-ds-train-7e9fc5dfe436a81a/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "Found cached dataset json (C:/Users/Raj/.cache/huggingface/datasets/huggingface-course___json/huggingface-course--codeparrot-ds-valid-65557c3279496c87/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    }
   ],
   "source": [
    "# To speed up getting the dataset, we can reuse the filtered dataset from HF hub\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train.shuffle().select(range(50000)),\n",
    "        \"validation\": ds_valid.shuffle().select(range(500)),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_NAME: TobiasHiort/TheGreatEscape\n",
      "PATH: gui/utils.py\n",
      "COPIES: 1\n",
      "SIZE: 52862\n",
      "CONTENT: #!/usr/bin/python3\n",
      "\n",
      "import pygame\n",
      "import sys\n",
      "import os\n",
      "import numpy\n",
      "import math\n",
      "import time\n",
      "import subprocess\n",
      "import doctest # read from txt, read docs\n",
      "import random\n",
      "import scipy.spatial as sp\n",
      "import \n",
      "LICENSE: mit\n"
     ]
    }
   ],
   "source": [
    "# let's look at an example\n",
    "for key in raw_datasets[\"train\"][0]:\n",
    "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the dataset\n",
    "from transformers import AutoTokenizer\n",
    "context_length = 128 # keeping it small for now; gpt-2 can handle up to 1024, gpt-3 up to 2048\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
