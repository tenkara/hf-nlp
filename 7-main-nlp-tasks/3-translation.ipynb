{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset kde4 (C:/Users/Raj/.cache/huggingface/datasets/kde4/en-fr-lang1=en,lang2=fr/0.0.0/243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5448d45898ff4ba4bda4ae5d9e502091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 210173\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\Raj\\.cache\\huggingface\\datasets\\kde4\\en-fr-lang1=en,lang2=fr\\0.0.0\\243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac\\cache-a067345757f9ef23.arrow and C:\\Users\\Raj\\.cache\\huggingface\\datasets\\kde4\\en-fr-lang1=en,lang2=fr\\0.0.0\\243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac\\cache-22b7e661839e4670.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this one train dataset needs to be split into train and validation\n",
    "split_datasets = raw_datasets[\"train\"].train_test_split(test_size=0.1, seed = 20)\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can rename the \"test\" key to \"validation\" like this\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '92924', 'translation': {'en': \"Calibration is about to check the value range your device delivers. Please move axis %1 %2 on your device to the maximum position. Press any button on the device or click on the'Next 'button to continue with the next step.\", 'fr': \"Le calibrage va vérifier la plage de valeurs que votre matériel produit. Veuillez déplacer l'axe %1 %2 de votre périphérique à la position maximale. Appuyez sur n'importe quel bouton du périphérique ou sur le bouton « & #160; Suivant & #160; » pour la prochaine étape.\"}}\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at one element of the dataset\n",
    "example = split_datasets[\"train\"][0]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> **English**\n",
       "> Calibration is about to check the value range your device delivers. Please move axis %1 %2 on your device to the maximum position. Press any button on the device or click on the'Next 'button to continue with the next step."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> **French**\n",
       "> Le calibrage va vérifier la plage de valeurs que votre matériel produit. Veuillez déplacer l'axe %1 %2 de votre périphérique à la position maximale. Appuyez sur n'importe quel bouton du périphérique ou sur le bouton « & #160; Suivant & #160; » pour la prochaine étape."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, JSON\n",
    "display(Markdown(\"> **English**\\n> {}\".format(example[\"translation\"][\"en\"])))\n",
    "display(Markdown(\"> **French**\\n> {}\".format(example[\"translation\"][\"fr\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "id": "92924",
       "translation": {
        "en": "Calibration is about to check the value range your device delivers. Please move axis %1 %2 on your device to the maximum position. Press any button on the device or click on the'Next 'button to continue with the next step.",
        "fr": "Le calibrage va vérifier la plage de valeurs que votre matériel produit. Veuillez déplacer l'axe %1 %2 de votre périphérique à la position maximale. Appuyez sur n'importe quel bouton du périphérique ou sur le bouton « & #160; Suivant & #160; » pour la prochaine étape."
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(JSON(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '92924',\n",
      " 'translation': {'en': 'Calibration is about to check the value range your '\n",
      "                       'device delivers. Please move axis %1 %2 on your device '\n",
      "                       'to the maximum position. Press any button on the '\n",
      "                       \"device or click on the'Next 'button to continue with \"\n",
      "                       'the next step.',\n",
      "                 'fr': 'Le calibrage va vérifier la plage de valeurs que votre '\n",
      "                       \"matériel produit. Veuillez déplacer l'axe %1 %2 de \"\n",
      "                       'votre périphérique à la position maximale. Appuyez sur '\n",
      "                       \"n'importe quel bouton du périphérique ou sur le bouton \"\n",
      "                       '« & #160; Suivant & #160; » pour la prochaine étape.'}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"92924\",\n",
      "  \"translation\": {\n",
      "    \"en\": \"Calibration is about to check the value range your device delivers. Please move axis %1 %2 on your device to the maximum position. Press any button on the device or click on the'Next 'button to continue with the next step.\",\n",
      "    \"fr\": \"Le calibrage va v\\u00e9rifier la plage de valeurs que votre mat\\u00e9riel produit. Veuillez d\\u00e9placer l'axe %1 %2 de votre p\\u00e9riph\\u00e9rique \\u00e0 la position maximale. Appuyez sur n'importe quel bouton du p\\u00e9riph\\u00e9rique ou sur le bouton \\u00ab & #160; Suivant & #160; \\u00bb pour la prochaine \\u00e9tape.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(example, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "id": "92924",
       "translation": {
        "en": "Calibration is about to check the value range your device delivers. Please move axis %1 %2 on your device to the maximum position. Press any button on the device or click on the'Next 'button to continue with the next step.",
        "fr": "Le calibrage va vérifier la plage de valeurs que votre matériel produit. Veuillez déplacer l'axe %1 %2 de votre périphérique à la position maximale. Appuyez sur n'importe quel bouton du périphérique ou sur le bouton « & #160; Suivant & #160; » pour la prochaine étape."
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '92924',\n",
       " 'translation': {'en': \"Calibration is about to check the value range your device delivers. Please move axis %1 %2 on your device to the maximum position. Press any button on the device or click on the'Next 'button to continue with the next step.\",\n",
       "  'fr': \"Le calibrage va vérifier la plage de valeurs que votre matériel produit. Veuillez déplacer l'axe %1 %2 de votre périphérique à la position maximale. Appuyez sur n'importe quel bouton du périphérique ou sur le bouton « & #160; Suivant & #160; » pour la prochaine étape.\"}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e82c80c8ab453daeb50dcf899fa03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca444d6016ce42549af28b70fe0501dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2efbf639dc472596e60b350f3b8c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95068cb4ed654af0957a385ba599727d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a24e712f5364065aa9bee33d3fa2d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82be8deca8e640c8a846785be7af926d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8957a44669d745f1bac5c9e79a87620c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Par défaut pour les threads élargis'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the behavior of the translation - shows lazy translation - leaves computer science related words in english while translating the rest to french\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint, tokenizer=model_checkpoint)\n",
    "translator(\"Default to expanded threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',\n",
       " 'fr': \"Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct.\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another example of this behavior can be seen with the word plugin\n",
    "split_datasets[\"train\"][172][\"translation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format.\"}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(split_datasets[\"train\"][172][\"translation\"][\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Ecrivez-moi un e-mail bientôt'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar behavior with email translation\n",
    "translator(\"Write me an email soon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data prep: remember that the tokenizer processes the target in the output language, here French, so we need to set the source and target languages accordingly\n",
    "# to see how this work, let's process one sample of each language in the training set\n",
    "en_sentence = split_datasets[\"train\"][1][\"translation\"][\"en\"]\n",
    "fr_sentence = split_datasets[\"train\"][1][\"translation\"][\"fr\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence, text_target = fr_sentence)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Default to expanded threads',\n",
       " 'Par défaut, développer les fils de discussion')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentence, fr_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [577, 1205, 483, 13077, 2, 1205, 517, 13926, 1588, 16, 3842, 9, 5, 1710, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['▁Par', '▁dé', 'f', 'aut', ',', '▁dé', 've', 'lop', 'per', '▁les', '▁fil', 's', '▁de', '▁discussion', '</s>']\n",
      "['▁Par', '▁défaut', ',', '▁développer', '▁les', '▁fils', '▁de', '▁discussion', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# if we forget to indicate that we are tokenizing labels, they will be tokenized by the input tokenizer, which in the case of a Marian model is not going to go well at all\n",
    "\n",
    "wrong_targets = tokenizer(fr_sentence)\n",
    "print(wrong_targets)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function on the dataset\n",
    "max_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, \n",
    "                             text_target=targets, \n",
    "                             max_length=max_length, \n",
    "                             truncation=True)\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261400675baf4a0bbfb7e97a8f88299b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/189155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ceb52a3d8324ec18dd190bf241aac04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21018 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can now apply that preprocessing function to our datasets in one go on all the splits\n",
    "tokenized_datasets = split_datasets.map(preprocess_function, \n",
    "                                        batched=True,\n",
    "                                        remove_columns=split_datasets[\"train\"].column_names,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will start with the AutoModel to fine tune; There will be no warning about missing weights or newly initialized weights, because the model was trained on a translation task\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will need a data collator to deal with the padding for dynamic batching. We can't just use the DataCollatorWithPadding because that only pads the inputs (inputIDs, attention mask, and token type IDs). Our labels should also be padded to the maximum length encountered in the labels. The padding value used to pad the labels should be -100 and not the padding token of the tokenizer, to make sure those padded values are ignored in the loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to test this on a few samples, we just call it on a list of examples from our tokkenized training set:\n",
    "\n",
    "batch = data_collator([tokenized_datasets[\"train\"][item] for item in range(3)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   60,  7418,  5244,  8234,   740,  4993,     8,  6471,     5,  2218,\n",
       "            29,   193,  2220,   742,     3,  4366, 14237,    14,     6, 16600,\n",
       "           301,   548,   301,   331,     5,   193, 24275,    17,     8,   668,\n",
       "          6142,     3, 33640,    36,    81,     6,  5411,  2709,  9376,    22,\n",
       "         24275,    59,    36,    19,  9376,   153,   402, 29033, 13774,   402,\n",
       "         29033,   416,    27,     8,  4034,  4888,     3,     0],\n",
       "        [  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,\n",
       "           550,  7032,  5821,  7907, 12649,     0,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can check our labels padded to the maximum length of the batch with -100\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[59513,    60,  7418,  5244,  8234,   740,  4993,     8,  6471,     5,\n",
       "          2218,    29,   193,  2220,   742,     3,  4366, 14237,    14,     6,\n",
       "         16600,   301,   548,   301,   331,     5,   193, 24275,    17,     8,\n",
       "           668,  6142,     3, 33640,    36,    81,     6,  5411,  2709,  9376,\n",
       "            22, 24275,    59,    36,    19,  9376,   153,   402, 29033, 13774,\n",
       "           402, 29033,   416,    27,     8,  4034,  4888,     3],\n",
       "        [59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513],\n",
       "        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
       "           817,   550,  7032,  5821,  7907, 12649,     0, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and decoder input ids to see that they are shifted versions of the labels\n",
    "batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0], [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]]\n"
     ]
    },
    {
     "data": {
      "application/json": "",
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": "",
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here are the labels for the first and second elements of our dataset\n",
    "print([tokenized_datasets[\"train\"][i][\"labels\"] for i in range(1, 3)])\n",
    "display(JSON(tokenizer.batch_decode([batch[\"input_ids\"][i] for i in range(1, 3)], skip_special_tokens=True)))\n",
    "display(JSON(tokenizer.batch_decode([batch[\"labels\"][i] for i in range(1,3)], skip_special_tokens=True)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we take a look at the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "                                              0.0/118.9 kB ? eta -:--:--\n",
      "     --------------------                    61.4/118.9 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 118.9/118.9 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\raj\\repos\\hf-nlp\\.venv\\lib\\site-packages (from sacrebleu) (2023.5.5)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\raj\\repos\\hf-nlp\\.venv\\lib\\site-packages (from sacrebleu) (1.24.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\raj\\repos\\hf-nlp\\.venv\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Using cached lxml-4.9.2-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\raj\\repos\\hf-nlp\\.venv\\lib\\site-packages (from portalocker->sacrebleu) (306)\n",
      "Installing collected packages: tabulate, portalocker, lxml, sacrebleu\n",
      "Successfully installed lxml-4.9.2 portalocker-2.7.0 sacrebleu-2.3.1 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "# SacreBLEU is a popular library to compute the BLEU score\n",
    "! pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8625628c15ad4cb9b4c009c340a5ff79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we then load it via evaluate.load()\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 36.67625610580775,\n",
       " 'counts': [10, 5, 3, 2],\n",
       " 'totals': [12, 11, 10, 9],\n",
       " 'precisions': [83.33333333333333, 45.45454545454545, 30.0, 22.22222222222222],\n",
       " 'bp': 0.9200444146293233,\n",
       " 'sys_len': 12,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try an example - this ia a good translation example with high bleu score\n",
    "predictions = [\n",
    "    \"This plugin lets you translate web pages between several langauages automatically.\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 2.156693969393992,\n",
       " 'counts': [1, 0, 0, 0],\n",
       " 'totals': [5, 4, 3, 2],\n",
       " 'precisions': [20.0, 12.5, 8.333333333333334, 6.25],\n",
       " 'bp': 0.20189651799465538,\n",
       " 'sys_len': 5,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try another example - this is a bad translation example with low bleu score\n",
    "predictions = [\n",
    "    \"This the the the the\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "    \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [2, 1, 0, 0],\n",
       " 'totals': [2, 1, 0, 0],\n",
       " 'precisions': [100.0, 100.0, 0.0, 0.0],\n",
       " 'bp': 0.004086771438464067,\n",
       " 'sys_len': 2,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another example with a bad translation and low bleu score\n",
    "predictions = [\n",
    "    \"This plugin\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    [\n",
    "    \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score can go from 0 to 100, and higher is better"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get from the model outputs to texts the metric can use, we will use the tokenizer.batch_decode() method. We just have to clean up all the -100s in the labels (the tokenizer will automatically do the same for the padding token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "        \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920a8cd6b3634fe5b6b3e5a4ea9d6294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# log in to hf\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now define our training arguments - hyperparameters for the training\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"marian-finetuned-kde4-en-fr\",\n",
    "    evaluation_strategy=\"no\", # this will take a while and hence not set\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True, # this is needed to generate the translations by the decoder\n",
    "    fp16=True, # speeds up training in modern GPUs\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/RajkNakka/marian-finetuned-kde4-en-fr into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "# instantiate the trainer\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01055dba9a414d3d88e126b0ce944384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.6964491605758667,\n",
       " 'eval_bleu': 39.16748044596104,\n",
       " 'eval_runtime': 821.3136,\n",
       " 'eval_samples_per_second': 25.591,\n",
       " 'eval_steps_per_second': 0.401}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before we train let's takke a look at the model's current performance on the validation set\n",
    "trainer.evaluate(max_length=max_length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A BLEU score of 39 is not too bad which reflects that our model is already good at translating english sentences to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7454b609034a49806e0ac318042bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4152, 'learning_rate': 1.9436175011276502e-05, 'epoch': 0.08}\n",
      "{'loss': 1.2227, 'learning_rate': 1.8872350022553e-05, 'epoch': 0.17}\n",
      "{'loss': 1.1686, 'learning_rate': 1.83085250338295e-05, 'epoch': 0.25}\n",
      "{'loss': 1.1294, 'learning_rate': 1.774582769508345e-05, 'epoch': 0.34}\n",
      "{'loss': 1.1176, 'learning_rate': 1.718200270635995e-05, 'epoch': 0.42}\n",
      "{'loss': 1.0673, 'learning_rate': 1.6618177717636447e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0685, 'learning_rate': 1.6054352728912948e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0282, 'learning_rate': 1.549052774018945e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0236, 'learning_rate': 1.4926702751465946e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0275, 'learning_rate': 1.4362877762742447e-05, 'epoch': 0.85}\n",
      "{'loss': 0.999, 'learning_rate': 1.3800180423996391e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding files tracked by Git LFS: ['source.spm', 'target.spm']. This may take a bit of time if the files are large.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.984, 'learning_rate': 1.3237483085250339e-05, 'epoch': 1.01}\n",
      "{'loss': 0.9151, 'learning_rate': 1.267365809652684e-05, 'epoch': 1.1}\n",
      "{'loss': 0.9119, 'learning_rate': 1.210983310780334e-05, 'epoch': 1.18}\n",
      "{'loss': 0.9008, 'learning_rate': 1.1546008119079838e-05, 'epoch': 1.27}\n",
      "{'loss': 0.9002, 'learning_rate': 1.098218313035634e-05, 'epoch': 1.35}\n",
      "{'loss': 0.8897, 'learning_rate': 1.0418358141632838e-05, 'epoch': 1.44}\n",
      "{'loss': 0.8865, 'learning_rate': 9.854533152909338e-06, 'epoch': 1.52}\n",
      "{'loss': 0.8695, 'learning_rate': 9.290708164185838e-06, 'epoch': 1.61}\n",
      "{'loss': 0.8852, 'learning_rate': 8.726883175462338e-06, 'epoch': 1.69}\n",
      "{'loss': 0.8764, 'learning_rate': 8.163058186738837e-06, 'epoch': 1.78}\n",
      "{'loss': 0.8881, 'learning_rate': 7.599233198015337e-06, 'epoch': 1.86}\n",
      "{'loss': 0.8606, 'learning_rate': 7.035408209291836e-06, 'epoch': 1.95}\n",
      "{'loss': 0.8593, 'learning_rate': 6.472710870545783e-06, 'epoch': 2.03}\n",
      "{'loss': 0.8039, 'learning_rate': 5.908885881822283e-06, 'epoch': 2.11}\n",
      "{'loss': 0.8235, 'learning_rate': 5.345060893098782e-06, 'epoch': 2.2}\n",
      "{'loss': 0.8275, 'learning_rate': 4.781235904375282e-06, 'epoch': 2.28}\n",
      "{'loss': 0.8099, 'learning_rate': 4.217410915651782e-06, 'epoch': 2.37}\n",
      "{'loss': 0.8122, 'learning_rate': 3.653585926928282e-06, 'epoch': 2.45}\n",
      "{'loss': 0.8158, 'learning_rate': 3.0897609382047817e-06, 'epoch': 2.54}\n",
      "{'loss': 0.8296, 'learning_rate': 2.5259359494812813e-06, 'epoch': 2.62}\n",
      "{'loss': 0.8163, 'learning_rate': 1.962110960757781e-06, 'epoch': 2.71}\n",
      "{'loss': 0.8103, 'learning_rate': 1.3994136220117278e-06, 'epoch': 2.79}\n",
      "{'loss': 0.7945, 'learning_rate': 8.355886332882274e-07, 'epoch': 2.88}\n",
      "{'loss': 0.8189, 'learning_rate': 2.7176364456472717e-07, 'epoch': 2.96}\n",
      "{'train_runtime': 5573.973, 'train_samples_per_second': 101.806, 'train_steps_per_second': 3.182, 'train_loss': 0.937283973194857, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17736, training_loss=0.937283973194857, metrics={'train_runtime': 5573.973, 'train_samples_per_second': 101.806, 'train_steps_per_second': 3.182, 'train_loss': 0.937283973194857, 'epoch': 3.0})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe93860c773b4616bb942f2da4a12469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8557320237159729,\n",
       " 'eval_bleu': 52.87444194376072,\n",
       " 'eval_runtime': 981.0221,\n",
       " 'eval_samples_per_second': 21.425,\n",
       " 'eval_steps_per_second': 0.335,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(max_length=max_length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly a 14-point improvement with fine tuning based on the Bleu scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/RajkNakka/marian-finetuned-kde4-en-fr\n",
      "   ac9fb55..ad525fc  main -> main\n",
      "\n",
      "To https://huggingface.co/RajkNakka/marian-finetuned-kde4-en-fr\n",
      "   ad525fc..c7e6f67  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/RajkNakka/marian-finetuned-kde4-en-fr/commit/ad525fc21c5d65e66ee1b63ab08b8dd17372182d'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push to the hub to upload the latest version of the model\n",
    "trainer.push_to_hub(tags = \"translation\", commit_message = \"Fine-tuned on KDE4 English to French translation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom training loop using Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will build the dataloaders from our datasets\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], \n",
    "    shuffle=True, \n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we instantiate our model from the pre-trained checkpoint (not fine-tuned checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# then we instantiate the optimizer\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once we have all those objects, we can send them to the accelerator.prepare() method to tell it to handle the training loop for us\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we use the length of train_dataloader to compute the total number of training steps\n",
    "# Remember to always do this after preparing the dataloader as that method will change the length of the dataloader\n",
    "# We use a classic linear schedule from the learning rate scheduler to 0\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_update_steps_per_epoch * num_train_epochs\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RajkNakka/marian-finetuned-kde4-en-fr-accelerate'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lastly, push the model to the hub\n",
    "# we will need to create a Repository object in a woring folder\n",
    "# First log into HF if not logged in already\n",
    "# It needs to contain your username and the name of the repository you want to create\n",
    "\n",
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"marian-finetuned-kde4-en-fr-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/RajkNakka/marian-finetuned-kde4-en-fr-accelerate into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "# we can clone that repository locally.\n",
    "output_dir = \"marian-finetuned-kde4-en-fr-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now upload anything we save in output_dir by calling the repo.push_to_hub() method. This will help us upload the intermediate models at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./RajkNakka/marian-finetuned-kde4-en-fr-accelerate'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"./{}\".format(repo_name)\n",
    "output_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
