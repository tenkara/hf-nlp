{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use DistilBERT model for this Masked Language Model (MLM) fine-tuning task\n",
    "# DistilBERT is a small, fast, cheap and light Transformer model trained by distilling Bert base.\n",
    "# It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of Bert’s performances as measured on the GLUE language understanding benchmark.\n",
    "# DistilBERT is thus a good solution for quick prototyping and for production environments where real-time inference is necessary.\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in distilbert-base-uncased: 66985530\n"
     ]
    }
   ],
   "source": [
    "# how many parameters does our model have?\n",
    "distilbert_num_parameters = model.num_parameters()\n",
    "print(f\"Number of parameters in {model_checkpoint}: {distilbert_num_parameters}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model as is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test example\n",
    "text = \"This is a great [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deal 0.0365118607878685\n",
      "success 0.0239587239921093\n",
      "adventure 0.0237447340041399\n",
      "idea 0.016085002571344376\n",
      "feat 0.010877519845962524\n"
     ]
    }
   ],
   "source": [
    "# tokenize our text, pass it to the model and get output predictions\n",
    "import torch\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "\n",
    "# get the top 5 predicted tokens and their probabilities for the masked token\n",
    "masked_index = (inputs[\"input_ids\"][0] == tokenizer.mask_token_id).nonzero().item()\n",
    "probs = torch.nn.functional.softmax(token_logits[0, masked_index], dim=-1)\n",
    "top_5_tokens = torch.topk(token_logits[0, masked_index], 5, dim=-1).indices.tolist()\n",
    "top_5_probabilities = torch.topk(probs, 5, dim=-1).values.tolist()\n",
    "\n",
    "# print the results\n",
    "for token, prob in zip(top_5_tokens, top_5_probabilities):\n",
    "    print(tokenizer.decode([token]), prob)\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are everyday choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To showcase domain adaptation, we'll use the famous Large Movie Dataset (or IMDb for short), which is a corpus of movie reviews that is often used to benchmark sentiment analysis models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By fine-tuning DistilBERT on this corpus, we expect the language model will adapt its vocabulary from the factual data of Wikipedia that it was pretrained on to the more subjective elements of movie reviews."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the movie reviews data from the Hugging Face Hub with load_dataset() function from Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (C:/Users/Raj/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de304afc7134512be37313cb7b54425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\Raj\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-9c48ce5d173413c7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...\n",
      "label:  1\n",
      "\n",
      "text:  This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.\n",
      "label:  1\n",
      "\n",
      "text:  George P. Cosmatos' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn't win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn't appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.\n",
      "label:  0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's checkout the train dataset\n",
    "sample_train = imdb_dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
    "for row in sample_train:\n",
    "    print(f\"text: \", row[\"text\"])\n",
    "    print(f\"label: \", row[\"label\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\Raj\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-c1eaa46e94dfbfd3.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  <br /><br />When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?<br /><br />Very quickly, however, I realized that this story was about A Thousand Other Things besides just Acres. I started crying and couldn't stop until long after the movie ended. Thank you Jane, Laura and Jocelyn, for bringing us such a wonderfully subtle and compassionate movie! Thank you cast, for being involved and portraying the characters with such depth and gentleness!<br /><br />I recognized the Angry sister; the Runaway sister and the sister in Denial. I recognized the Abusive Husband and why he was there and then the Father, oh oh the Father... all superbly played. I also recognized myself and this movie was an eye-opener, a relief, a chance to face my OWN truth and finally doing something about it. I truly hope A Thousand Acres has had the same effect on some others out there.<br /><br />Since I didn't understand why the cover said the film was about sisters fighting over land -they weren't fighting each other at all- I watched it a second time. Then I was able to see that if one hadn't lived a similar story, one would easily miss the overwhelming undercurrent of dread and fear and the deep bond between the sisters that runs through it all. That is exactly the reason why people in general often overlook the truth about their neighbors for instance.<br /><br />But yet another reason why this movie is so perfect!<br /><br />I don't give a rat's ass (pardon my French) about to what extend the King Lear story is followed. All I know is that I can honestly say: this movie has changed my life.<br /><br />Keep up the good work guys, you CAN and DO make a difference.<br /><br />\n",
      "label:  1\n",
      "\n",
      "text:  This is the latest entry in the long series of films with the French agent, O.S.S. 117 (the French answer to James Bond). The series was launched in the early 1950's, and spawned at least eight films (none of which was ever released in the U.S.). 'O.S.S.117:Cairo,Nest Of Spies' is a breezy little comedy that should not...repeat NOT, be taken too seriously. Our protagonist finds himself in the middle of a spy chase in Egypt (with Morroco doing stand in for Egypt) to find out about a long lost friend. What follows is the standard James Bond/Inspector Cloussou kind of antics. Although our man is something of an overt xenophobe,sexist,homophobe, it's treated as pure farce (as I said, don't take it too seriously). Although there is a bit of rough language & cartoon violence, it's basically okay for older kids (ages 12 & up). As previously stated in the subject line, just sit back,pass the popcorn & just enjoy.\n",
      "label:  1\n",
      "\n",
      "text:  This movie was so frustrating. Everything seemed energetic and I was totally prepared to have a good time. I at least thought I'd be able to stand it. But, I was wrong. First, the weird looping? It was like watching \"America's Funniest Home Videos\". The damn parents. I hated them so much. The stereo-typical Latino family? I need to speak with the person responsible for this. We need to have a talk. That little girl who was always hanging on someone? I just hated her and had to mention it. Now, the final scene transcends, I must say. It's so gloriously bad and full of badness that it is a movie of its own. What crappy dancing. Horrible and beautiful at once.\n",
      "label:  0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# useful sanity check on the test dataset that the labels are indeed correct\n",
    "sample_test = imdb_dataset[\"test\"].shuffle(seed=42).select(range(3))\n",
    "for row in sample_test:\n",
    "    print(f\"text: \", row[\"text\"])\n",
    "    print(f\"label: \", row[\"label\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\Raj\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-a1b3692aa5b43ab2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  If you've seen the classic Roger Corman version starring Vincent Price it's hard to put it out of your head, but you probably should do because this one is totally different. Subtlety has been abandoned in favour of gross-out horror - nudity, gore and all-round unpleasantness. OK it's ridiculous, trashy, sensationalised and historically dubious (did any members of the Inquisition really wear horn-rimmed glasses?), but despite all this it is strangely compelling. I literally couldn't tear myself away from the screen until the end of the movie. If there's a bigger compliment you can pay to a film I don't know what it is.\n",
      "label:  -1\n",
      "\n",
      "text:  For me, this was the most moving film of the decade. Samira Makhmalbaf shows pure bravery and vision in the making. She has an intelligence and gift for speaking to the people, regardless of their nationality or beliefs. I am inspired and touched by her humanity and can only hope that she has touched many people the same way. Her message in this film is strong, simple and pure. The human soul can survive the most unheard of cruelties and repression, yet still have the capability to hope and dream even the biggest dreams. Under the most incredible circumstances, the most unexpected people rise up to be heroes. This young girl who has recently regained her voice, yet is still afraid to use her new found freedom, is our hero. She daydreams of becoming president of war torn Afghanistan, the only vision of power that she can imagine that could truly change her current situation. We catch a glimpse of her spirit while witnessing her hardships. In the end, we are left with hope, hope that when her young voice does eventually speak out, it speaks loud and clear for all to hear- sounding a message that transcends borders, nationality and religion. The true epitome of the phoenix rising from the ashes. Hats off to the simple tale of the complex truth.\n",
      "label:  -1\n",
      "\n",
      "text:  There really isn't much to say about this \"film\". It has the odd smile or chuckle moment, but on the whole it's bland, predictable and generally pretty dull.<br /><br />The only reason I gave it three out of ten was for the annoyingly catchy jingle (which I hope I will forget soon....please God!). Otherwise its junk. Or mostly junk, interspersed with adverts for Smirnoff Ice.<br /><br />The lead characters give OK performances, but they really don't have anything much to work with.<br /><br />Best advice: Avoid it like a dentist's appointment. Or better yet, make a dentist's appointment instead of watching it.\n",
      "label:  -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# what does the unsupervised dataset look like?\n",
    "sample_unsupervised = imdb_dataset[\"unsupervised\"].shuffle(seed=42).select(range(3))\n",
    "for row in sample_unsupervised:\n",
    "    print(f\"text: \", row[\"text\"])\n",
    "    print(f\"label: \", row[\"label\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Raj\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-3dc38a01e511f796.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Raj\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-7ab987bb46990d3d.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Raj\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-03b325102b410e23.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate all the examples and then split them into equal chunks so we don't lose information if we have to truncate them using individual examples.\n",
    "# we will not set truncate=True in the tokenizer\n",
    "# we will also grab the word ids for masking later on\n",
    "# we will wrap this in a simple function and remove text and label columns since we don't need them\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast: # use fast tokenizers to grab the word ids for masking later on\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "# Use batched=True and activate fast multithreading!\n",
    "\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next step is to group the tokenized examples into chunks of equal sizes\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to run our experiments on a GPU that we have, we will pick something smaller than the maximum length of 512\n",
    "# we will pick 128\n",
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 length of input ids: 363\n",
      "Review 1 length of input ids: 304\n",
      "Review 2 length of input ids: 133\n"
     ]
    }
   ],
   "source": [
    "# slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"Review {idx} length of input ids: {len(sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of concatenated reviews: 800\n"
     ]
    }
   ],
   "source": [
    "# we can concatenate all these examples with a simple dictionary comprehension\n",
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"Total length of concatenated reviews: {total_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_ids chunks: 7\n",
      "Length of attention_mask chunks: 7\n",
      "Length of word_ids chunks: 7\n",
      "Chunk 0 length of input ids: 128\n",
      "Chunk 1 length of input ids: 128\n",
      "Chunk 2 length of input ids: 128\n",
      "Chunk 3 length of input ids: 128\n",
      "Chunk 4 length of input ids: 128\n",
      "Chunk 5 length of input ids: 128\n",
      "Chunk 6 length of input ids: 32\n"
     ]
    }
   ],
   "source": [
    "# now we can group the concatenated examples into chunks of equal sizes\n",
    "# use list comprehension to create slices of each feature\n",
    "chunks = {\n",
    "    k: [concatenated_examples[k][i:i+chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k in concatenated_examples.keys()\n",
    "}\n",
    "\n",
    "# print the chunk lengths\n",
    "for k, v in chunks.items():\n",
    "    print(f\"Length of {k} chunks: {len(v)}\")\n",
    "\n",
    "# print the chunk lengths of input_ids\n",
    "for idx, chunk in enumerate(chunks[\"input_ids\"]):\n",
    "    print(f\"Chunk {idx} length of input ids: {len(chunk)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's wrap the above logic in a function\n",
    "def group_texts(examples):\n",
    "    # concatenate all the input_ids into one long list\n",
    "    concatenated_examples = {\n",
    "        k: sum(examples[k], []) for k in examples.keys()\n",
    "    }\n",
    "    # compute the total length of the concatenated examples\n",
    "    total_length = len(concatenated_examples[\"input_ids\"])\n",
    "    # we drop the last chunk if it is smaller than chunk size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # split the concatenated examples into chunks of equal size\n",
    "    result = {\n",
    "        k: [concatenated_examples[k][i:i+chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k in concatenated_examples.keys()\n",
    "    }\n",
    "    # create a new labels column. By providing a copy of the input_ids, we provide the ground truth for the model to train on\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Raj\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-1f4439da88ac132d.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Raj\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-6d8ad827e94fa248.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Raj\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-f1bea5bab647283d.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 61291\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 59904\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 122957\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply group texts to our tokenized datasets using our dataset.map() method\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    ")\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i rented i am curious - yellow from my video store because of all the controversy that surrounded it when it was first released in 1967. i also heard that at first it was seized by u. s. customs if it ever tried to enter this country, therefore being a fan of films considered \" controversial \" i really had to see this for myself. < br / > < br / > the plot is centered around a young swedish drama student named lena who wants to learn everything she can about life. in particular she wants to focus her attentions to making some sort of documentary on what the average swede thought about certain political issues such'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take a look at the first example\n",
    "tokenizer.decode(lm_datasets[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take a look at the second example\n",
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arguably their answer to good old boy john ford, had sex scenes in his films. < br / > < br / > i do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in america. i am curious - yellow is a good film for anyone wanting to study the meat and potatoes ( no pun intended ) of swedish cinema. but really, this film doesn\\'t have much of a plot. [SEP] [CLS] \" i am curious : yellow \" is a risible and pretentious steaming pile. it doesn'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take a look at the third example\n",
    "tokenizer.decode(lm_datasets[\"train\"][2][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arguably their answer to good old boy john ford, had sex scenes in his films. < br / > < br / > i do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in america. i am curious - yellow is a good film for anyone wanting to study the meat and potatoes ( no pun intended ) of swedish cinema. but really, this film doesn\\'t have much of a plot. [SEP] [CLS] \" i am curious : yellow \" is a risible and pretentious steaming pile. it doesn'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take a look at the third label\n",
    "tokenizer.decode(lm_datasets[\"train\"][2][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask 15% of the input tokens at random for the model to train and predict\n",
    "# we can use the hf transformers data collator for language modeling for this task\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] i rented [MASK] am curious fragment yellow from my video store because of [MASK] the [MASK] that surrounded it when it was first 木 in 1967. i also heard that [MASK] first it was [MASK] by u. s. customs if it ever tried to enter [MASK] country, therefore being a fan of films considered \" controversial \" i really had to see this doubtful myself. < br / > [MASK] br / > the plot is [MASK] around a [MASK] swedish drama [MASK] [MASK] lena who wants to learn everything she [MASK] about life. in particular she wants to [MASK] [MASK] attentions [MASK] making some sort of documentary on what the average swede thought about certain political issues such'\n",
      "\n",
      "'>>> as the vietnam war [MASK] race issues [MASK] the united states [MASK] in between asking politicians and ordinary denize [MASK] of stockholm about their opinions on politics, she has sex with her drama [MASK], classmates, and married [MASK]. < br / > < br / > [MASK] kills me about i am curious - yellow is that 40 years ago, this was considered [MASK]. [MASK], the sex [MASK] nudity scenes are few and far between, even then it's not shot like [MASK] cheaply made porno. while my countrymen mind find it shocking [MASK] in reality sex and nu [MASK] are a major staple in swedish cinema. even ing [MASK] bergman,'\n"
     ]
    }
   ],
   "source": [
    "# let's take a look at the first example of the training dataset for masks\n",
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will build a data collator ourselves to do whole word masking\n",
    "# we will use the word_ids computed earlier to make a map between word indices and the corresponding tokens\n",
    "# then randomly decide which words to mask and apply that mask to the inputs\n",
    "# we will use the default data collator to handle the rest of the data collation\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list) # default value of list\n",
    "        current_word_index = -1  # index of the current word\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # randomly decide which words to mask\n",
    "        mask = np.random.binomial(1, wwm_probability, len(mapping))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels) # -100 is the default value for labels\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] i rented i am curious - yellow from my video [MASK] [MASK] of all the controversy that surrounded it [MASK] it was first released in 1967. [MASK] also heard that at [MASK] [MASK] was seized [MASK] [MASK]. s [MASK] customs [MASK] it ever tried to enter this country [MASK] therefore being a [MASK] of films considered \" [MASK] [MASK] i really had to [MASK] this for myself [MASK] [MASK] [MASK] / > < br / [MASK] the plot is centered [MASK] a young swedish drama student named lena who wants to learn everything she [MASK] about life. [MASK] [MASK] [MASK] wants to [MASK] her attentions to making some sort of documentary [MASK] what the average swede thought [MASK] [MASK] political issues such'\n",
      "\n",
      "'>>> [MASK] the vietnam war and race issues [MASK] [MASK] united states. in between asking [MASK] and ordinary denizens of [MASK] about [MASK] [MASK] on [MASK] [MASK] she has sex [MASK] her drama teacher, classmates, and married men. < br / > < br / > what kills me about i [MASK] curious - [MASK] is [MASK] 40 [MASK] ago, this was considered pornographic. really, the sex and nudity scenes are [MASK] and far between, even then it's not [MASK] [MASK] some [MASK] [MASK] made porno. while [MASK] [MASK] [MASK] mind find it [MASK], in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,'\n"
     ]
    }
   ],
   "source": [
    "# let's try it on the same samples as before\n",
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "# print the input_ids\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\Raj\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-b291189f27c93d76.arrow and C:\\Users\\Raj\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-5f9b2012bee45a4b.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's down sample the training and validation datasets\n",
    "train_size = 10_000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size,\n",
    "    test_size=test_size,\n",
    "    seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353a138afabf4663a45469c545c2e5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# login to huggingface.co\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning DistilBERT with HF Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To eliminate the source of random masking with DataCollatorForLanguageModeling for each iteration causing fluctuations in the perplexity scores with each training run, we will implement a simple function that applies the msaking on a batch of inputs and labels.\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Raj\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-5964b10f4b05bf65.arrow\n"
     ]
    }
   ],
   "source": [
    "# apply the function to the downsampled test dataset and drop the unmasked columns so we can replace them with the masked ones\n",
    "downsampled_dataset = downsampled_dataset.remove_columns([\"word_ids\"])\n",
    "eval_dataset = downsampled_dataset[\"test\"].map(\n",
    "    insert_random_mask, \n",
    "    batched=True,\n",
    "    remove_columns=downsampled_dataset[\"test\"].column_names\n",
    ")\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the dataloaders as usual. We will use the default data collator for the evaluation dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(\n",
    "    downsampled_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will follow the standard steps with HF Accelerator\n",
    "# First step is to load a fresh version of the pretrained model\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we specify the optimizer; we will use the standard AdamW optimizer with weight decay\n",
    "from torch.optim import AdamW\n",
    "\n",
    "Optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now prepare for training with the Accelerator object\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, Optimizer, train_dataloader, eval_dataloader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the model, optimizer and dataloaders ready, we can now define the learning rate scheduler\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_ephocs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_update_steps_per_epoch * num_train_ephocs\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RajkNakka/distilbert-base-uncased-finetuned-imdb-accelerate'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One last thing before we start training is to create a model repository on the Hub to save our model checkpoints\n",
    "# We can use the HF Hub library to first generate the full name of the repository\n",
    "from huggingface_hub import get_full_repo_name\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-imdb-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raj\\repos\\hf-nlp\\7-main-nlp-tasks\\../../distilbert-base-uncased-finetuned-imdb-accelerate is already a clone of https://huggingface.co/RajkNakka/distilbert-base-uncased-finetuned-imdb-accelerate. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "# We can now create and clone the repository on the Hub\n",
    "\n",
    "from huggingface_hub import Repository\n",
    "\n",
    "output_dir = \"../../distilbert-base-uncased-finetuned-imdb-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d3dfa165534dcc99c450cf6a244aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: perplexity: 10.882795953043349\n",
      "epoch 1: perplexity: 10.72849054041991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: perplexity: 10.72849054041991\n"
     ]
    }
   ],
   "source": [
    "# We can now set up the training and evaluation loop\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_ephocs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    print(f\"epoch {epoch}: perplexity: {perplexity}\")\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(commit_message=f\"Training in progress epoch {epoch}\", blocking=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
