{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a task-specific model using pipeline\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "camembert_fill_mask = pipeline(\"fill-mask\", model=\"camembert-base\")\n",
    "results = camembert_fill_mask(\"Le camembert est <mask> :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4909117817878723,\n",
       "  'token': 7200,\n",
       "  'token_str': 'délicieux',\n",
       "  'sequence': 'Le camembert est délicieux :)'},\n",
       " {'score': 0.10556931048631668,\n",
       "  'token': 2183,\n",
       "  'token_str': 'excellent',\n",
       "  'sequence': 'Le camembert est excellent :)'},\n",
       " {'score': 0.03453318774700165,\n",
       "  'token': 26202,\n",
       "  'token_str': 'succulent',\n",
       "  'sequence': 'Le camembert est succulent :)'},\n",
       " {'score': 0.03303121030330658,\n",
       "  'token': 528,\n",
       "  'token_str': 'meilleur',\n",
       "  'sequence': 'Le camembert est meilleur :)'},\n",
       " {'score': 0.03007640689611435,\n",
       "  'token': 1654,\n",
       "  'token_str': 'parfait',\n",
       "  'sequence': 'Le camembert est parfait :)'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>NOTUSED\n"
     ]
    }
   ],
   "source": [
    "# instantiate a model directly from the model architecture\n",
    "from transformers import CamembertTokenizer, CamembertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# use the model to predict a masked token\n",
    "input_ids = tokenizer(\"Le camembert est <mask> :)\", padding=True, return_tensors=\"pt\")  # Batch size 1\n",
    "output = model(**input_ids)\n",
    "logits = output.logits\n",
    "\n",
    "# get the predicted token\n",
    "predicted_index = torch.argmax(logits[0, -1, :]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "# assert predicted_token == \"fromage\"\n",
    "print(predicted_token)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 19.9792,  -4.5944,   7.1252,  ...,  -6.3529,  -3.5433,   1.3889],\n",
       "         [  0.8480,  -3.5700,  10.2163,  ...,  -9.4812,  -1.8443,  -1.3015],\n",
       "         [ -0.2291,  -9.3549,  -0.0379,  ..., -22.2491, -10.7558,  -2.2051],\n",
       "         ...,\n",
       "         [ -2.2079,  -3.1100,   0.8896,  ...,  -6.0595,  -2.9762,  -4.5024],\n",
       "         [ -1.2716,  -3.9497,   8.1755,  ...,  -2.3597,  -9.8304,  -2.0070],\n",
       "         [  5.3539,  -5.8456,  22.9419,  ...,  -8.5553,  -5.7814,   2.1215]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the model to predict a masked token\n",
    "input_ids = tokenizer(\"Le camembert est <mask> :)\", padding=True, return_tensors=\"pt\")  # Batch size 1\n",
    "output = model(**input_ids)\n",
    "logits = output.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 54, 730, 25543, 110, 30, 32004, 4522, 6]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Le camembert est <mask> :)\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 54, 730, 25543, 110, 30, 32004, 4522, 6]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Le camembert est <mask> :)\", add_special_tokens=True)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    5,    54,   730, 25543,   110,    30, 32004,  4522,     6]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Le camembert est <mask> :)\", return_tensors=\"pt\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    5,    54,   730, 25543,   110,    30, 32004,  4522,     6]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\"Le camembert est <mask> :)\", return_tensors=\"pt\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ 19.9792,  -4.5944,   7.1252,  ...,  -6.3529,  -3.5433,   1.3889],\n",
       "         [  0.8480,  -3.5700,  10.2163,  ...,  -9.4812,  -1.8443,  -1.3015],\n",
       "         [ -0.2291,  -9.3549,  -0.0379,  ..., -22.2491, -10.7558,  -2.2051],\n",
       "         ...,\n",
       "         [ -2.2079,  -3.1100,   0.8896,  ...,  -6.0595,  -2.9762,  -4.5024],\n",
       "         [ -1.2716,  -3.9497,   8.1755,  ...,  -2.3597,  -9.8304,  -2.0070],\n",
       "         [  5.3539,  -5.8456,  22.9419,  ...,  -8.5553,  -5.7814,   2.1215]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(input_ids)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    5,    54,   730, 25543,   110,    30, 32004,  4522,     6]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\"Le camembert est <mask> :)\", return_tensors=\"pt\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ 19.9792,  -4.5944,   7.1252,  ...,  -6.3529,  -3.5433,   1.3889],\n",
       "         [  0.8480,  -3.5700,  10.2163,  ...,  -9.4812,  -1.8443,  -1.3015],\n",
       "         [ -0.2291,  -9.3549,  -0.0379,  ..., -22.2491, -10.7558,  -2.2051],\n",
       "         ...,\n",
       "         [ -2.2079,  -3.1100,   0.8896,  ...,  -6.0595,  -2.9762,  -4.5024],\n",
       "         [ -1.2716,  -3.9497,   8.1755,  ...,  -2.3597,  -9.8304,  -2.0070],\n",
       "         [  5.3539,  -5.8456,  22.9419,  ...,  -8.5553,  -5.7814,   2.1215]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**input_ids)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs:  tensor([2.2994e-08, 3.1457e-13, 9.9999e-01,  ..., 2.0938e-14, 3.3545e-13,\n",
      "        9.0744e-10], grad_fn=<SoftmaxBackward0>)\n",
      "values:  tensor([9.9999e-01, 1.3571e-06, 1.1593e-06, 5.1620e-07, 4.9516e-07],\n",
      "       grad_fn=<TopkBackward0>)\n",
      "indices:  tensor([ 2, 83,  8, 43, 38])\n",
      "predicted_index: 2\n",
      "predicted_token: </s>NOTUSED\n"
     ]
    }
   ],
   "source": [
    "# get the predicted token\n",
    "predicted_index = torch.argmax(output.logits[0, -1, :]).item()\n",
    "probs = torch.softmax(output.logits[0, -1, :], dim=0)\n",
    "print(f\"probs: \", probs)\n",
    "values, indices = torch.topk(probs, 5)\n",
    "print(f\"values: \", values)\n",
    "print(f\"indices: \", indices)\n",
    "\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "print(f\"predicted_index: {predicted_index}\")\n",
    "print(f\"predicted_token: {predicted_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    5,    54,   730, 25543,   110,    30, 32004,  4522,     6]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Le camembert est <mask> :)\", add_special_tokens=True, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 19.9792,  -4.5944,   7.1252,  ...,  -6.3529,  -3.5433,   1.3889],\n",
       "         [  0.8480,  -3.5700,  10.2163,  ...,  -9.4812,  -1.8443,  -1.3015],\n",
       "         [ -0.2291,  -9.3549,  -0.0379,  ..., -22.2491, -10.7558,  -2.2051],\n",
       "         ...,\n",
       "         [ -2.2079,  -3.1100,   0.8896,  ...,  -6.0595,  -2.9762,  -4.5024],\n",
       "         [ -1.2716,  -3.9497,   8.1755,  ...,  -2.3597,  -9.8304,  -2.0070],\n",
       "         [  5.3539,  -5.8456,  22.9419,  ...,  -8.5553,  -5.7814,   2.1215]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(**inputs)[0] # The last hidden-state is the first element of the output tuple\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 32005])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ 19.9792,  -4.5944,   7.1252,  ...,  -6.3529,  -3.5433,   1.3889],\n",
       "         [  0.8480,  -3.5700,  10.2163,  ...,  -9.4812,  -1.8443,  -1.3015],\n",
       "         [ -0.2291,  -9.3549,  -0.0379,  ..., -22.2491, -10.7558,  -2.2051],\n",
       "         ...,\n",
       "         [ -2.2079,  -3.1100,   0.8896,  ...,  -6.0595,  -2.9762,  -4.5024],\n",
       "         [ -1.2716,  -3.9497,   8.1755,  ...,  -2.3597,  -9.8304,  -2.0070],\n",
       "         [  5.3539,  -5.8456,  22.9419,  ...,  -8.5553,  -5.7814,   2.1215]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MaskedLMOutput' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m logits\u001b[39m.\u001b[39;49msize() \u001b[39m# Model outputs a MLM object containing the loss and logits\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MaskedLMOutput' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "logits.size() # Model outputs a MLM object containing the loss and logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    5,    54,   730, 25543,   110,    30, 32004,  4522,     6]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the logits out of the MLM object\n",
    "logits = model(**input_ids)[0]\n",
    "masked_index = (inputs.input_ids.squeeze() == tokenizer.mask_token_id).nonzero().item() # squeeze() removes the batch dimension and converts to scalar\n",
    "masked_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 19.9792,  -4.5944,   7.1252,  ...,  -6.3529,  -3.5433,   1.3889],\n",
       "         [  0.8480,  -3.5700,  10.2163,  ...,  -9.4812,  -1.8443,  -1.3015],\n",
       "         [ -0.2291,  -9.3549,  -0.0379,  ..., -22.2491, -10.7558,  -2.2051],\n",
       "         ...,\n",
       "         [ -2.2079,  -3.1100,   0.8896,  ...,  -6.0595,  -2.9762,  -4.5024],\n",
       "         [ -1.2716,  -3.9497,   8.1755,  ...,  -2.3597,  -9.8304,  -2.0070],\n",
       "         [  5.3539,  -5.8456,  22.9419,  ...,  -8.5553,  -5.7814,   2.1215]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the logits out of the MLM object\n",
    "logits = output.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.2079, -3.1100,  0.8896,  ..., -6.0595, -2.9762, -4.5024],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the logits for the masked index\n",
    "logits_mask_token = logits[0, masked_index, :] # logits for the masked index\n",
    "logits_mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask token logits size:  torch.Size([32005])\n",
      "mask token logits shape:  torch.Size([32005])\n"
     ]
    }
   ],
   "source": [
    "print(f\"mask token logits size: \", logits_mask_token.size())\n",
    "print(f\"mask token logits shape: \", logits_mask_token.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3250e-08, 9.4327e-09, 5.1481e-07,  ..., 4.9397e-10, 1.0783e-08,\n",
       "        2.3438e-09], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the probabilities for the masked token logits\n",
    "prob = logits_mask_token.softmax(dim=0)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4909, 0.1056, 0.0345, 0.0330, 0.0301], grad_fn=<TopkBackward0>),\n",
       " tensor([ 7200,  2183, 26202,   528,  1654]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the top 5 probabilities and indices\n",
    "values, indices = prob.topk(5)\n",
    "values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁délicieux'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pred_token = tokenizer.convert_ids_to_tokens(indices[0].item())\n",
    "top_pred_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁délicieux', '▁excellent', '▁succulent', '▁meilleur', '▁parfait']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the top 5 predicted tokens\n",
    "top_pred_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "top_pred_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked_index: 6\n",
      "[('Le camembert est délicieux :)', 0.4909117817878723, ' délicieux'), ('Le camembert est excellent :)', 0.10556931048631668, ' excellent'), ('Le camembert est succulent :)', 0.03453318774700165, ' succulent')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import CamembertForMaskedLM\n",
    "from transformers import CamembertTokenizer\n",
    "\n",
    "\n",
    "def fill_mask(masked_input, model, tokenizer, topk=5):\n",
    "    # Adapted from https://github.com/pytorch/fairseq/blob/master/fairseq/models/roberta/hub_interface.py\n",
    "    assert masked_input.count(\"<mask>\") == 1\n",
    "    input_ids = torch.tensor(tokenizer.encode(masked_input, add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "    logits = model(input_ids)[0]  # The last hidden-state is the first element of the output tuple\n",
    "    masked_index = (input_ids.squeeze() == tokenizer.mask_token_id).nonzero().item()\n",
    "    print (f\"masked_index: {masked_index}\")\n",
    "    logits = logits[0, masked_index, :]\n",
    "    prob = logits.softmax(dim=0)\n",
    "    values, indices = prob.topk(k=topk, dim=0)\n",
    "    topk_predicted_token_bpe = \" \".join(\n",
    "        [tokenizer.convert_ids_to_tokens(indices[i].item()) for i in range(len(indices))]\n",
    "    )\n",
    "    masked_token = tokenizer.mask_token\n",
    "    topk_filled_outputs = []\n",
    "    for index, predicted_token_bpe in enumerate(topk_predicted_token_bpe.split(\" \")):\n",
    "        predicted_token = predicted_token_bpe.replace(\"\\u2581\", \" \")\n",
    "        if \" {0}\".format(masked_token) in masked_input:\n",
    "            topk_filled_outputs.append(\n",
    "                (\n",
    "                    masked_input.replace(\" {0}\".format(masked_token), predicted_token),\n",
    "                    values[index].item(),\n",
    "                    predicted_token,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            topk_filled_outputs.append(\n",
    "                (masked_input.replace(masked_token, predicted_token), values[index].item(), predicted_token,)\n",
    "            )\n",
    "    return topk_filled_outputs\n",
    "\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n",
    "model.eval()\n",
    "\n",
    "masked_input = \"Le camembert est <mask> :)\"\n",
    "print(fill_mask(masked_input, model, tokenizer, topk=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
