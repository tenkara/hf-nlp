{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 508/508 [00:00<00:00, 508kB/s]\n",
      "c:\\Users\\Raj\\repos\\hf-nlp\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Raj\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading pytorch_model.bin: 100%|██████████| 445M/445M [00:37<00:00, 11.8MB/s] \n",
      "Downloading (…)tencepiece.bpe.model: 100%|██████████| 811k/811k [00:00<00:00, 4.05MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.40M/1.40M [00:00<00:00, 5.69MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a task-specific model using pipeline\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "camembert_fill_mask = pipeline(\"fill-mask\", model=\"camembert-base\")\n",
    "results = camembert_fill_mask(\"Le camembert est <mask> :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4909117817878723,\n",
       "  'token': 7200,\n",
       "  'token_str': 'délicieux',\n",
       "  'sequence': 'Le camembert est délicieux :)'},\n",
       " {'score': 0.10556931048631668,\n",
       "  'token': 2183,\n",
       "  'token_str': 'excellent',\n",
       "  'sequence': 'Le camembert est excellent :)'},\n",
       " {'score': 0.03453318774700165,\n",
       "  'token': 26202,\n",
       "  'token_str': 'succulent',\n",
       "  'sequence': 'Le camembert est succulent :)'},\n",
       " {'score': 0.03303121030330658,\n",
       "  'token': 528,\n",
       "  'token_str': 'meilleur',\n",
       "  'sequence': 'Le camembert est meilleur :)'},\n",
       " {'score': 0.03007640689611435,\n",
       "  'token': 1654,\n",
       "  'token_str': 'parfait',\n",
       "  'sequence': 'Le camembert est parfait :)'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>NOTUSED\n"
     ]
    }
   ],
   "source": [
    "# instantiate a model directly from the model architecture\n",
    "from transformers import CamembertTokenizer, CamembertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# use the model to predict a masked token\n",
    "input_ids = tokenizer(\"Le camembert est <mask> :)\", padding=True, return_tensors=\"pt\")  # Batch size 1\n",
    "output = model(**input_ids)\n",
    "logits = output.logits\n",
    "\n",
    "# get the predicted token\n",
    "predicted_index = torch.argmax(logits[0, -1, :]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "# assert predicted_token == \"fromage\"\n",
    "print(predicted_token)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 19.9792,  -4.5944,   7.1252,  ...,  -6.3529,  -3.5433,   1.3889],\n",
       "         [  0.8480,  -3.5700,  10.2163,  ...,  -9.4812,  -1.8443,  -1.3015],\n",
       "         [ -0.2291,  -9.3549,  -0.0379,  ..., -22.2491, -10.7558,  -2.2051],\n",
       "         ...,\n",
       "         [ -2.2079,  -3.1100,   0.8896,  ...,  -6.0595,  -2.9762,  -4.5024],\n",
       "         [ -1.2716,  -3.9497,   8.1755,  ...,  -2.3597,  -9.8304,  -2.0070],\n",
       "         [  5.3539,  -5.8456,  22.9419,  ...,  -8.5553,  -5.7814,   2.1215]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the model to predict a masked token\n",
    "input_ids = tokenizer(\"Le camembert est <mask> :)\", padding=True, return_tensors=\"pt\")  # Batch size 1\n",
    "output = model(**input_ids)\n",
    "logits = output.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 54, 730, 25543, 110, 30, 32004, 4522, 6]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Le camembert est <mask> :)\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 54, 730, 25543, 110, 30, 32004, 4522, 6]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Le camembert est <mask> :)\", add_special_tokens=True)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    5,    54,   730, 25543,   110,    30, 32004,  4522,     6]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Le camembert est <mask> :)\", return_tensors=\"pt\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ 19.9792,  -4.5944,   7.1252,  ...,  -6.3529,  -3.5433,   1.3889],\n",
       "         [  0.8480,  -3.5700,  10.2163,  ...,  -9.4812,  -1.8443,  -1.3015],\n",
       "         [ -0.2291,  -9.3549,  -0.0379,  ..., -22.2491, -10.7558,  -2.2051],\n",
       "         ...,\n",
       "         [ -2.2079,  -3.1100,   0.8896,  ...,  -6.0595,  -2.9762,  -4.5024],\n",
       "         [ -1.2716,  -3.9497,   8.1755,  ...,  -2.3597,  -9.8304,  -2.0070],\n",
       "         [  5.3539,  -5.8456,  22.9419,  ...,  -8.5553,  -5.7814,   2.1215]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(input_ids)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    5,    54,   730, 25543,   110,    30, 32004,  4522,     6]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\"Le camembert est <mask> :)\", return_tensors=\"pt\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ 19.9792,  -4.5944,   7.1252,  ...,  -6.3529,  -3.5433,   1.3889],\n",
       "         [  0.8480,  -3.5700,  10.2163,  ...,  -9.4812,  -1.8443,  -1.3015],\n",
       "         [ -0.2291,  -9.3549,  -0.0379,  ..., -22.2491, -10.7558,  -2.2051],\n",
       "         ...,\n",
       "         [ -2.2079,  -3.1100,   0.8896,  ...,  -6.0595,  -2.9762,  -4.5024],\n",
       "         [ -1.2716,  -3.9497,   8.1755,  ...,  -2.3597,  -9.8304,  -2.0070],\n",
       "         [  5.3539,  -5.8456,  22.9419,  ...,  -8.5553,  -5.7814,   2.1215]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**input_ids)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs:  tensor([2.2994e-08, 3.1457e-13, 9.9999e-01,  ..., 2.0938e-14, 3.3545e-13,\n",
      "        9.0744e-10], grad_fn=<SoftmaxBackward0>)\n",
      "values:  tensor([9.9999e-01, 1.3571e-06, 1.1593e-06, 5.1620e-07, 4.9516e-07],\n",
      "       grad_fn=<TopkBackward0>)\n",
      "indices:  tensor([ 2, 83,  8, 43, 38])\n",
      "predicted_index: 2\n",
      "predicted_token: </s>NOTUSED\n"
     ]
    }
   ],
   "source": [
    "# get the predicted token\n",
    "predicted_index = torch.argmax(output.logits[0, -1, :]).item()\n",
    "probs = torch.softmax(output.logits[0, -1, :], dim=0)\n",
    "print(f\"probs: \", probs)\n",
    "values, indices = torch.topk(probs, 5)\n",
    "print(f\"values: \", values)\n",
    "print(f\"indices: \", indices)\n",
    "\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "print(f\"predicted_index: {predicted_index}\")\n",
    "print(f\"predicted_token: {predicted_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to predict a masked token\n",
    "input_ids = tokenizer(\"Le camembert est <mask> :)\", padding=True, return_tensors=\"pt\")  # Batch size 1\n",
    "output = model(**input_ids)\n",
    "logits = output.logits\n",
    "\n",
    "# get the predicted token\n",
    "predicted_index = torch.argmax(logits[0, -1, :]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "# assert predicted_token == \"fromage\"\n",
    "print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Le camembert est délicieux :)', 0.4909117817878723, ' délicieux'), ('Le camembert est excellent :)', 0.10556931048631668, ' excellent'), ('Le camembert est succulent :)', 0.03453318774700165, ' succulent')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import CamembertForMaskedLM\n",
    "from transformers import CamembertTokenizer\n",
    "\n",
    "\n",
    "def fill_mask(masked_input, model, tokenizer, topk=5):\n",
    "    # Adapted from https://github.com/pytorch/fairseq/blob/master/fairseq/models/roberta/hub_interface.py\n",
    "    assert masked_input.count(\"<mask>\") == 1\n",
    "    input_ids = torch.tensor(tokenizer.encode(masked_input, add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "    logits = model(input_ids)[0]  # The last hidden-state is the first element of the output tuple\n",
    "    masked_index = (input_ids.squeeze() == tokenizer.mask_token_id).nonzero().item()\n",
    "    logits = logits[0, masked_index, :]\n",
    "    prob = logits.softmax(dim=0)\n",
    "    values, indices = prob.topk(k=topk, dim=0)\n",
    "    topk_predicted_token_bpe = \" \".join(\n",
    "        [tokenizer.convert_ids_to_tokens(indices[i].item()) for i in range(len(indices))]\n",
    "    )\n",
    "    masked_token = tokenizer.mask_token\n",
    "    topk_filled_outputs = []\n",
    "    for index, predicted_token_bpe in enumerate(topk_predicted_token_bpe.split(\" \")):\n",
    "        predicted_token = predicted_token_bpe.replace(\"\\u2581\", \" \")\n",
    "        if \" {0}\".format(masked_token) in masked_input:\n",
    "            topk_filled_outputs.append(\n",
    "                (\n",
    "                    masked_input.replace(\" {0}\".format(masked_token), predicted_token),\n",
    "                    values[index].item(),\n",
    "                    predicted_token,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            topk_filled_outputs.append(\n",
    "                (masked_input.replace(masked_token, predicted_token), values[index].item(), predicted_token,)\n",
    "            )\n",
    "    return topk_filled_outputs\n",
    "\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n",
    "model.eval()\n",
    "\n",
    "masked_input = \"Le camembert est <mask> :)\"\n",
    "print(fill_mask(masked_input, model, tokenizer, topk=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
